---
title: "AI Systems: Data, Models, and Logic"
---

## Data, Models, and Logic: Core Components of AI Systems

<figure>
  <img src="/assets/images/nested_ai.svg">
       alt="Core AI system diagram showing a left-to-right flow: Data feeds into Model, Model feeds into Logic, and Logic triggers Action. Arrows indicate sequential flow from inputs to decisions.">
  <figcaption>An AI system is best understood as the interaction of data, a model, and decision logic—rather than as a model alone.</figcaption>
</figure>

### Data

Data forms the foundation of all analytics and AI systems. Regardless of how sophisticated a model or decision framework may be, the behavior of the system is entirely shaped by the data it has availble. Understanding what data represents, how it is generated, and how it enters a system is a critical step on the way to understanding how AI systems operate.

Very broadly defined, data is **recorded observations about the world**. These observations take many forms: transaction records, sensor readings, text documents, images, user interactions, or system logs. These data sources might all differ in structure and complexity, but they all capture **past events** or states that can be analyzed, modeled, and in some cases acted upon.

Data serves two very critical functions within AI systems. First, it is used to **train models**. Historical data provides the needed examples from which AI models learn patterns, relationships, or representations. The coverage and quality of this data directly influence what a model can learn and how well it generalizes *beyond* its training examples. Second, data is used during **system operation**, when new observations are provided as input to a trained model in order to generate predictions, classifications, or scores. The key thing to remember here is that errors, shifts, or inconsistencies in *either* training data *or* operational data can degrade system performance.

For the most part, data is not a complete or neutral representation of reality. Data always reflects the processes through which it was collected. These include factors such as organizational priorities, technical constraints, and human choices. Some phenomena are simply easier to observe and record than others, some groups or behaviors are overepresented, and some variables serve only as indirect proxies for what is actually of interest. As a result, data commonly contains noise, omissions, and systematic issues that need to be addressed.

From a systems perspective, data does not simply exist, ready to be used by whatever model is in place; it is **acquired and prepared**, in a process that is often referred to or thought of as a pipelines. These pipelines involve decisions about what to collect, how frequently to collect it, how it is stored, and how it is cleaned or transformed before use. Choices made at this stage, such as how missing values are handled or how categories are encoded, can have consequences that propagate throughout the system. Often in surprising ways.

Effective analysis of AI systems begins with careful attention to the data being collected. Asking where data comes from, what it represents, and what it leaves out is often the most, and sometimes the only, reliable way to understand system behavior, anticipate limitations, and diagnose failures.

A defining challenge for artificial intelligence systems is that they must operate under uncertainty. Data is often incomplete, noisy, delayed, or sometimes just plain wrong. Models can only approximate real-world processes. AI models don't try to eliminating uncertainty, but these systems are designed to manage and act despite it, using probabilistic reasoning, learned patterns, and decision logic to function in the imperfect environments they are placed in.

### Models

A **model** is a formal representation of a relationship between inputs and outputs. In analytics and AI systems, models are used to map the observed data to predictions, classifications, scores, or other quantities that support decision-making. While models can take many forms—from simple equations to complex neural networks—their role within a system is conceptually consistent: they provide a structured way to generalize from past observations to new situations.

Models differ from raw data in an important way. Data records what has already happened. The key word to remember there is "already". All data is from the past. we can't collect data on the future. Models on the other hand encode assumptions about **how the world works**.   The assumptions the model makes might explicit. Very clear assumptions that are easily interpretable, as in a linear equation that specifies how inputs specifically combine to produce some output. Or the model might be more implicit, as in a deep learning model that learns internal representations through training on large datasets . In both cases, the model embodies a hypothesis about underlying patterns found in the data.

Models are created through a **training process**.  Historical data is used to adjust a model’s parameters so that its outputs align as closely as possible with observed outcomes. This process allows the model to capture regularities in the data, but it also ties the model’s behavior to the quality and scope of the data it was trained on. A model cannot reliably learn patterns that are absent, rare, or systematically distorted in the training data.

Once trained, a model is given new outputs, and used the representation that is learned in training, to generate an output. These outputs are often **probabilistic** rather than deterministic, the model can't "know" that the process it followed to map the input data and adjust it's parameters is accurate or without loss. Instead of producing a single “correct” answer, a model may estimate the likelihood of different outcomes or assign scores that reflect a relative confidence in a range of outputs. This probabilistic nature is a strength as it allows models to operate under a high level of uncertainty. 

Talking about models this way is perhaps a little dangerous. Models are not inherently intelligent or autonomous. They do not understand context, intent, or consequences in a human sense. Instead, they apply their learned patterns mechanically, based on the structure learned in training. This allows them to perform exceptionally well within familiar constraints, boundaries and conditoins, while while behaving unpredictably when those conditions change.

Models can recognize patterns, make estimates, and scale decisions, but they do so within the boundaries defined by the data available, training procedures, and design choices of the models architect. 

### Logic

Data, and the models that result with training are often the most visible, and perhaps "coolest"  components of AI systems. But **logic** is what ultimately connects model outputs to real-world actions. Logic defines how predictions, scores, or classifications are interpreted and how they are translated into actions. 

Logic is the **rules, thresholds, constraints, and objectives** that govern a systems behavior. These elements specify what should happen when a model produces a given output. In a system designed to detect credit card fraud, a model may provide a probability score that fraud has occured. This score might then be compared against some threshold, the level at which the busines would like to take action on the prediction, and an alert triggered. These thresholds are not inherent to the model; the model jsut outputs the prediction. These thresholds are design choices that reflect priorities, trade-offs, and risk tolerance.

Logic also encodes **business or organizational constraints**. All businessnes and organizations face at least some constraints, things like resource limitations, regulatory requirements, fairness considerations, or cost structures. A model might identify many high-risk cases, but logic determines how many can realistically be acted upon, which cases are prioritized, and which actions are permissible. As a result, logic often mediates between what a model suggests and what an organization can or should do.

Logic can be implemented in various ways. In some systems, it takes the form of explicit rules written by humans, "when we see x we do y". In others, it can be nuanced, the logic embedded within optimization routines, data collection policies, or decision frameworks. Even when decisions by a system appear automated, they are often controlled by the logic layered on through past human judgments about acceptable or optimal outcomes.

**Models produce outputs, but logic determines actions**. 

### Where Systems Fail (Data, Model, Logic Layers)

Failures in analytics and AI systems rarely originate from a single cause. Instead, they tend to emerge from breakdowns at one or more layers of the the system: **data**, **models**, or **logic**. Understanding these layers, and the interactions between them, provide a structured way to diagnose system results and understan why a system produces s sub optimal outcome.

Failures at the **data layer**  occur when the information the system is training on is incomplete, too noisy, or no longer representative of the process you want to model. These issues may take the form of missing values, measurement errors, outdated records, or shifts in underlying patterns over time. Because models learn from historical data, weaknesses at this layer often propagate forward, limiting what the system can reasonably achieve regardless of model sophistication.

Failures at the **model layer** arise when the model itself is poorly matched to the problme, task or data available. This may involve using overly simplistic models that fail to capture comples but important relationships. Conservesly this is also sometimes the use of overly complex models that overfit historical patterns. Even well-designed models can fail when deployed in contexts that differ meaningfully from those seen during training.

Failures at the **logic layer** occur when model outputs are translated into decisions inappropriately. This is often simply poorly chosen or arbitrary thresholds, rigid rules that do not adapt to changing conditions, or decision criteria that prioritize the wrong objectives. In these cases, a model may be producing reasonable outputs, but the surrounding logic causes undesirable actions or missed opportunities.

These layers are closely intertwined. High data quality and a strong model can't save a system with seriously flawed logic. Likewise, careful logic cannot compensate for fundamentally uninformative or skewed data. 

Viewing failures through this layered lens encourages more precise diagnosis and more effective intervention. Rather than asking whether an AI system “works” or “does not work,” it becomes possible to ask *where* it is breaking down and *why*. This perspective supports more thoughtful system evaluation and more responsible use of analytics and AI in decision-making contexts.

## AI Paradigms Overview

<figure>
  <img src="../assets/images/1_qmb_nested_ai.svg"
       alt="Conceptual diagram showing AI as the broad area, with machine learning as a subset and deep learning as a further subset.">
  <figcaption>A useful shorthand: AI includes many approaches, with machine learning and deep learning representing increasingly specialized subsets.</figcaption>
</figure>


### Symbolic AI

Symbolic systems rely on **explicit representations of knowledge** and **rule-based reasoning** to perform tasks. Contrast that with the types of systems we have been discussing so far, that use pattern recognition from data. These systems operate by manipulating inputs (words, categories,  etc) according to predefined rules.

The main idea in this approach is that intelligent behavior can be produced by encoding expert knowledge directly, as a series of rules and decision points, into a system. This often takes the form of *if–then* rules.  For example, a symbolic system might contain rules such as: *if a customer is late on payment and has missed multiple deadlines, then flag the account for review*. Each rule reflects a human and/or expert judgment that has been translated into formal logic.

Symbolic AI systems tend to be **transparent and interpretable**. Because their reasoning process is explicitly defined, it is usually possible to trace back to how particular outcome was reached by the system. This makes these symbolic approaches attractive, and sometimes even required, in domains where explanations, compliance, or auditability are critical. They perform best in environments where the rules are stable and the the problem space and scope is well understood, and rarely changes. 

There are some obvious limitations. Writing and maintaining rules is labor-intensive, complex, and such systems almost always struggle to scale as complexity increases. They also perform poorly in settings characterized by ambiguity, noise, or high variability (change). 

While symbolic AI is no longer the dominant paradigm in many areas, it remains an important conceptual foundation. Many modern systems still rely on symbolic components for constraints, validation, and control, even when learning-based models are used elsewhere. Understanding symbolic AI helps clarify both the strengths of explicit reasoning and the challenges that motivated the development of data-driven approaches addressed in the next sections.

### Statistical and Machine Learning

Statistical and machine learning approaches to AI differ from symbolic systems in a fundamental way: rather than relying on explicitly programmed rules, they **learn patterns from data**. These approaches use historical observations to infer relationships between inputs and outputs, allowing systems to generalize to new, unseen cases without being told exactly how to respond in every situation.

At the heart of machine learning is the idea that regularities in data can be captured through mathematical models whose parameters are estimated from examples. During training, a model is exposed to data and adjusted so that its predictions align with observed outcomes as closely as possible. This process allows the system to adapt to complex patterns that would be difficult to specify manually using rules alone.

Machine learning methods are often categorized based on the type of feedback available during training. In **supervised learning**, the model is trained using labeled examples, where the correct output is known in advance. Common applications include classification and regression tasks, such as predicting customer churn or estimating demand. In **unsupervised learning**, the model works with unlabeled data to identify structure, such as clusters or latent patterns, without predefined outcomes. Both approaches are widely used in analytics and AI systems.

Compared to symbolic AI, statistical and machine learning systems are generally more flexible and scalable. They perform well in environments with large volumes of data and can adapt to subtle patterns and correlations. However, this flexibility comes with trade-offs. Learned models may be less transparent, and their behavior can be sensitive to the data used for training. As a result, understanding and validating model performance often requires careful evaluation rather than direct inspection of rules.

Importantly, statistical and machine learning approaches do not eliminate the need for human judgment. Choices about which data to use, which features to include, how to evaluate performance, and how to deploy model outputs remain human decisions. Machine learning shifts the burden of specification from rule-writing to **data curation and model design**, redefining where expertise is applied within AI systems.

This paradigm has become central to modern analytics and AI, forming the basis for many applications encountered in practice. It also provides the foundation for more advanced approaches, such as neural and deep learning, discussed next.

### Neural and Deep Learning

Neural and deep learning approaches extend statistical machine learning by focusing on **learning representations** directly from data. Rather than relying on hand-crafted features or simple functional forms, these models use layered computational structures—commonly referred to as *neural networks*—to transform raw inputs into increasingly abstract representations.

The key idea behind neural networks is inspired by, but not equivalent to, biological neurons. A neural network is composed of interconnected units that apply weighted combinations of inputs followed by nonlinear transformations. By stacking many such layers, deep learning models can capture complex patterns in high-dimensional data. This layered structure allows them to excel in tasks such as image recognition, speech processing, and natural language understanding, where relationships are difficult to specify explicitly.

One defining characteristic of deep learning is its ability to operate on **unstructured or semi-structured data**, including images, audio, and text. In these domains, traditional statistical models often require extensive feature engineering. Deep learning models, by contrast, can learn relevant representations automatically from large volumes of data, reducing the need for manual specification of features.

This capability comes with important trade-offs. Neural and deep learning models are typically **data-intensive and computationally demanding**. Training them often requires large datasets, specialized hardware, and careful tuning. They also tend to be less interpretable than simpler models, making it more difficult to explain why a particular output was produced. As a result, deployment of deep learning systems often involves additional monitoring, validation, and governance mechanisms.

Despite these challenges, neural and deep learning approaches have reshaped the AI landscape. Many contemporary systems—including speech recognition, computer vision applications, and large language models—are built on deep learning architectures. Understanding this paradigm helps clarify why modern AI systems can handle tasks that were previously infeasible, as well as why concerns about transparency, robustness, and control remain central to their use.

Neural and deep learning approaches are rarely used in isolation. In practice, they are often combined with statistical methods and symbolic logic to form integrated systems, a topic addressed next.

### Hybrid Systems in Practice

<figure>
  <img src="../assets/images/1_hybrid_paradigm.svg"
       alt="Hybrid AI paradigm diagram showing learned components, rule-based components, and human oversight combining into a decision layer.">
  <figcaption>Most real-world AI systems blend machine learning, rules, and human oversight rather than relying on a single paradigm.</figcaption>
</figure>


In real-world applications, AI systems rarely rely on a single paradigm. Instead, they are typically **hybrid systems** that combine symbolic reasoning, statistical or machine learning models, and neural or deep learning components. Each paradigm contributes different strengths, and hybrid designs allow systems to balance performance, interpretability, and control.

A common pattern in hybrid systems is the use of **learning-based models for perception and prediction**, paired with **symbolic or rule-based logic for decision-making and constraints**. For example, a deep learning model may be used to recognize objects in an image or extract meaning from text, while a rule-based layer determines whether the output meets regulatory requirements or triggers a specific action. In this structure, learning handles complexity and variability, while symbolic logic enforces consistency and accountability.

Hybrid systems also help address practical limitations of individual approaches. Machine learning models can adapt to data and capture subtle patterns, but they may behave unpredictably outside familiar conditions. Symbolic logic can impose guardrails, prevent certain actions, or require human review under specified circumstances. Statistical models can provide calibrated probabilities that support decision thresholds and prioritization. Together, these components form systems that are more robust than any single approach alone.

Many modern AI applications illustrate this hybrid structure. Recommendation systems often combine learned user preference models with business rules and inventory constraints. Fraud detection systems use predictive models to score transactions and rule-based logic to manage alerts and workflows. Large language model applications frequently pair neural models with retrieval systems, validation rules, and structured decision logic to ensure usable and reliable outputs.

Understanding AI systems as hybrids reinforces an important perspective: intelligence in practice is **distributed across system components**, not concentrated in a single model. Performance, reliability, and responsibility emerge from how data, models, and logic are assembled and governed. This systems-level view provides a foundation for analyzing and designing AI applications that operate effectively within real organizational and societal constraints.



