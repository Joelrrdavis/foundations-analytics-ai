[
  {
    "objectID": "guides/using_colab.html",
    "href": "guides/using_colab.html",
    "title": "Using Google Colab for This Course",
    "section": "",
    "text": "Google Colab is a free, cloud-based environment for running Python notebooks.\nYou can use it from any computer without installing Python locally. It is especially helpful if:\n\nYour computer cannot install packages reliably\n\nYou are troubleshooting your Python installation\n\nYou want quick access to a working Python environment\n\nThis guide explains how to open Colab, upload or access notebooks, install packages, work with files, and download your work."
  },
  {
    "objectID": "guides/using_colab.html#overview",
    "href": "guides/using_colab.html#overview",
    "title": "Using Google Colab for This Course",
    "section": "",
    "text": "Google Colab is a free, cloud-based environment for running Python notebooks.\nYou can use it from any computer without installing Python locally. It is especially helpful if:\n\nYour computer cannot install packages reliably\n\nYou are troubleshooting your Python installation\n\nYou want quick access to a working Python environment\n\nThis guide explains how to open Colab, upload or access notebooks, install packages, work with files, and download your work."
  },
  {
    "objectID": "guides/using_colab.html#what-is-google-colab",
    "href": "guides/using_colab.html#what-is-google-colab",
    "title": "Using Google Colab for This Course",
    "section": "1. What Is Google Colab?",
    "text": "1. What Is Google Colab?\nGoogle Colab provides:\n\nA hosted Python runtime (no setup required)\n\nBuilt-in Jupyter-like notebook interface\n\nFree GPU/TPU access (not required for this course but available)\n\nIntegration with Google Drive\n\nColab notebooks use the .ipynb format, but you can also copy/paste Python code from .py files."
  },
  {
    "objectID": "guides/using_colab.html#opening-google-colab",
    "href": "guides/using_colab.html#opening-google-colab",
    "title": "Using Google Colab for This Course",
    "section": "2. Opening Google Colab",
    "text": "2. Opening Google Colab\nGo to:\nhttps://colab.research.google.com/\nYou will see a window prompting you to:\n\nStart a new notebook\n\nUpload a .ipynb file\n\nOpen from Google Drive\n\nOpen from GitHub"
  },
  {
    "objectID": "guides/using_colab.html#option-a-create-a-new-notebook",
    "href": "guides/using_colab.html#option-a-create-a-new-notebook",
    "title": "Using Google Colab for This Course",
    "section": "3. Option A — Create a New Notebook",
    "text": "3. Option A — Create a New Notebook\n\nGo to File → New Notebook.\n\nA new notebook will open with a default Python environment.\n\nYou can copy/paste code from the course textbook or .py files.\n\nUse Shift + Enter to run a cell."
  },
  {
    "objectID": "guides/using_colab.html#option-b-upload-a-notebook-from-your-computer",
    "href": "guides/using_colab.html#option-b-upload-a-notebook-from-your-computer",
    "title": "Using Google Colab for This Course",
    "section": "4. Option B — Upload a Notebook from Your Computer",
    "text": "4. Option B — Upload a Notebook from Your Computer\n\nGo to File → Upload Notebook\n\nSelect the notebook you downloaded\n\nColab will open it in a new tab"
  },
  {
    "objectID": "guides/using_colab.html#option-c-open-a-notebook-directly-from-github",
    "href": "guides/using_colab.html#option-c-open-a-notebook-directly-from-github",
    "title": "Using Google Colab for This Course",
    "section": "5. Option C — Open a Notebook Directly from GitHub",
    "text": "5. Option C — Open a Notebook Directly from GitHub\n\nGo to File → Open Notebook\n\nChoose the GitHub tab\n\nPaste the repository URL\n\nSelect the notebook you want to open\n\nExample:\nhttps://github.com/&lt;your-username&gt;/undergrad-ai-course"
  },
  {
    "objectID": "guides/using_colab.html#installing-packages-in-colab",
    "href": "guides/using_colab.html#installing-packages-in-colab",
    "title": "Using Google Colab for This Course",
    "section": "6. Installing Packages in Colab",
    "text": "6. Installing Packages in Colab\nColab comes with many Python packages preinstalled.\nTo install additional packages:\n!pip install seaborn\nOr install from a requirements file:\n!pip install -r requirements.txt"
  },
  {
    "objectID": "guides/using_colab.html#working-with-files",
    "href": "guides/using_colab.html#working-with-files",
    "title": "Using Google Colab for This Course",
    "section": "7. Working with Files",
    "text": "7. Working with Files\n\nTemporary Upload\nfrom google.colab import files\nuploaded = files.upload()\n\n\nMount Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\nYour Drive will appear at:\n/content/drive/MyDrive/"
  },
  {
    "objectID": "guides/using_colab.html#saving-your-work",
    "href": "guides/using_colab.html#saving-your-work",
    "title": "Using Google Colab for This Course",
    "section": "8. Saving Your Work",
    "text": "8. Saving Your Work\n\nSave to Drive\nFile → Save a copy in Drive\n\n\nDownload\nFile → Download → .ipynb\nFile → Download → .py"
  },
  {
    "objectID": "guides/using_colab.html#limitations-of-colab",
    "href": "guides/using_colab.html#limitations-of-colab",
    "title": "Using Google Colab for This Course",
    "section": "9. Limitations of Colab",
    "text": "9. Limitations of Colab\n\nSessions time out\n\nTemporary files are deleted\n\nPackages must be reinstalled each session\n\nInternet connection required"
  },
  {
    "objectID": "guides/using_colab.html#summary",
    "href": "guides/using_colab.html#summary",
    "title": "Using Google Colab for This Course",
    "section": "10. Summary",
    "text": "10. Summary\nGoogle Colab allows you to:\n\nRun Python without installing anything\n\nUpload/open notebooks\n\nInstall packages\n\nWork with Drive\n\nDownload your work\n\nIt is a useful fallback or complementary environment for this course."
  },
  {
    "objectID": "guides/setup_python_uv_vscode.html",
    "href": "guides/setup_python_uv_vscode.html",
    "title": "Course Setup Guide: Python, uv, and VS Code",
    "section": "",
    "text": "This guide walks you through everything required to get set up for this course, end to end:\n\nInstalling Python\nInstalling uv\nInstalling and configuring VS Code\nCreating a project-based Python environment\nRunning code successfully\n\nThis is a one-time setup. Once completed, you will reuse this workflow all semester.\n\n\n\n\n\n\nNote\n\n\n\nRead this once, then follow it top-to-bottom.\nMost setup problems happen when steps are skipped or done out of order.\n\n\n\n\n\nThis guide assumes you know how to:\n- open Terminal (Mac) or PowerShell (Windows)\n- run a command\n- copy/paste commands\n- recognize when a command succeeds or fails\nIf not, complete the short Terminal Basics video and guide before continuing.\n\n\n\nBy the end of this guide, you should have a project folder that looks like this:\nCLASS_FOLDER/\n├── pyproject.toml\n├── .venv/\n└── hello.py\nAnd you should be able to: - open this folder in VS Code - run Python code using the project environment - install packages in a repeatable way\n\n\n\n\n\n\nTip\n\n\n\nIf your folder does not look like this at the end, stop and fix it before moving on."
  },
  {
    "objectID": "guides/setup_python_uv_vscode.html#overview",
    "href": "guides/setup_python_uv_vscode.html#overview",
    "title": "Course Setup Guide: Python, uv, and VS Code",
    "section": "",
    "text": "This guide walks you through everything required to get set up for this course, end to end:\n\nInstalling Python\nInstalling uv\nInstalling and configuring VS Code\nCreating a project-based Python environment\nRunning code successfully\n\nThis is a one-time setup. Once completed, you will reuse this workflow all semester.\n\n\n\n\n\n\nNote\n\n\n\nRead this once, then follow it top-to-bottom.\nMost setup problems happen when steps are skipped or done out of order.\n\n\n\n\n\nThis guide assumes you know how to:\n- open Terminal (Mac) or PowerShell (Windows)\n- run a command\n- copy/paste commands\n- recognize when a command succeeds or fails\nIf not, complete the short Terminal Basics video and guide before continuing.\n\n\n\nBy the end of this guide, you should have a project folder that looks like this:\nCLASS_FOLDER/\n├── pyproject.toml\n├── .venv/\n└── hello.py\nAnd you should be able to: - open this folder in VS Code - run Python code using the project environment - install packages in a repeatable way\n\n\n\n\n\n\nTip\n\n\n\nIf your folder does not look like this at the end, stop and fix it before moving on."
  },
  {
    "objectID": "guides/setup_python_uv_vscode.html#phase-1-install-required-tools",
    "href": "guides/setup_python_uv_vscode.html#phase-1-install-required-tools",
    "title": "Course Setup Guide: Python, uv, and VS Code",
    "section": "Phase 1 — Install Required Tools",
    "text": "Phase 1 — Install Required Tools\n\n1. Install Python (3.xx)\nThis course works with Python 3.12 or higher.\nExamples below use Python 3.13.\n\n\n\n\n\n\nImportant\n\n\n\nIf you already have Python installed, you still need to verify the version and that it runs from the terminal.\n\n\n\nmacOS\n\nDownload Python from https://www.python.org\n\nRun the installer (accept defaults)\n\nVerify in Terminal:\n\npython3 --version\npip3 --version\n\n\nWindows\n\nDownload Python from https://www.python.org\n\nCheck “Add Python to PATH” during install\n\nVerify in PowerShell:\n\npython --version\npip --version\n\n\n\n\n\n\nWarning\n\n\n\nIf python or pip is “not recognized,” restart your terminal first.\nIf it still fails, Python was not added to PATH.\n\n\n\n\n\n\n2. Install uv\nuv manages your project, environment, and packages.\n\n\n\n\n\n\nNote\n\n\n\nDo not install uv using plain pip install uv.\nModern systems intentionally block this to protect your Python installation.\n\n\n\nmacOS\n\nOption A — Homebrew (if you already use it)\nbrew install uv\nuv --version\n\n\nOption B — pipx (no Homebrew required)\npython3 -m pip install --user pipx\npython3 -m pipx ensurepath\nRestart your terminal, then:\npipx install uv\nuv --version\n\n\n\n\nWindows\n\nOption A — winget\nwinget install uv\nuv --version\n\n\nOption B — pipx\npython -m pip install --user pipx\npython -m pipx ensurepath\npipx install uv\nuv --version\n\n\n\n\n\n\nTip\n\n\n\nIf uv is “not found,” close and reopen your terminal.\nThis fixes most PATH-related issues.\n\n\n\n\n\n\n\n3. Install VS Code\n\nDownload from https://code.visualstudio.com\n\nInstall using defaults\n\nOn Windows, check Add to PATH\n\nOpen VS Code\n\n\nInstall the Python extension\n\nExtensions → search Python\nInstall Python (by Microsoft)\n\n\n\n\n\n\n\nImportant\n\n\n\nVS Code will not work correctly for Python until the Python extension is installed."
  },
  {
    "objectID": "guides/setup_python_uv_vscode.html#phase-2-create-your-course-project",
    "href": "guides/setup_python_uv_vscode.html#phase-2-create-your-course-project",
    "title": "Course Setup Guide: Python, uv, and VS Code",
    "section": "Phase 2 — Create Your Course Project",
    "text": "Phase 2 — Create Your Course Project\n\n4. Create a project folder\nChoose a location (Documents recommended).\n\nmacOS\nmkdir -p ~/Documents/CLASS_FOLDER\ncd ~/Documents/CLASS_FOLDER\n\n\nWindows\nmkdir $HOME\\Documents\\CLASS_FOLDER\ncd $HOME\\Documents\\CLASS_FOLDER\n\n\n\n\n\n\nNote\n\n\n\nAvoid spaces in folder names.\nUse letters, numbers, and hyphens only.\n\n\n\n\n\n\n5. Initialize the project\nuv init\nuv venv\nYou should now see: - pyproject.toml - .venv/\n\n\n\n\n\n\nWarning\n\n\n\nIf you do not see .venv, you are not in the correct folder.\n\n\n\n\n\n6. Add a test package\nuv add rich\n\n\n\n\n\n\nTip\n\n\n\nUsing uv add both installs the package and records it in pyproject.toml.\n\n\n\n\n\n7. Create a test file\nCreate hello.py:\nfrom rich import print\nprint(\"[bold green]Hello from Python + uv + VS Code![/bold green]\")"
  },
  {
    "objectID": "guides/setup_python_uv_vscode.html#phase-3-connect-vs-code-to-the-project",
    "href": "guides/setup_python_uv_vscode.html#phase-3-connect-vs-code-to-the-project",
    "title": "Course Setup Guide: Python, uv, and VS Code",
    "section": "Phase 3 — Connect VS Code to the Project",
    "text": "Phase 3 — Connect VS Code to the Project\n\n8. Open the folder in VS Code\n\nFile → Open Folder…\nSelect CLASS_FOLDER\n\n\n\n\n\n\n\nImportant\n\n\n\nOpen the folder, not just the Python file.\nVS Code needs the folder to find .venv.\n\n\n\n\n\n9. Select the Python interpreter\n\nCommand Palette\n\nmacOS: Cmd + Shift + P\n\nWindows: Ctrl + Shift + P\n\nChoose Python: Select Interpreter\nSelect the interpreter inside .venv\n\n\n\n\n\n\n\nWarning\n\n\n\nThe wrong interpreter is the #1 cause of “ModuleNotFoundError.”"
  },
  {
    "objectID": "guides/setup_python_uv_vscode.html#phase-4-run-code-choose-one-style",
    "href": "guides/setup_python_uv_vscode.html#phase-4-run-code-choose-one-style",
    "title": "Course Setup Guide: Python, uv, and VS Code",
    "section": "Phase 4 — Run Code (Choose One Style)",
    "text": "Phase 4 — Run Code (Choose One Style)\nYou only need one of the following approaches.\nPick one and use it consistently.\n\n\nOption A — Activation style (recommended for beginners)\n\nmacOS\nsource .venv/bin/activate\npython hello.py\n\n\nWindows\n. .\\.venv\\Scripts\\Activate.ps1\npython hello.py\nIf PowerShell blocks activation:\nSet-ExecutionPolicy -Scope CurrentUser RemoteSigned\n\n\n\n\n\n\nNote\n\n\n\nThis setting applies only to your user account\nand is commonly required for Python environments.\n\n\n\n\n\n\nOption B — No activation (uv-managed)\nuv run python hello.py\n\n\n\n\n\n\nTip\n\n\n\nThis avoids activation entirely and is often easier on Windows."
  },
  {
    "objectID": "guides/setup_python_uv_vscode.html#troubleshooting",
    "href": "guides/setup_python_uv_vscode.html#troubleshooting",
    "title": "Course Setup Guide: Python, uv, and VS Code",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\n“ModuleNotFoundError”\n\n\n\n\n\n\nWarning\n\n\n\nThis almost always means Python is running outside the project environment.\n\n\nChecklist: - VS Code interpreter is set to .venv - You are in the project folder - You activated .venv or used uv run\n\n\n\n“uv: command not found”\n\nRestart your terminal\nRe-run pipx ensurepath (pipx installs)\n\n\n\n\nFinal checkpoint\n\n\n\n\n\n\nImportant\n\n\n\nDo not move on until all of the following work.\n\n\n\npython --version\nuv --version\nhello.py runs successfully\nVS Code shows the .venv interpreter"
  },
  {
    "objectID": "textbook_src/ch5_python_foundations_data_control_functions.html#python-basics-variables-and-data-types",
    "href": "textbook_src/ch5_python_foundations_data_control_functions.html#python-basics-variables-and-data-types",
    "title": "Python Foundations: Data, Control, and Functions",
    "section": "Python Basics: Variables and Data Types",
    "text": "Python Basics: Variables and Data Types\nThis section discusses variables and introduces Python’s core data types. The goal is to move from thinking of variables as simple containers to understanding how data type influences program behavior, interpretation, and outcomes.\n\nVariables revisited\nA variable in Python is a name that refers to a value. When a variable is created, Python associates that name with an object stored in memory. This association can change over time as the program runs.\nOne important idea to reinforce is that variables are not fixed containers. They are labels that can be reassigned to different values:\nx = 5\nx = 10\nprint(x)\nAfter this code runs, the value associated with x is 10, not 5. The original value is no longer referenced by x. This ability to reassign variables is what allows programs to update state, respond to inputs, and evolve during execution.\nIt can sometimes be helpful to think about variables as a pointer. A variable my point at a value, but it can be changed to point at a different value.\nVariables also help make programs easier to understand. Using meaningful variable names communicates intent and reduces the need to mentally track raw values.\nThe key takeaway is that variables:\n\nbind names to values,\n\ncan be reassigned,\n\nand allow programs to remember and update information as they run.\n\n\n\n\nNumbers and numeric behavior\nPython supports several numeric data types, the most common being integers (whole numbers) and floating-point numbers (numbers with decimal points). While they may look similar, they behave differently in certain operations.\nFor example:\na = 10\nb = 3\nprint(a / b)\nEven though both a and b are integers, the result of division is a floating-point number. Python automatically determines the appropriate type based on the operation being performed.\nThis automatic behavior is convenient, but it also means that numeric results may not always be what you expect at first glance. Floating-point numbers, in particular, can introduce small rounding effects due to how they are represented internally.\nThe most important ideas are:\n\nPython distinguishes between integers and floats,\n\narithmetic operations can change types,\n\nand numeric behavior is governed by both the values and the operation.\n\nUnderstanding these basics helps prevent confusion later when numeric results appear slightly different than anticipated.\n\n\n\nStrings as data\nA string is a sequence of characters used to represent text. Strings are created by enclosing characters in quotation marks:\ntext = \"analytics\"\nStrings behave differently from numbers. Although they may look similar when printed, Python treats them as text rather than quantities. Strings can be indexed, meaning individual characters can be accessed by position:\nprint(text[0])\nHere, Python returns the first character of the string. Indexing begins at zero, which is a common convention in programming.\nAn important property of strings is that they are immutable. Once a string is created, its individual characters cannot be changed. Operations on strings create new strings rather than modifying existing ones.\nStrings are used extensively in analytics and AI workflows to represent labels, categories, identifiers, and unstructured text. Treating strings as data rather than just “things to print” is an important conceptual shift.\n\n\n\nBooleans and logical values\nA boolean represents one of two logical values: True or False. Booleans often arise from comparisons, where Python evaluates whether a statement is correct:\nx = 5\nprint(x &gt; 3)\nIn this example, the comparison produces a boolean result. Booleans are fundamental to decision-making in programs because they control whether certain code paths are executed.\nAlthough booleans are simple, they play a central role in program logic. They act as the bridge between data and behavior, determining how a program responds to different conditions.\nLater sections will build on booleans to introduce conditional logic and loops. For now, the key idea is that booleans encode yes/no decisions that programs can act upon.\n\n\n\nType behavior and common surprises\nPython uses dynamic typing, meaning that variable types are determined at runtime rather than declared in advance. A variable can be reassigned to a value of a different type without error:\nx = 5\nx = \"five\"\nprint(x)\nThis flexibility is powerful, but it can also lead to surprises, especially when mixing types in expressions. For example:\nprint(\"5\" + \"5\")\nprint(5 + 5)\nAlthough both lines use the + operator, they behave differently. In the first case, Python performs string concatenation. In the second, it performs numeric addition. The operator’s meaning depends on the data types involved.\nThese behaviors are not bugs; they are consistent rules applied by Python. Understanding them requires paying attention to type, not just appearance.\nCommon beginner surprises often come from assuming that values that look similar behave the same way. Developing the habit of asking “what type is this?” is one of the most effective ways to reason about Python programs and avoid errors."
  },
  {
    "objectID": "textbook_src/ch5_python_foundations_data_control_functions.html#working-with-strings",
    "href": "textbook_src/ch5_python_foundations_data_control_functions.html#working-with-strings",
    "title": "Python Foundations: Data, Control, and Functions",
    "section": "Working with Strings",
    "text": "Working with Strings\nStrings are one of the most common data types used in Python. They represent text, but they are also structured objects that can be indexed, sliced, transformed, and formatted. In analytics and AI contexts, strings are frequently used to represent labels, categories, identifiers, and unstructured text, making it important to understand how to work with them beyond simple printing.\nThis section builds fluency in treating strings as data rather than as static text.\n\n\nString creation and indexing\nA string is created by enclosing characters in quotation marks. Python allows both single and double quotation marks, as long as they are used consistently:\ntext = \"analytics\"\nlabel = 'AI'\nOnce created, a string behaves like a sequence of characters. This means each character has a position, known as an index. Python uses zero-based indexing, so the first character is at position 0, the second at position 1, and so on.\nCharacters can be accessed using square brackets:\ntext = \"analytics\"\nprint(text[0])\nprint(text[1])\nPython also allows negative indexing, which counts from the end of the string. An index of -1 refers to the last character:\nprint(text[-1])\nIndexing allows programs to inspect or extract specific parts of a string, but it must be done carefully. Attempting to access an index that does not exist results in an error. This reinforces the idea that strings have a fixed length and well-defined boundaries.\nAt a conceptual level, indexing answers the question: Which character is at this position in the string?\n\n\n\nString slicing\nWhile indexing accesses a single character, slicing extracts a range of characters from a string. A slice is specified using a start position and an end position. The start position is included, while the end position is excluded.\nFor example:\ntext = \"analytics\"\nprint(text[0:4])\nThis slice returns the characters at positions 0 through 3. Python also allows either the start or end to be omitted, which defaults to the beginning or end of the string:\nprint(text[:4])\nprint(text[4:])\nSlicing always returns a new string. The original string remains unchanged, which reflects the immutable nature of strings.\nSlicing is often safer and more expressive than manual indexing, especially when working with variable-length strings. Rather than counting exact positions, slices allow code to express intent more clearly, such as “everything before this point” or “everything after that point.”\nAt a conceptual level, slicing answers the question: Which portion of this string do I want to work with?\n\n\n\nCommon string operations and methods\nPython provides many built-in operations and methods for working with strings. These allow programs to measure, transform, and search text without modifying the original string.\nOne common operation is measuring the length of a string:\ntext = \"Analytics and AI\"\nprint(len(text))\nOther common operations involve transforming the string, such as changing letter case:\nprint(text.lower())\nprint(text.upper())\nStrings can also be searched or modified using methods that return new strings:\nprint(text.replace(\"AI\", \"analytics\"))\nThese methods do not alter the original string. Instead, they produce a new string with the requested changes applied.\nBecause strings are immutable, all transformations result in new objects. This behavior is consistent and predictable, but it also means that results must be captured in variables if they are needed later.\nString operations are widely used in data cleaning, labeling, and preprocessing tasks. Understanding how these methods behave helps prevent subtle bugs and makes string manipulation more intentional.\n\n\n\nFormatting strings for output\nIn most programs, strings are not used in isolation. They are combined with variables to produce readable output for users, logs, or reports. String formatting is the process of embedding variable values into text.\nOne common and modern approach is the use of formatted strings, often called f-strings. These allow variables to be inserted directly into a string:\nname = \"Jordan\"\nscore = 92\nprint(f\"{name} scored {score} points\")\nFormatted strings improve readability by keeping text and values together in a single expression. They also reduce the need for manual concatenation, which can become error-prone as output becomes more complex.\nFormatting matters because output is often how results are interpreted by humans. Clear, well-formatted strings make it easier to understand what a program has done and what its results mean."
  },
  {
    "objectID": "textbook_src/ch5_python_foundations_data_control_functions.html#data-structures-lists",
    "href": "textbook_src/ch5_python_foundations_data_control_functions.html#data-structures-lists",
    "title": "Python Foundations: Data, Control, and Functions",
    "section": "Data Structures: Lists",
    "text": "Data Structures: Lists\nLists allow programs to work with collections of values rather than single items. Instead of storing one number, one string, or one result at a time, lists make it possible to store many related values together and treat them as a unit. This is essential for working with real-world data, where information almost always comes in groups.\n\n\nWhat a list is and when to use one\nA list is an ordered collection of values. Each value in the list occupies a position, and those positions are preserved. This ordering allows values to be accessed, updated, and processed in a predictable way.\nLists are useful whenever a program needs to work with:\n- multiple related values,\n- values that should be processed together,\n- or data whose size may change over time.\nFor example, a list can represent a set of scores, a group of names, or a sequence of measurements. Rather than creating separate variables for each value, a list groups them into a single structure.\nnumbers = [1, 2, 3, 4]\nConceptually, lists answer the question: How do I represent many values as one thing? This makes lists a foundational structure for iteration, aggregation, and analysis.\n\n\n\nCreating lists\nLists are created using square brackets, with individual values separated by commas. A list can contain values of the same type or a mixture of different types.\nnames = [\"Alice\", \"Bob\", \"Charlie\"]\nmixed = [1, \"two\", True]\nKeep in mind that although Python allows mixed-type lists, using a consistent type within a list often improves readability and reduces confusion. For example, a list of numbers or a list of strings communicates intent more clearly than a list with unrelated values.\nLists can also be created empty and filled later as the program runs. This pattern is especially useful when values are generated dynamically.\nresults = []\nAt a conceptual level, creating a list establishes a container, a place to put things later, whose contents can grow, shrink, or change as the program executes.\n\n\n\nIndexing and slicing lists\nLike strings, lists are sequences, which means their elements are accessed using zero-based indexing. The first element is at index 0, the second at index 1, and so on. In plain english- Python starts counting at 0. The best way to understand this is to run a lot of examples and look at the results.\nvalues = [10, 20, 30, 40, 50]\nprint(values[0])\nNegative indexing also works with lists, allowing access from the end:\nprint(values[-1])\nLists support slicing, which extracts a portion of the list and returns a new list containing those elements.\nprint(values[1:4])\nSlicing behavior for lists closely mirrors slicing for strings, but instead of returning a string, Python returns a list. The original list remains unchanged.\nIndexing and slicing allow programs to focus on specific elements or subsets of data, which is especially useful when analyzing or transforming collections.\n\n\n\nModifying list contents\nOne key difference between lists and strings is that lists are mutable. This means their contents can be changed after the list is created. This in contrast to something that is immutable in Python, which can’t be changed.\nIndividual elements can be updated by assigning a new value to a specific index:\nvalues = [10, 20, 30]\nvalues[1] = 25\nElements can also be added to a list. A common pattern is appending new values to the end:\nvalues.append(40)\nRemoving elements is possible, either by value or by position. Because lists can change over time, they are well suited for workflows where data is accumulated, filtered, or updated incrementally.\nMutability is powerful, but it also means that changes to a list affect all parts of the program that reference it.\n\n\n\nCommon list patterns and mistakes\nLists are frequently used in patterns that involve building up data step by step. A common approach is to start with an empty list and add values as they are produced.\nresults = []\nresults.append(10)\nresults.append(20)\nOne common mistake involves indexing errors, such as attempting to access an index that does not exist. These errors usually arise from forgetting that indexing starts at zero or miscounting list length.\nAnother frequent source of confusion involves references. When one variable is assigned to another, both names may refer to the same list rather than creating a copy.\na = [1, 2, 3]\nb = a\nb.append(4)\nIn this example, both a and b refer to the same list. Modifying the list through one variable affects the other. This behavior is consistent but can be surprising if it is not anticipated.\nDeveloping a habit of thinking carefully about list creation, modification, and referencing helps prevent subtle bugs. Lists are powerful tools, but they require attention to how and when data changes."
  },
  {
    "objectID": "textbook_src/ch5_python_foundations_data_control_functions.html#data-structures-dictionaries",
    "href": "textbook_src/ch5_python_foundations_data_control_functions.html#data-structures-dictionaries",
    "title": "Python Foundations: Data, Control, and Functions",
    "section": "Data Structures: Dictionaries",
    "text": "Data Structures: Dictionaries\nDictionaries provide a way to store structured, labeled data. While lists organize values by position, dictionaries organize values by meaning. This makes dictionaries especially useful for representing real-world entities, records, and attributes where each value has a clear label.\nThis section introduces dictionaries as a complementary data structure to lists, emphasizing when dictionaries are the better choice and how they support clearer, more expressive programs.\n\n\nWhy dictionaries exist\nLists are effective when values are naturally ordered and accessed by position. However, many real-world data problems are not about position; they are about association. For example, a person is not best described by “the first value, the second value, and the the third value,” but by attributes such as name, age, or location.\nDictionaries exist to solve this problem. A dictionary allows values to be accessed using keys that describe what the values represent. This makes programs easier to read and reason about, especially as the number of attributes grows.\nperson = {\"name\": \"Alex\", \"age\": 30}\nIn this example, each value is paired with a descriptive label. The dictionary structure makes it immediately clear what each value represents, without relying on positional knowledge.\nConceptually, dictionaries answer the question: How do I store related pieces of information under meaningful names? This makes them a natural choice for representing records, configurations, and structured inputs.\n\n\n\nKey–value pairs\nA dictionary is composed of key–value pairs. The key acts as an identifier, and the value is the data associated with that identifier. Together, they form a mapping from meaning to data.\nrecord = {\"city\": \"Gainesville\", \"state\": \"FL\"}\nKeys are typically strings, although Python allows other immutable types to be used as keys. The important property is that keys must be unique within a dictionary. Each key identifies exactly one value.\nAccessing data through keys is fundamentally different from indexing into a list. Instead of asking “what is at position 0?” the program asks “what is the value associated with this label?” This shift from positional access to semantic access improves clarity and reduces errors.\nAt a conceptual level, key–value pairs encode relationships: this label corresponds to that value.\n\n\n\nCreating and accessing dictionaries\nDictionaries are created using curly braces, with key–value pairs separated by commas. Each key is followed by a colon and its associated value.\nscores = {\"math\": 90, \"history\": 85}\nValues are accessed by referencing the key:\nprint(scores[\"math\"])\nIf the specified key exists, Python returns the associated value. If the key does not exist, Python raises an error. This behavior reinforces the idea that keys define the valid structure of the data.\nDictionary access is explicit and intentional. Unlike lists, where accessing an invalid index might result from a miscount, accessing a dictionary with a missing key usually indicates a mismatch between what the program expects and what the data contains.\nUnderstanding how dictionaries are created and accessed helps establish a clear mental model of structured data: keys define what can be asked, and values define what is returned.\n\n\n\nUpdating and iterating over dictionaries\nDictionaries are mutable, meaning their contents can be changed after creation. Existing values can be updated by assigning a new value to an existing key:\nscores[\"math\"] = 92\nNew key–value pairs can also be added dynamically:\nscores[\"science\"] = 88\nThis flexibility allows dictionaries to evolve as a program runs, making them well suited for tasks where information is accumulated, updated, or refined over time.\nDictionaries are often iterated over to process their contents. While full loop syntax is introduced later, it is important to recognize that dictionaries can be traversed by their keys, values, or key–value pairs. This makes it possible to perform operations across structured data in a systematic way.\nBecause dictionaries are mutable and often shared across a program, changes to a dictionary affect all references to it. This behavior is powerful but requires careful reasoning about when and where updates occur.\n\n\n\nStructured data with dictionaries\nOne of the most common uses of dictionaries is to represent records—collections of related attributes that describe a single entity. For example, a dictionary can represent a student, a transaction, or a configuration setting.\nstudent = {\"name\": \"Alex\", \"score\": 90}\nWhen working with multiple records, it is common to use lists of dictionaries, where each dictionary represents one structured item:\nstudents = [\n    {\"name\": \"Alex\", \"score\": 90},\n    {\"name\": \"Jordan\", \"score\": 85}\n]\nThis pattern bridges the gap between simple Python data structures and more advanced representations such as tables, data frames, and JSON objects. It is widely used in data processing, APIs, and machine learning pipelines.\nDictionaries matter because they allow programs to work with data in a way that mirrors real-world structure. Instead of relying on position or implicit meaning, dictionaries make relationships explicit. This clarity becomes increasingly important as programs grow in size and complexity."
  },
  {
    "objectID": "textbook_src/ch5_python_foundations_data_control_functions.html#control-flow-conditional-logic",
    "href": "textbook_src/ch5_python_foundations_data_control_functions.html#control-flow-conditional-logic",
    "title": "Python Foundations: Data, Control, and Functions",
    "section": "Control Flow: Conditional Logic",
    "text": "Control Flow: Conditional Logic\nPython executes code sequentially from top to bottom, unless control structures such as conditionals or loops explicitly alter the flow of execution.\nConditional logic allows programs to make decisions. Without conditional logic, a program would execute the same instructions in the same order every time, regardless of what data it receives. Real programs do not work that way; they respond differently depending on inputs, context, and state.\nThis section introduces conditional logic as a fundamental mechanism for decision-making in programs.\n\n\nWhy programs need branching\nMany early programming examples look like calculators: values go in, a result comes out, and the program ends. That is useful for learning syntax, but it does not capture how most programs behave in practice. This is not to say you shouldn’t start with these simple examples, you absolutely should start here.\nReal programs must handle situations where different cases require different actions. For example:\n\nIf a user enters invalid input, the program should respond differently than if the input is valid.\n\nIf a customer’s risk score is high, the program may trigger review, while a low score may be ignored.\n\nIf a file exists, the program may load it; otherwise it may create a new one.\n\nThis is the purpose of branching. Branching means that a program can follow one of multiple paths depending on a condition.\nConditional logic is also the foundation for many forms of decision logic in analytics and AI systems. Models may produce scores or probabilities, but something still has to decide what happens next. That decision is often implemented using thresholds and conditional statements. Even simple branching logic is therefore a core building block for larger systems.\n\n\n\nIf, elif, and else\nIn Python, branching is expressed using the if statement. An if statement evaluates a condition. If the condition is true, Python executes the block of code under the if. If the condition is false, Python skips that block.\nA minimal example looks like this:\nx = 10\nif x &gt; 5:\n    print(\"Large value\")\nThe condition is x &gt; 5. Python evaluates it as either true or false. If it is true, the indented block runs.\nOften, programs need to handle more than two cases. Python supports this using elif (short for “else if”). The program checks conditions in order, and the first condition that evaluates to true determines which block runs.\nscore = 85\nif score &gt;= 90:\n    print(\"A\")\nelif score &gt;= 80:\n    print(\"B\")\nelse:\n    print(\"Below B\")\nIn this structure:\n\nif checks the first condition.\nelif checks additional conditions only if earlier ones were false.\nelse acts as a fallback when no conditions were met.\n\nA useful way to read this is: “check the first condition; if it fails, check the next; if all fail, use the default.”\n\n\n\nBoolean expressions in conditions\nConditions in if statements are boolean expressions. A boolean expression is any expression that evaluates to either True or False.\nThe most common boolean expressions are comparisons. Python supports standard comparison operators such as:\n\ngreater than and less than,\n\nequality and inequality,\n\ngreater than or equal to, less than or equal to.\n\nFor example:\nx = 5\nprint(x == 5)\nprint(x &gt; 10)\nThese expressions evaluate to booleans. That is why they can be used in conditions: the if statement is ultimately asking a yes/no question.\nA common source of confusion is that conditions often look like natural language, but they are not. They are strict expressions that must evaluate cleanly to true or false. Small mistakes in a condition can change program behavior dramatically, so it is important to write conditions clearly and test them when needed.\nAt this stage, the key idea is simple: conditional logic works because Python evaluates conditions as booleans and then decides which block of code to execute.\n\n\n\nNested conditionals and readability\nA nested conditional is an if statement inside another if statement. Nesting is sometimes necessary when decisions depend on multiple layers of conditions.\nage = 20\nif age &gt;= 18:\n    if age &lt; 21:\n        print(\"Adult, but under 21\")\nThis structure expresses two related conditions:\n\nThe outer condition checks whether the person is an adult.\n\nThe inner condition refines the case for adults under 21.\n\nNesting can be useful, but it can also reduce readability if overused. Deeply nested logic is harder to follow, harder to debug, and easier to misunderstand. As conditional logic becomes more complex, readability becomes a design concern, not just a style preference.\nTwo practical habits help keep conditionals readable:\n\nUse clear conditions that communicate intent.\n\nAvoid unnecessary nesting when a simpler structure is possible.\n\nEven when nesting is appropriate, indentation must be treated as part of the logic. In Python, indentation is not cosmetic; it determines which code belongs to which branch. Learning to visually interpret indentation is therefore part of learning how conditional logic works."
  },
  {
    "objectID": "textbook_src/ch5_python_foundations_data_control_functions.html#control-flow-loops",
    "href": "textbook_src/ch5_python_foundations_data_control_functions.html#control-flow-loops",
    "title": "Python Foundations: Data, Control, and Functions",
    "section": "Control Flow: Loops",
    "text": "Control Flow: Loops\nLoops allow programs to repeat actions systematically. Instead of writing the same instruction multiple times, loops provide a way to apply the same logic across collections of data or across repeated conditions. This ability to repeat is essential for working with real data, where the number of values is often large or unknown in advance.\n\n\nWhy repetition matters\nWithout loops, programs would be limited to one-time calculations. Any task that involved processing multiple values would require duplicated code, which is inefficient, error-prone, and difficult to maintain.\nMany real-world tasks involve repetition:\n\nChecking each item in a list.\n\nProcessing each record in a dataset.\n\nRepeating an action until a condition is met.\n\nAccumulating results across many values.\n\nLoops make these tasks possible by allowing a program to apply the same operation repeatedly, while only writing the logic once.\nConceptually, loops are closely tied to data structures. When data is stored in collections such as lists or dictionaries, loops provide the mechanism to visit each element in turn. This connection between structure (data) and behavior (loops) is central to programming and analytics.\nAt a high level, loops answer the question:\nHow do I apply the same logic across many values or over time?\n\n\n\nFor loops and iteration patterns\nA for loop is used when a program needs to iterate over a collection of values. The loop variable takes on each value in the collection, one at a time, and the loop body runs once for each value.\nvalues = [10, 20, 30]\nfor v in values:\n    print(v)\nThis loop can be read as: “for each value in the list, print the value.” The loop variable v is assigned a new value on each iteration.\nFor loops work naturally with sequences such as lists and strings. The number of iterations is determined by the size of the collection, which makes for loops predictable and easy to reason about.\nA helpful habit is to read for loops out loud in plain language. Doing so reinforces the intent of the loop and reduces confusion about what the code is doing.\nFor loops are especially useful when:\n\nthe collection is known,\n\nevery element should be processed,\n\nand the order of processing matters.\n\n\n\n\nWhile loops and termination conditions\nA while loop repeats as long as a condition remains true. Instead of iterating over a collection, a while loop continues until a specific condition changes.\ncount = 0\nwhile count &lt; 3:\n    print(count)\n    count = count + 1\nThis loop runs while the condition count &lt; 3 is true. Each iteration updates count, and eventually the condition becomes false, causing the loop to stop.\nWhile loops are useful when:\n\nthe number of iterations is not known in advance,\n\nrepetition depends on a changing condition,\n\nor the loop should stop until some state is reached.\n\nThe most important concept with while loops is termination. Every while loop must include logic that eventually makes the condition false. Without this, the loop will continue indefinitely.\nConceptually, while loops answer the question:\nShould I keep going right now?\n\n\n\nCommon looping errors\nLoops are powerful, but they also introduce common sources of error. One of the most frequent mistakes is creating an infinite loop, where the condition never becomes false.\nDon’t run this!\ncount = 0\nwhile count &lt; 3:\n    print(count)\n    # count is never updated\nIn this example, the condition remains true forever because count never changes. Infinite loops often occur when a loop variable is not updated or when the termination condition is incorrect.\nAnother common issue is off-by-one errors, where a loop runs one time too many or one time too few. These errors often arise from misunderstandings about starting values, ending conditions, or zero-based indexing.\nLoops can also behave unexpectedly when data is modified while being iterated over. Changing a list while looping through it can lead to skipped values or unintended behavior, which is why careful reasoning about loop structure is important.\nWhen debugging loops, a useful strategy is to:\n\ntrace the loop step by step,\n\ntrack how loop variables change,\n\nand verify exactly when the condition becomes false.\n\n\n\n\nChoosing between for and while\nBoth for and while loops enable repetition, but they serve different purposes. Choosing between them is a matter of intent, not preference.\nA for loop is usually the better choice when:\n\niterating over a known collection,\n\napplying logic to each element,\n\nor when the number of iterations is determined by the data.\n\nA while loop is usually the better choice when:\n\nrepetition depends on a condition,\nthe number of iterations is not known ahead of time,\nor the loop should stop based on changing state.\n\n# for loop: known collection\nfor x in [1, 2, 3]:\n    print(x)\n\n# while loop: condition-based repetition\ncount = 0\nwhile count &lt; 3:\n    print(count)\n    count = count + 1\nIn many cases, either loop could be used, but one will communicate intent more clearly than the other. Readability and correctness are more important than cleverness.\nUnderstanding when and why to use each type of loop makes programs easier to understand, debug, and extend. Loops are not just a syntactic feature; they are a way of expressing repeated reasoning in code."
  },
  {
    "objectID": "textbook_src/ch5_python_foundations_data_control_functions.html#functions-and-modular-design",
    "href": "textbook_src/ch5_python_foundations_data_control_functions.html#functions-and-modular-design",
    "title": "Python Foundations: Data, Control, and Functions",
    "section": "Functions and Modular Design",
    "text": "Functions and Modular Design\nAs programs grow, writing code from top to bottom becomes difficult to manage. Logic is repeated, scripts become long, and small changes require edits in multiple places. Functions address this problem by allowing programs to be broken into reusable, named units of logic.\nThis section introduces functions not just as a Python feature, but as a way of thinking about program design. Functions help manage complexity, improve readability, and make programs easier to reason about and extend.\n\n\nWhy functions exist\nOne of the earliest signs that a program needs functions is repetition. When the same logic appears multiple times, it becomes harder to maintain and easier to introduce errors.\nprint(\"Processing value\")\nprint(\"Processing value\")\nIf the message or behavior needs to change, every repeated instance must be updated. Functions solve this by allowing logic to be written once and reused many times.\nFunctions also support abstraction. By giving a block of code a meaningful name, the details of how something is done can be hidden behind what it does. This allows programs to be read at a higher level, focusing on intent rather than mechanics.\nConceptually, functions answer the question:\nHow can I name and reuse a piece of behavior?\nThis idea scales from small scripts to large systems. In analytics and AI workflows, functions often represent steps in a pipeline, transformations applied to data, or decisions applied consistently across many values.\n\n\n\nDefining a function\nA function is defined using the def keyword, followed by the function name, parentheses, and a colon. The body of the function is indented beneath the definition.\ndef greet():\n    print(\"Hello\")\nThis code defines a function named greet, but it does not execute it. Defining a function tells Python what the function does, not when it should run.\nThe function body contains the instructions that will be executed whenever the function is called. Indentation is critical here: it determines which statements belong to the function.\nFunction names should be descriptive and reflect behavior. A well-named function makes code easier to read, especially when functions are combined into larger programs.\nA useful way to read a function definition is:\n“Define a function called greet that performs these actions.”\n\n\n\nCalling functions\nA function runs only when it is called. Calling a function means telling Python to execute the instructions inside the function body.\ngreet()\ngreet()\nEach time the function is called, the same block of code runs. After the function finishes executing, control returns to the point where it was called.\nThis separation between definition and execution is essential. It allows functions to be defined once and used many times, in different parts of a program, or under different conditions.\nUnderstanding the difference between defining a function and calling a function is one of the most important conceptual steps in learning Python. Many early errors come from assuming that defining a function automatically runs it.\n\n\n\nParameters and return values\nFunctions become more powerful when they can accept inputs and produce outputs. Inputs are specified using parameters, and outputs are produced using return values.\ndef add(a, b):\n    return a + b\nIn this example, a and b are parameters. When the function is called, concrete values—called arguments—are passed in.\nresult = add(2, 3)\nThe function computes a value and returns it to the caller. The returned value can then be stored in a variable, used in expressions, or passed to other functions.\nReturning values is often preferable to printing results inside functions. Printing is useful for communication with a user, but return values allow functions to be composed and reused as part of larger computations.\nConceptually, a function with parameters and a return value represents a transformation: it takes inputs, applies logic, and produces an output.\n\n\n\nFunctions as building blocks\nAs programs grow, functions act as building blocks that can be combined with loops, conditionals, and data structures. Each function handles a specific task, making the overall program easier to understand.\ndef is_passing(score):\n    return score &gt;= 70\nThis function encodes a simple rule. Instead of repeating the condition score &gt;= 70 throughout a program, the logic is captured once and reused wherever needed.\nFunctions also improve testability and debugging. When behavior is isolated inside a function, it can be tested independently. If something goes wrong, the scope of the problem is smaller and easier to locate.\nAt a higher level, functions support modular design. Programs can be understood as collections of interacting functions, each with a clear purpose. This mirrors how larger analytics and AI systems are structured, where components are designed to do one thing well and interact through well-defined interfaces.\nThinking in terms of functions encourages a shift from writing code that merely works to writing code that is understandable, maintainable, and scalable.\nOrganizing logic into functions supports modular design, making programs easier to understand, reuse, test, and debug as they grow."
  },
  {
    "objectID": "textbook_src/ch5_python_foundations_data_control_functions.html#debugging-fundamentals",
    "href": "textbook_src/ch5_python_foundations_data_control_functions.html#debugging-fundamentals",
    "title": "Python Foundations: Data, Control, and Functions",
    "section": "Debugging Fundamentals",
    "text": "Debugging Fundamentals\nDebugging is the process of identifying, understanding, and correcting problems in code. It is not a special activity reserved for advanced programmers, nor is it a sign that something has gone wrong in learning. Debugging is how programming actually happens.\nThis section reframes errors as information and introduces debugging as a systematic reasoning process, not a guessing game.\n\n\nWhat an error message is telling you\nWhen Python encounters a problem it cannot resolve, it produces an error message. That message is Python’s way of explaining what it expected, what it encountered instead, and where the problem occurred.\nAn error message is not a judgment about your ability. It is a report. Learning to debug begins with learning to read error messages rather than avoiding them.\nSome errors occur before the program runs at all. These are typically syntax errors, where Python cannot understand the structure of the code. For example:\nprint(\"Hello\"\nIn this case, Python reaches the end of the line and realizes something is missing. The error message points to the location where Python became confused.\nOther errors occur while the program is running. These runtime errors indicate that Python understood the code structurally, but something went wrong during execution.\nAt a high level, error messages answer three questions:\n\nWhere did the problem occur?\n\nWhat kind of problem was it?\n\nWhat was Python trying to do at the time?\n\nDebugging begins by treating error messages as clues rather than obstacles.\n\n\n\nReading a traceback from top to bottom\nWhen a runtime error occurs, Python often produces a traceback. A traceback shows the sequence of steps Python followed before encountering the error.\ndef divide(a, b):\n    return a / b\n\ndivide(10, 0)\nThe traceback lists function calls from top to bottom, ending with the line where the error actually occurred. While the traceback may look intimidating at first, it follows a consistent structure.\nA useful strategy is to:\n\nskim the traceback to understand the context,\n\nthen focus on the last line, which usually describes the actual error.\n\nThe file name and line number tell you where Python was executing when the problem occurred. This information narrows the search space and prevents unnecessary changes elsewhere in the code.\nOver time, reading tracebacks becomes a skill. Instead of seeing a wall of text, you begin to recognize patterns and quickly identify the relevant information.\n\n\n\nCommon beginner errors\nCertain errors appear frequently when learning Python. These errors are predictable and shared by almost everyone at this stage.\nSyntax errors occur when Python cannot parse the code structure. Missing quotation marks, parentheses, or colons are common causes.\nName errors occur when a variable or function is used before it has been defined. This often results from spelling mistakes or assumptions about what exists in the current scope.\nType errors occur when an operation is applied to incompatible data types. These errors highlight the importance of understanding how different types behave.\nIndex and key errors occur when attempting to access elements that do not exist.\nvalues = [1, 2, 3]\nprint(values[3])\nThis error does not mean the list is broken. It means the program asked for something outside the valid range.\nRecognizing these errors as categories rather than isolated failures helps reduce frustration and speeds up debugging.\n\n\n\nDebugging as a systematic process\nEffective debugging is not about trying random fixes. It is about narrowing the problem space and testing assumptions deliberately.\nA systematic debugging process usually involves:\n\nreproducing the error consistently,\n\nidentifying the exact location of failure,\n\ninspecting the values of variables at that point,\n\nand changing one thing at a time.\n\nOne of the simplest and most effective debugging tools is printing intermediate values.\nprint(\"Current value:\", x)\nBy inspecting program state as it runs, it becomes easier to understand how data flows through the code and where expectations diverge from reality.\nChanging multiple things at once makes debugging harder. When only one change is made, its effect is easier to interpret.\nDebugging rewards patience and methodical thinking. The goal is not to fix the error as quickly as possible, but to understand why the error occurred.\n\n\n\nWhen to fix code vs. rethink logic\nNot all bugs are caused by incorrect syntax or misuse of language features. Sometimes the code runs exactly as written, but the result is still wrong. In these cases, the issue lies in the logic, not the implementation.\ndef is_valid(score):\n    return score &gt; 100\nThis function works exactly as defined, but the condition may not reflect the intended rule. Debugging logic errors requires stepping back and examining assumptions.\nA useful question to ask is: Is Python doing something unexpected, or is Python doing exactly what I told it to do?\nIf the latter is true, the solution may involve redesigning conditions, rethinking data structures, or clarifying the problem definition rather than fixing syntax.\nDebugging is therefore part of program design. It is how understanding improves over time. Learning when to adjust code and when to rethink logic is one of the most valuable skills you can develop as you write and refine programs."
  },
  {
    "objectID": "textbook_src/ch5_python_foundations_data_control_functions.html#chapter-summary",
    "href": "textbook_src/ch5_python_foundations_data_control_functions.html#chapter-summary",
    "title": "Python Foundations: Data, Control, and Functions",
    "section": "Chapter Summary",
    "text": "Chapter Summary\nThis chapter focused on building foundational Python skills by moving from individual values to structured data, decision-making, repetition, and modular program design. The goal was not just to learn Python syntax, but to develop a mental model of how programs reason about data and control their own behavior.\nConceptually, the chapter emphasized that data representation matters. Numbers, strings, booleans, lists, and dictionaries are not interchangeable; each type carries assumptions about how it can be used, combined, and transformed. Understanding these differences is essential for writing programs that behave predictably and for interpreting results correctly in analytics and AI contexts.\nThe introduction of control flow—conditional logic and loops—marked a shift from linear execution to decision-making and repetition. Programs were no longer treated as calculators that run once, but as systems that respond differently depending on conditions and that can operate over collections of data. This logic-based view of execution directly connects to how larger decision systems and algorithms function.\nFunctions extended this idea by introducing modularity and abstraction. By encapsulating logic into reusable units, programs become easier to read, test, and extend. Functions also reinforce the idea that complex behavior is built from smaller, well-defined components—a principle that scales from simple scripts to full analytics and AI systems.\nFinally, the chapter reframed debugging as a core reasoning skill rather than a remedial activity. Error messages, tracebacks, and unexpected results were treated as sources of information that guide understanding. Learning to debug systematically—by isolating problems, checking assumptions, and distinguishing between syntax issues and logic flaws—is a skill that will be used continuously in later chapters.\nBy the end of this chapter, you should be able to write Python programs that store and manipulate structured data, make decisions, repeat actions, and organize logic into functions. More importantly, you should be developing confidence in reading, reasoning about, and correcting code—skills that form the foundation for more advanced analytics and AI techniques."
  },
  {
    "objectID": "textbook_src/ch2_ai_systems_data_models_logic.html",
    "href": "textbook_src/ch2_ai_systems_data_models_logic.html",
    "title": "AI Systems: Data, Models, and Logic",
    "section": "",
    "text": "alt=“Core AI system diagram showing a left-to-right flow: Data feeds into Model, Model feeds into Logic, and Logic triggers Action. Arrows indicate sequential flow from inputs to decisions.”&gt;\n\nAn AI system is best understood as the interaction of data, a model, and decision logic—rather than as a model alone.\n\n\n\n\nData forms the foundation of all analytics and AI systems. Regardless of how sophisticated a model or decision framework may be, the behavior of the system is entirely shaped by the data it has availble. Understanding what data represents, how it is generated, and how it enters a system is a critical step on the way to understanding how AI systems operate.\nVery broadly defined, data is recorded observations about the world. These observations take many forms: transaction records, sensor readings, text documents, images, user interactions, or system logs. These data sources might all differ in structure and complexity, but they all capture past events or states that can be analyzed, modeled, and in some cases acted upon.\nData serves two very critical functions within AI systems. First, it is used to train models. Historical data provides the needed examples from which AI models learn patterns, relationships, or representations. The coverage and quality of this data directly influence what a model can learn and how well it generalizes beyond its training examples. Second, data is used during system operation, when new observations are provided as input to a trained model in order to generate predictions, classifications, or scores. The key thing to remember here is that errors, shifts, or inconsistencies in either training data or operational data can degrade system performance.\nFor the most part, data is not a complete or neutral representation of reality. Data always reflects the processes through which it was collected. These include factors such as organizational priorities, technical constraints, and human choices. Some phenomena are simply easier to observe and record than others, some groups or behaviors are overepresented, and some variables serve only as indirect proxies for what is actually of interest. As a result, data commonly contains noise, omissions, and systematic issues that need to be addressed.\nFrom a systems perspective, data does not simply exist, ready to be used by whatever model is in place; it is acquired and prepared, in a process that is often referred to or thought of as a pipelines. These pipelines involve decisions about what to collect, how frequently to collect it, how it is stored, and how it is cleaned or transformed before use. Choices made at this stage, such as how missing values are handled or how categories are encoded, can have consequences that propagate throughout the system. Often in surprising ways.\nEffective analysis of AI systems begins with careful attention to the data being collected. Asking where data comes from, what it represents, and what it leaves out is often the most, and sometimes the only, reliable way to understand system behavior, anticipate limitations, and diagnose failures.\nA defining challenge for artificial intelligence systems is that they must operate under uncertainty. Data is often incomplete, noisy, delayed, or sometimes just plain wrong. Models can only approximate real-world processes. AI models don’t try to eliminating uncertainty, but these systems are designed to manage and act despite it, using probabilistic reasoning, learned patterns, and decision logic to function in the imperfect environments they are placed in.\n\n\n\nA model is a formal representation of a relationship between inputs and outputs. In analytics and AI systems, models are used to map the observed data to predictions, classifications, scores, or other quantities that support decision-making. While models can take many forms—from simple equations to complex neural networks—their role within a system is conceptually consistent: they provide a structured way to generalize from past observations to new situations.\nModels differ from raw data in an important way. Data records what has already happened. The key word to remember there is “already”. All data is from the past. we can’t collect data on the future. Models on the other hand encode assumptions about how the world works. The assumptions the model makes might explicit. Very clear assumptions that are easily interpretable, as in a linear equation that specifies how inputs specifically combine to produce some output. Or the model might be more implicit, as in a deep learning model that learns internal representations through training on large datasets . In both cases, the model embodies a hypothesis about underlying patterns found in the data.\nModels are created through a training process. Historical data is used to adjust a model’s parameters so that its outputs align as closely as possible with observed outcomes. This process allows the model to capture regularities in the data, but it also ties the model’s behavior to the quality and scope of the data it was trained on. A model cannot reliably learn patterns that are absent, rare, or systematically distorted in the training data.\nOnce trained, a model is given new outputs, and used the representation that is learned in training, to generate an output. These outputs are often probabilistic rather than deterministic, the model can’t “know” that the process it followed to map the input data and adjust it’s parameters is accurate or without loss. Instead of producing a single “correct” answer, a model may estimate the likelihood of different outcomes or assign scores that reflect a relative confidence in a range of outputs. This probabilistic nature is a strength as it allows models to operate under a high level of uncertainty.\nTalking about models this way is perhaps a little dangerous. Models are not inherently intelligent or autonomous. They do not understand context, intent, or consequences in a human sense. Instead, they apply their learned patterns mechanically, based on the structure learned in training. This allows them to perform exceptionally well within familiar constraints, boundaries and conditoins, while while behaving unpredictably when those conditions change.\nModels can recognize patterns, make estimates, and scale decisions, but they do so within the boundaries defined by the data available, training procedures, and design choices of the models architect.\n\n\n\nData, and the models that result with training are often the most visible, and perhaps “coolest” components of AI systems. But logic is what ultimately connects model outputs to real-world actions. Logic defines how predictions, scores, or classifications are interpreted and how they are translated into actions.\nLogic is the rules, thresholds, constraints, and objectives that govern a systems behavior. These elements specify what should happen when a model produces a given output. In a system designed to detect credit card fraud, a model may provide a probability score that fraud has occured. This score might then be compared against some threshold, the level at which the busines would like to take action on the prediction, and an alert triggered. These thresholds are not inherent to the model; the model jsut outputs the prediction. These thresholds are design choices that reflect priorities, trade-offs, and risk tolerance.\nLogic also encodes business or organizational constraints. All businessnes and organizations face at least some constraints, things like resource limitations, regulatory requirements, fairness considerations, or cost structures. A model might identify many high-risk cases, but logic determines how many can realistically be acted upon, which cases are prioritized, and which actions are permissible. As a result, logic often mediates between what a model suggests and what an organization can or should do.\nLogic can be implemented in various ways. In some systems, it takes the form of explicit rules written by humans, “when we see x we do y”. In others, it can be nuanced, the logic embedded within optimization routines, data collection policies, or decision frameworks. Even when decisions by a system appear automated, they are often controlled by the logic layered on through past human judgments about acceptable or optimal outcomes.\nModels produce outputs, but logic determines actions.\n\n\n\nFailures in analytics and AI systems rarely originate from a single cause. Instead, they tend to emerge from breakdowns at one or more layers of the the system: data, models, or logic. Understanding these layers, and the interactions between them, provide a structured way to diagnose system results and understan why a system produces s sub optimal outcome.\nFailures at the data layer occur when the information the system is training on is incomplete, too noisy, or no longer representative of the process you want to model. These issues may take the form of missing values, measurement errors, outdated records, or shifts in underlying patterns over time. Because models learn from historical data, weaknesses at this layer often propagate forward, limiting what the system can reasonably achieve regardless of model sophistication.\nFailures at the model layer arise when the model itself is poorly matched to the problme, task or data available. This may involve using overly simplistic models that fail to capture comples but important relationships. Conservesly this is also sometimes the use of overly complex models that overfit historical patterns. Even well-designed models can fail when deployed in contexts that differ meaningfully from those seen during training.\nFailures at the logic layer occur when model outputs are translated into decisions inappropriately. This is often simply poorly chosen or arbitrary thresholds, rigid rules that do not adapt to changing conditions, or decision criteria that prioritize the wrong objectives. In these cases, a model may be producing reasonable outputs, but the surrounding logic causes undesirable actions or missed opportunities.\nThese layers are closely intertwined. High data quality and a strong model can’t save a system with seriously flawed logic. Likewise, careful logic cannot compensate for fundamentally uninformative or skewed data.\nViewing failures through this layered lens encourages more precise diagnosis and more effective intervention. Rather than asking whether an AI system “works” or “does not work,” it becomes possible to ask where it is breaking down and why. This perspective supports more thoughtful system evaluation and more responsible use of analytics and AI in decision-making contexts."
  },
  {
    "objectID": "textbook_src/ch2_ai_systems_data_models_logic.html#data-models-and-logic-core-components-of-ai-systems",
    "href": "textbook_src/ch2_ai_systems_data_models_logic.html#data-models-and-logic-core-components-of-ai-systems",
    "title": "AI Systems: Data, Models, and Logic",
    "section": "",
    "text": "alt=“Core AI system diagram showing a left-to-right flow: Data feeds into Model, Model feeds into Logic, and Logic triggers Action. Arrows indicate sequential flow from inputs to decisions.”&gt;\n\nAn AI system is best understood as the interaction of data, a model, and decision logic—rather than as a model alone.\n\n\n\n\nData forms the foundation of all analytics and AI systems. Regardless of how sophisticated a model or decision framework may be, the behavior of the system is entirely shaped by the data it has availble. Understanding what data represents, how it is generated, and how it enters a system is a critical step on the way to understanding how AI systems operate.\nVery broadly defined, data is recorded observations about the world. These observations take many forms: transaction records, sensor readings, text documents, images, user interactions, or system logs. These data sources might all differ in structure and complexity, but they all capture past events or states that can be analyzed, modeled, and in some cases acted upon.\nData serves two very critical functions within AI systems. First, it is used to train models. Historical data provides the needed examples from which AI models learn patterns, relationships, or representations. The coverage and quality of this data directly influence what a model can learn and how well it generalizes beyond its training examples. Second, data is used during system operation, when new observations are provided as input to a trained model in order to generate predictions, classifications, or scores. The key thing to remember here is that errors, shifts, or inconsistencies in either training data or operational data can degrade system performance.\nFor the most part, data is not a complete or neutral representation of reality. Data always reflects the processes through which it was collected. These include factors such as organizational priorities, technical constraints, and human choices. Some phenomena are simply easier to observe and record than others, some groups or behaviors are overepresented, and some variables serve only as indirect proxies for what is actually of interest. As a result, data commonly contains noise, omissions, and systematic issues that need to be addressed.\nFrom a systems perspective, data does not simply exist, ready to be used by whatever model is in place; it is acquired and prepared, in a process that is often referred to or thought of as a pipelines. These pipelines involve decisions about what to collect, how frequently to collect it, how it is stored, and how it is cleaned or transformed before use. Choices made at this stage, such as how missing values are handled or how categories are encoded, can have consequences that propagate throughout the system. Often in surprising ways.\nEffective analysis of AI systems begins with careful attention to the data being collected. Asking where data comes from, what it represents, and what it leaves out is often the most, and sometimes the only, reliable way to understand system behavior, anticipate limitations, and diagnose failures.\nA defining challenge for artificial intelligence systems is that they must operate under uncertainty. Data is often incomplete, noisy, delayed, or sometimes just plain wrong. Models can only approximate real-world processes. AI models don’t try to eliminating uncertainty, but these systems are designed to manage and act despite it, using probabilistic reasoning, learned patterns, and decision logic to function in the imperfect environments they are placed in.\n\n\n\nA model is a formal representation of a relationship between inputs and outputs. In analytics and AI systems, models are used to map the observed data to predictions, classifications, scores, or other quantities that support decision-making. While models can take many forms—from simple equations to complex neural networks—their role within a system is conceptually consistent: they provide a structured way to generalize from past observations to new situations.\nModels differ from raw data in an important way. Data records what has already happened. The key word to remember there is “already”. All data is from the past. we can’t collect data on the future. Models on the other hand encode assumptions about how the world works. The assumptions the model makes might explicit. Very clear assumptions that are easily interpretable, as in a linear equation that specifies how inputs specifically combine to produce some output. Or the model might be more implicit, as in a deep learning model that learns internal representations through training on large datasets . In both cases, the model embodies a hypothesis about underlying patterns found in the data.\nModels are created through a training process. Historical data is used to adjust a model’s parameters so that its outputs align as closely as possible with observed outcomes. This process allows the model to capture regularities in the data, but it also ties the model’s behavior to the quality and scope of the data it was trained on. A model cannot reliably learn patterns that are absent, rare, or systematically distorted in the training data.\nOnce trained, a model is given new outputs, and used the representation that is learned in training, to generate an output. These outputs are often probabilistic rather than deterministic, the model can’t “know” that the process it followed to map the input data and adjust it’s parameters is accurate or without loss. Instead of producing a single “correct” answer, a model may estimate the likelihood of different outcomes or assign scores that reflect a relative confidence in a range of outputs. This probabilistic nature is a strength as it allows models to operate under a high level of uncertainty.\nTalking about models this way is perhaps a little dangerous. Models are not inherently intelligent or autonomous. They do not understand context, intent, or consequences in a human sense. Instead, they apply their learned patterns mechanically, based on the structure learned in training. This allows them to perform exceptionally well within familiar constraints, boundaries and conditoins, while while behaving unpredictably when those conditions change.\nModels can recognize patterns, make estimates, and scale decisions, but they do so within the boundaries defined by the data available, training procedures, and design choices of the models architect.\n\n\n\nData, and the models that result with training are often the most visible, and perhaps “coolest” components of AI systems. But logic is what ultimately connects model outputs to real-world actions. Logic defines how predictions, scores, or classifications are interpreted and how they are translated into actions.\nLogic is the rules, thresholds, constraints, and objectives that govern a systems behavior. These elements specify what should happen when a model produces a given output. In a system designed to detect credit card fraud, a model may provide a probability score that fraud has occured. This score might then be compared against some threshold, the level at which the busines would like to take action on the prediction, and an alert triggered. These thresholds are not inherent to the model; the model jsut outputs the prediction. These thresholds are design choices that reflect priorities, trade-offs, and risk tolerance.\nLogic also encodes business or organizational constraints. All businessnes and organizations face at least some constraints, things like resource limitations, regulatory requirements, fairness considerations, or cost structures. A model might identify many high-risk cases, but logic determines how many can realistically be acted upon, which cases are prioritized, and which actions are permissible. As a result, logic often mediates between what a model suggests and what an organization can or should do.\nLogic can be implemented in various ways. In some systems, it takes the form of explicit rules written by humans, “when we see x we do y”. In others, it can be nuanced, the logic embedded within optimization routines, data collection policies, or decision frameworks. Even when decisions by a system appear automated, they are often controlled by the logic layered on through past human judgments about acceptable or optimal outcomes.\nModels produce outputs, but logic determines actions.\n\n\n\nFailures in analytics and AI systems rarely originate from a single cause. Instead, they tend to emerge from breakdowns at one or more layers of the the system: data, models, or logic. Understanding these layers, and the interactions between them, provide a structured way to diagnose system results and understan why a system produces s sub optimal outcome.\nFailures at the data layer occur when the information the system is training on is incomplete, too noisy, or no longer representative of the process you want to model. These issues may take the form of missing values, measurement errors, outdated records, or shifts in underlying patterns over time. Because models learn from historical data, weaknesses at this layer often propagate forward, limiting what the system can reasonably achieve regardless of model sophistication.\nFailures at the model layer arise when the model itself is poorly matched to the problme, task or data available. This may involve using overly simplistic models that fail to capture comples but important relationships. Conservesly this is also sometimes the use of overly complex models that overfit historical patterns. Even well-designed models can fail when deployed in contexts that differ meaningfully from those seen during training.\nFailures at the logic layer occur when model outputs are translated into decisions inappropriately. This is often simply poorly chosen or arbitrary thresholds, rigid rules that do not adapt to changing conditions, or decision criteria that prioritize the wrong objectives. In these cases, a model may be producing reasonable outputs, but the surrounding logic causes undesirable actions or missed opportunities.\nThese layers are closely intertwined. High data quality and a strong model can’t save a system with seriously flawed logic. Likewise, careful logic cannot compensate for fundamentally uninformative or skewed data.\nViewing failures through this layered lens encourages more precise diagnosis and more effective intervention. Rather than asking whether an AI system “works” or “does not work,” it becomes possible to ask where it is breaking down and why. This perspective supports more thoughtful system evaluation and more responsible use of analytics and AI in decision-making contexts."
  },
  {
    "objectID": "textbook_src/ch2_ai_systems_data_models_logic.html#ai-paradigms-overview",
    "href": "textbook_src/ch2_ai_systems_data_models_logic.html#ai-paradigms-overview",
    "title": "AI Systems: Data, Models, and Logic",
    "section": "AI Paradigms Overview",
    "text": "AI Paradigms Overview\n\n\n\nA useful shorthand: AI includes many approaches, with machine learning and deep learning representing increasingly specialized subsets.\n\n\n\nSymbolic AI\nSymbolic systems rely on explicit representations of knowledge and rule-based reasoning to perform tasks. Contrast that with the types of systems we have been discussing so far, that use pattern recognition from data. These systems operate by manipulating inputs (words, categories, etc) according to predefined rules.\nThe main idea in this approach is that intelligent behavior can be produced by encoding expert knowledge directly, as a series of rules and decision points, into a system. This often takes the form of if–then rules. For example, a symbolic system might contain rules such as: if a customer is late on payment and has missed multiple deadlines, then flag the account for review. Each rule reflects a human and/or expert judgment that has been translated into formal logic.\nSymbolic AI systems tend to be transparent and interpretable. Because their reasoning process is explicitly defined, it is usually possible to trace back to how particular outcome was reached by the system. This makes these symbolic approaches attractive, and sometimes even required, in domains where explanations, compliance, or auditability are critical. They perform best in environments where the rules are stable and the the problem space and scope is well understood, and rarely changes.\nThere are some obvious limitations. Writing and maintaining rules is labor-intensive, complex, and such systems almost always struggle to scale as complexity increases. They also perform poorly in settings characterized by ambiguity, noise, or high variability (change).\nWhile symbolic AI is no longer the dominant paradigm in many areas, it remains an important conceptual foundation. Many modern systems still rely on symbolic components for constraints, validation, and control, even when learning-based models are used elsewhere. Understanding symbolic AI helps clarify both the strengths of explicit reasoning and the challenges that motivated the development of data-driven approaches addressed in the next sections.\n\n\nStatistical and Machine Learning\nStatistical and machine learning approaches to AI differ from symbolic systems in a fundamental way: rather than relying on explicitly programmed rules, they learn patterns from data. These approaches use historical observations to infer relationships between inputs and outputs, allowing systems to generalize to new, unseen cases without being told exactly how to respond in every situation.\nAt the heart of machine learning is the idea that regularities in data can be captured through mathematical models whose parameters are estimated from examples. During training, a model is exposed to data and adjusted so that its predictions align with observed outcomes as closely as possible. This process allows the system to adapt to complex patterns that would be difficult to specify manually using rules alone.\nMachine learning methods are often categorized based on the type of feedback available during training. In supervised learning, the model is trained using labeled examples, where the correct output is known in advance. Common applications include classification and regression tasks, such as predicting customer churn or estimating demand. In unsupervised learning, the model works with unlabeled data to identify structure, such as clusters or latent patterns, without predefined outcomes. Both approaches are widely used in analytics and AI systems.\nCompared to symbolic AI, statistical and machine learning systems are generally more flexible and scalable. They perform well in environments with large volumes of data and can adapt to subtle patterns and correlations. However, this flexibility comes with trade-offs. Learned models may be less transparent, and their behavior can be sensitive to the data used for training. As a result, understanding and validating model performance often requires careful evaluation rather than direct inspection of rules.\nImportantly, statistical and machine learning approaches do not eliminate the need for human judgment. Choices about which data to use, which features to include, how to evaluate performance, and how to deploy model outputs remain human decisions. Machine learning shifts the burden of specification from rule-writing to data curation and model design, redefining where expertise is applied within AI systems.\nThis paradigm has become central to modern analytics and AI, forming the basis for many applications encountered in practice. It also provides the foundation for more advanced approaches, such as neural and deep learning, discussed next.\n\n\nNeural and Deep Learning\nNeural and deep learning approaches extend statistical machine learning by focusing on learning representations directly from data. Rather than relying on hand-crafted features or simple functional forms, these models use layered computational structures—commonly referred to as neural networks—to transform raw inputs into increasingly abstract representations.\nThe key idea behind neural networks is inspired by, but not equivalent to, biological neurons. A neural network is composed of interconnected units that apply weighted combinations of inputs followed by nonlinear transformations. By stacking many such layers, deep learning models can capture complex patterns in high-dimensional data. This layered structure allows them to excel in tasks such as image recognition, speech processing, and natural language understanding, where relationships are difficult to specify explicitly.\nOne defining characteristic of deep learning is its ability to operate on unstructured or semi-structured data, including images, audio, and text. In these domains, traditional statistical models often require extensive feature engineering. Deep learning models, by contrast, can learn relevant representations automatically from large volumes of data, reducing the need for manual specification of features.\nThis capability comes with important trade-offs. Neural and deep learning models are typically data-intensive and computationally demanding. Training them often requires large datasets, specialized hardware, and careful tuning. They also tend to be less interpretable than simpler models, making it more difficult to explain why a particular output was produced. As a result, deployment of deep learning systems often involves additional monitoring, validation, and governance mechanisms.\nDespite these challenges, neural and deep learning approaches have reshaped the AI landscape. Many contemporary systems—including speech recognition, computer vision applications, and large language models—are built on deep learning architectures. Understanding this paradigm helps clarify why modern AI systems can handle tasks that were previously infeasible, as well as why concerns about transparency, robustness, and control remain central to their use.\nNeural and deep learning approaches are rarely used in isolation. In practice, they are often combined with statistical methods and symbolic logic to form integrated systems, a topic addressed next.\n\n\nHybrid Systems in Practice\n\n\n\nMost real-world AI systems blend machine learning, rules, and human oversight rather than relying on a single paradigm.\n\n\nIn real-world applications, AI systems rarely rely on a single paradigm. Instead, they are typically hybrid systems that combine symbolic reasoning, statistical or machine learning models, and neural or deep learning components. Each paradigm contributes different strengths, and hybrid designs allow systems to balance performance, interpretability, and control.\nA common pattern in hybrid systems is the use of learning-based models for perception and prediction, paired with symbolic or rule-based logic for decision-making and constraints. For example, a deep learning model may be used to recognize objects in an image or extract meaning from text, while a rule-based layer determines whether the output meets regulatory requirements or triggers a specific action. In this structure, learning handles complexity and variability, while symbolic logic enforces consistency and accountability.\nHybrid systems also help address practical limitations of individual approaches. Machine learning models can adapt to data and capture subtle patterns, but they may behave unpredictably outside familiar conditions. Symbolic logic can impose guardrails, prevent certain actions, or require human review under specified circumstances. Statistical models can provide calibrated probabilities that support decision thresholds and prioritization. Together, these components form systems that are more robust than any single approach alone.\nMany modern AI applications illustrate this hybrid structure. Recommendation systems often combine learned user preference models with business rules and inventory constraints. Fraud detection systems use predictive models to score transactions and rule-based logic to manage alerts and workflows. Large language model applications frequently pair neural models with retrieval systems, validation rules, and structured decision logic to ensure usable and reliable outputs.\nUnderstanding AI systems as hybrids reinforces an important perspective: intelligence in practice is distributed across system components, not concentrated in a single model. Performance, reliability, and responsibility emerge from how data, models, and logic are assembled and governed. This systems-level view provides a foundation for analyzing and designing AI applications that operate effectively within real organizational and societal constraints."
  },
  {
    "objectID": "textbook_src/ch3_data_pipelines_decision_frameworks.html",
    "href": "textbook_src/ch3_data_pipelines_decision_frameworks.html",
    "title": "AI Systems: Data, Models, and Logic",
    "section": "",
    "text": "An AI system is best understood as the interaction of data, a model, and decision logic—rather than as a model alone.\n\n\n\n\nData forms the foundation of all analytics and AI systems. Regardless of how sophisticated a model or decision framework may be, the system’s behavior is shaped by the data it has available. Understanding what data represents, how it is generated, and how it enters a system is a critical step toward understanding how AI systems operate.\nData takes many forms, such as transaction records, sensor readings, text documents, images, user interactions, or system logs. These sources differ in structure and complexity, but all capture past events or states that can be analyzed, modeled, and acted upon.\nData serves two very critical functions within AI systems. First, it is used to train models. Historical data provides the needed examples from which AI models learn patterns, relationships, or representations. The coverage and quality of this data directly influence what a model can learn and how well it generalizes beyond its training examples. Second, data is used during system operation, when new observations are provided as input to a trained model in order to generate predictions, classifications, or scores. The key thing to remember here is that errors, shifts, or inconsistencies in either training data or operational data can degrade system performance.\nFor the most part, data is not a complete or neutral representation of reality. Data always reflects the processes through which it was collected. These include factors such as organizational priorities, technical constraints, and human choices. Some phenomena are easier to observe and record than others, some groups or behaviors are overrepresented, and some variables serve only as indirect proxies for what is actually of interest. As a result, data commonly contains noise, omissions, and systematic issues that need to be addressed.\nFrom a systems perspective, data does not simply exist, ready to be used by whatever model is in place; it is acquired and prepared, in a process often referred to as a pipeline. These pipelines involve decisions about what to collect, how frequently to collect it, how it is stored, and how it is cleaned or transformed before use. Choices made at this stage can have consequences that propagate throughout the system. Often in surprising ways.\nPractical analysis of AI systems begins with careful attention to the data being collected. Asking where data comes from, what it represents, and what it leaves out is often the most, and sometimes the only, reliable way to understand system behavior, anticipate limitations, and diagnose failures.\nA defining challenge for artificial intelligence systems is operating under uncertainty. Data is often incomplete, noisy, delayed, or sometimes just plain wrong. Models can only approximate real-world processes. AI models don’t try to eliminate uncertainty; they’re designed to manage and act despite it, using probabilistic reasoning, learned patterns, and decision logic to operate in the imperfect environments they are placed in.\n\n\n\nA model is a formal representation of a relationship between inputs and outputs. In analytics and AI systems, models are used to map the observed data (inputs) to predictions, classifications, scores, or other quantities (outputs) that support decision-making. Models provide a structured way to generalize from past observations to new situations.\nData records what has already happened. The keyword to remember there is “already”. All data is from the past. We can’t collect data on the future. Models, on the other hand, encode assumptions about how the world works. The model’s assumptions might be explicit. Obvious assumptions that are easily interpretable, as in a linear equation that specifies how inputs specifically combine to produce some output. Alternatively, the model might be more implicit, as in a deep learning model that learns internal representations through training on large datasets.\nIn supervised learning models, data (the model’s input) is used to adjust a model’s parameters so that its outputs align as closely as possible with the observed outcomes. This training proceess allows the model to capture the relationships and patterns in the data. Of course this also ties the model’s behavior to the quality and scope of the data it was trained on. A model cannot reliably learn patterns that are absent, rare, or systematically distorted in the training data.\nOnce trained, a model is given *new inputs and uses the representation that is learned in training to generate an output. These outputs are often probabilistic rather than deterministic; the model can’t “know” that the process it followed to map the input data and adjust its parameters is accurate or without loss. Instead of producing a single “correct” answer, a model may estimate the likelihood of different outcomes or assign scores that reflect a relative confidence in a range of outputs. This probabilistic nature is a strength as it allows models to operate under a high level of uncertainty.\nTalking about models this way is perhaps a little dangerous. Models are not inherently intelligent or autonomous. They do not understand context, intent, or consequences in a human sense. Instead, they apply learned patterns mechanically, based on the structure learned in training. This allows them to perform exceptionally well within familiar contexts, constraints, boundaries, and conditions, while behaving unpredictably when those conditions change.\nModels can recognize patterns, make estimates, and scale decisions, but they do so within the boundaries defined by the data available, training procedures, and design choices of the model’s architect.\n\n\n\nData, and the models that result from training, are often the most visible and perhaps “coolest” components of AI systems. But logic is what ultimately connects model outputs to real-world actions. Logic defines how predictions, scores, or classifications are interpreted and how they are translated into actions.\nLogic is the rules, thresholds, constraints, and objectives that govern a system’s behavior. These elements specify what should happen when a model produces a given output. In a system designed to detect credit card fraud, a model may provide a probability score that fraud has occurred. This score might then be compared against some threshold, the level at which the business would like to take Action on the prediction, and an alert triggered. These thresholds are not inherent to the model; the model just outputs the prediction. These thresholds are design choices that reflect priorities, trade-offs, and risk tolerance.\nLogic also encodes business or organizational constraints. All businesses and organizations face at least some constraints, such as resource limitations, regulatory requirements, fairness considerations, or cost structures. A model might identify many high-risk cases, but logic determines how many can realistically be acted upon, which cases are prioritized, and which actions are permissible. As a result, logic often mediates between what a model suggests and what an organization can or should do.\nLogic can be implemented in various ways. In some systems, it takes the form of explicit rules written by humans, “when we see x, we do y. In others, it can be nuanced, the logic embedded within optimization routines, data collection policies, or decision frameworks. Even when decisions by a system appear automated, they are often controlled by the logic layered on through past human judgments about acceptable or optimal outcomes.\nModels produce outputs, but logic determines actions.\n\n\n\nFailures in analytics and AI systems tend to come from breakdowns at one or more layers of the system: data, models, or logic. Understanding these layers and the interactions between them provides a structured way to diagnose system results and understand why a system produces a sub-optimal outcome.\nMissing values, measurement errors, outdated records, or shifts in underlying patterns over time can cause failures in this data layer. Because models base their learning here, weaknesses or problems at this layer often propagate forward, limiting what the system can achieve, regardless of how sophisticated the model is\nFailures at the model layer arise when the model itself is poorly matched to the problem, task, or data available. This may involve using overly simplistic models that fail to capture complex but important relationships. Conservatively, this is also sometimes the use of overly complex models that overfit historical patterns. Even well-designed models can fail when deployed in contexts that differ meaningfully from those seen during training.\nPoorly chosen or arbitrary thresholds, rigid rules that do not adapt to changing conditions, or decision criteria that prioritize the wrong objectives can all lead to failures inte *logic layer**. In these cases, a model may be producing reasonable outputs, but the surrounding logic causes undesirable actions or missed opportunities.\nThese layers are closely intertwined. High data quality and a strong model can’t save a system with seriously flawed logic. Likewise, careful logic cannot compensate for fundamentally uninformative or skewed data.\nConsidering failures throughout these different layers, allows for more precide intervention when models go wrong. Rather than asking whether an AI system “works” or “does not work,” it becomes possible to ask where it is breaking down and why."
  },
  {
    "objectID": "textbook_src/ch3_data_pipelines_decision_frameworks.html#data-models-and-logic-core-components-of-ai-systems",
    "href": "textbook_src/ch3_data_pipelines_decision_frameworks.html#data-models-and-logic-core-components-of-ai-systems",
    "title": "AI Systems: Data, Models, and Logic",
    "section": "",
    "text": "An AI system is best understood as the interaction of data, a model, and decision logic—rather than as a model alone.\n\n\n\n\nData forms the foundation of all analytics and AI systems. Regardless of how sophisticated a model or decision framework may be, the system’s behavior is shaped by the data it has available. Understanding what data represents, how it is generated, and how it enters a system is a critical step toward understanding how AI systems operate.\nData takes many forms, such as transaction records, sensor readings, text documents, images, user interactions, or system logs. These sources differ in structure and complexity, but all capture past events or states that can be analyzed, modeled, and acted upon.\nData serves two very critical functions within AI systems. First, it is used to train models. Historical data provides the needed examples from which AI models learn patterns, relationships, or representations. The coverage and quality of this data directly influence what a model can learn and how well it generalizes beyond its training examples. Second, data is used during system operation, when new observations are provided as input to a trained model in order to generate predictions, classifications, or scores. The key thing to remember here is that errors, shifts, or inconsistencies in either training data or operational data can degrade system performance.\nFor the most part, data is not a complete or neutral representation of reality. Data always reflects the processes through which it was collected. These include factors such as organizational priorities, technical constraints, and human choices. Some phenomena are easier to observe and record than others, some groups or behaviors are overrepresented, and some variables serve only as indirect proxies for what is actually of interest. As a result, data commonly contains noise, omissions, and systematic issues that need to be addressed.\nFrom a systems perspective, data does not simply exist, ready to be used by whatever model is in place; it is acquired and prepared, in a process often referred to as a pipeline. These pipelines involve decisions about what to collect, how frequently to collect it, how it is stored, and how it is cleaned or transformed before use. Choices made at this stage can have consequences that propagate throughout the system. Often in surprising ways.\nPractical analysis of AI systems begins with careful attention to the data being collected. Asking where data comes from, what it represents, and what it leaves out is often the most, and sometimes the only, reliable way to understand system behavior, anticipate limitations, and diagnose failures.\nA defining challenge for artificial intelligence systems is operating under uncertainty. Data is often incomplete, noisy, delayed, or sometimes just plain wrong. Models can only approximate real-world processes. AI models don’t try to eliminate uncertainty; they’re designed to manage and act despite it, using probabilistic reasoning, learned patterns, and decision logic to operate in the imperfect environments they are placed in.\n\n\n\nA model is a formal representation of a relationship between inputs and outputs. In analytics and AI systems, models are used to map the observed data (inputs) to predictions, classifications, scores, or other quantities (outputs) that support decision-making. Models provide a structured way to generalize from past observations to new situations.\nData records what has already happened. The keyword to remember there is “already”. All data is from the past. We can’t collect data on the future. Models, on the other hand, encode assumptions about how the world works. The model’s assumptions might be explicit. Obvious assumptions that are easily interpretable, as in a linear equation that specifies how inputs specifically combine to produce some output. Alternatively, the model might be more implicit, as in a deep learning model that learns internal representations through training on large datasets.\nIn supervised learning models, data (the model’s input) is used to adjust a model’s parameters so that its outputs align as closely as possible with the observed outcomes. This training proceess allows the model to capture the relationships and patterns in the data. Of course this also ties the model’s behavior to the quality and scope of the data it was trained on. A model cannot reliably learn patterns that are absent, rare, or systematically distorted in the training data.\nOnce trained, a model is given *new inputs and uses the representation that is learned in training to generate an output. These outputs are often probabilistic rather than deterministic; the model can’t “know” that the process it followed to map the input data and adjust its parameters is accurate or without loss. Instead of producing a single “correct” answer, a model may estimate the likelihood of different outcomes or assign scores that reflect a relative confidence in a range of outputs. This probabilistic nature is a strength as it allows models to operate under a high level of uncertainty.\nTalking about models this way is perhaps a little dangerous. Models are not inherently intelligent or autonomous. They do not understand context, intent, or consequences in a human sense. Instead, they apply learned patterns mechanically, based on the structure learned in training. This allows them to perform exceptionally well within familiar contexts, constraints, boundaries, and conditions, while behaving unpredictably when those conditions change.\nModels can recognize patterns, make estimates, and scale decisions, but they do so within the boundaries defined by the data available, training procedures, and design choices of the model’s architect.\n\n\n\nData, and the models that result from training, are often the most visible and perhaps “coolest” components of AI systems. But logic is what ultimately connects model outputs to real-world actions. Logic defines how predictions, scores, or classifications are interpreted and how they are translated into actions.\nLogic is the rules, thresholds, constraints, and objectives that govern a system’s behavior. These elements specify what should happen when a model produces a given output. In a system designed to detect credit card fraud, a model may provide a probability score that fraud has occurred. This score might then be compared against some threshold, the level at which the business would like to take Action on the prediction, and an alert triggered. These thresholds are not inherent to the model; the model just outputs the prediction. These thresholds are design choices that reflect priorities, trade-offs, and risk tolerance.\nLogic also encodes business or organizational constraints. All businesses and organizations face at least some constraints, such as resource limitations, regulatory requirements, fairness considerations, or cost structures. A model might identify many high-risk cases, but logic determines how many can realistically be acted upon, which cases are prioritized, and which actions are permissible. As a result, logic often mediates between what a model suggests and what an organization can or should do.\nLogic can be implemented in various ways. In some systems, it takes the form of explicit rules written by humans, “when we see x, we do y. In others, it can be nuanced, the logic embedded within optimization routines, data collection policies, or decision frameworks. Even when decisions by a system appear automated, they are often controlled by the logic layered on through past human judgments about acceptable or optimal outcomes.\nModels produce outputs, but logic determines actions.\n\n\n\nFailures in analytics and AI systems tend to come from breakdowns at one or more layers of the system: data, models, or logic. Understanding these layers and the interactions between them provides a structured way to diagnose system results and understand why a system produces a sub-optimal outcome.\nMissing values, measurement errors, outdated records, or shifts in underlying patterns over time can cause failures in this data layer. Because models base their learning here, weaknesses or problems at this layer often propagate forward, limiting what the system can achieve, regardless of how sophisticated the model is\nFailures at the model layer arise when the model itself is poorly matched to the problem, task, or data available. This may involve using overly simplistic models that fail to capture complex but important relationships. Conservatively, this is also sometimes the use of overly complex models that overfit historical patterns. Even well-designed models can fail when deployed in contexts that differ meaningfully from those seen during training.\nPoorly chosen or arbitrary thresholds, rigid rules that do not adapt to changing conditions, or decision criteria that prioritize the wrong objectives can all lead to failures inte *logic layer**. In these cases, a model may be producing reasonable outputs, but the surrounding logic causes undesirable actions or missed opportunities.\nThese layers are closely intertwined. High data quality and a strong model can’t save a system with seriously flawed logic. Likewise, careful logic cannot compensate for fundamentally uninformative or skewed data.\nConsidering failures throughout these different layers, allows for more precide intervention when models go wrong. Rather than asking whether an AI system “works” or “does not work,” it becomes possible to ask where it is breaking down and why."
  },
  {
    "objectID": "textbook_src/ch3_data_pipelines_decision_frameworks.html#ai-paradigms-overview",
    "href": "textbook_src/ch3_data_pipelines_decision_frameworks.html#ai-paradigms-overview",
    "title": "AI Systems: Data, Models, and Logic",
    "section": "AI Paradigms Overview",
    "text": "AI Paradigms Overview\n\n\n\nA useful shorthand: AI includes many approaches, with machine learning and deep learning representing increasingly specialized subsets.\n\n\n\nSymbolic AI\nSymbolic systems rely on explicit representations of knowledge and rule-based reasoning to perform tasks. Contrast that with the types of systems we have been discussing so far, which use pattern recognition from data. These systems operate by manipulating inputs (words, categories, etc) according to predefined rules.\nThe main idea in this approach is that intelligent behavior can be produced by encoding expert knowledge directly, as a series of rules and decision points, into a system. This often takes the form of if–then rules. For example, a symbolic system might contain rules such as: if a customer is late on payment and has missed multiple deadlines, then flag the account for review. Each rule reflects a human and/or expert judgment that has been translated into formal logic.\nSymbolic AI systems tend to be transparent and interpretable. Because their reasoning process is explicitly defined, it is usually possible to trace back to how a particular outcome was reached by the system. This makes these symbolic approaches attractive, and sometimes even required, in domains where explanations, compliance, or auditability are critical. They perform best in environments where the rules are stable, the problem space and scope are well understood, and rarely change.\nThere are some obvious limitations. Writing and maintaining rules is labor-intensive, complex, and such systems almost always struggle to scale as complexity increases. They also perform poorly in settings characterized by ambiguity, noise, or high variability (change).\nWhile symbolic AI is no longer the dominant approach/paradigm, it is still an important conceptual foundation. Many modern AI systems rely on symbolic components for managing constraints, validation, and control.\n\n\nStatistical and Machine Learning\nStatistical and machine learning approaches to AI differ from symbolic systems in a fundamental way: rather than relying on explicitly programmed rules, they learn patterns from data. These approaches use historical observations to infer relationships between inputs and outputs, allowing systems to generalize to new, unseen cases without being told exactly how to respond in every situation.\nAt the heart of machine learning is the idea that patterns (sometimes referred to as in data can be captured through mathematical models whose parameters are estimated from examples. During training, a model is exposed to data and adjusted so that its predictions align with observed outcomes as closely as possible. This process allows the system to adapt to complex patterns that would be difficult to specify manually using rules alone.\nMachine learning methods are often categorized based on the type of feedback available during training. In supervised learning, the model is trained using labeled examples. Each case (row or example) has a number of inputs, and then a labeled output. So the model knows the correct output in advance, for a large set of training data. In unsupervised learning, the model works with unlabeled data to identify structure. There is a number of cases (rows or examples), but there is no “known” outcome of interest. These models are often used to identify clusters or latent patterns, without predefined outcomes. Both approaches are widely used in analytics and AI systems.\nCompared to symbolic AI, statistical and machine learning systems are, for the most part, more flexible and scalable. Think of the fundamental differences: for every pattern in the data, a symbolic AI system needs to write a rule. Often, a human expert has to help with that rule. This is incredibly costly in terms of time and effort, and is brittle when new data is introduced. A machine learning system can build a model on data, and then build a model again when new data is introduced. These approaches perform well in situations with large volumes of data and are more adaptable when there are subtle patterns or relationships in the data. However, this flexibility comes with trade-offs. Learned models may be less transparent; you can’t simply follow the system’s logic in some of them as you would in Symbolic AI systems. In addition, they are obviously very sensitive to the data and the decisions made about which data to include/exclude. The data used is almost always a proxy for the data we would like to have.\nStatistical and machine learning approaches do not eliminate the need for human judgment. There are performance consequences to choices about data, features, and model evaluation/metrics. One way to think about Machine learning is that it shifts the burden of specification from rule-writing to data curation and model design. Expertise is still needed in the system, just applied within a different area.\nThis paradigm has become central to modern analytics and AI, forming the basis for many applications encountered in practice. It also provides the foundation for more advanced approaches, such as neural and deep learning.\n\n\nNeural and Deep Learning\n\n\n\nNeural Networks are comprised of inputs, hidden layers, and output layers.\n\n\nNeural and deep learning approaches extend the main idea of statistical machine learning by focusing on learning representations directly from data. These models use layered computational structures, neural networks, to transform raw inputs into increasingly abstract representations.\nThe key idea behind neural networks is inspired by, but not equivalent to, biological neurons. A neural network is composed of interconnected units that apply weighted combinations of inputs, followed by nonlinear transformations. If this sounds complicated it is. Later chapters will dig into the concept in more depth. But the essential idea is that the layered structure allows these models to excel in tasks such as image recognition, speech processing, and natural language understanding, where relationships are difficult to specify explicitly.\nOne defining characteristic of deep learning is its ability to operate on unstructured or semi-structured data, including images, audio, and text. In these domains, traditional statistical models often require extensive feature engineering. Deep learning models, by contrast, can learn relevant representations automatically from large volumes of data, reducing the need for manual (human) specification of features. One way to think of this, not exactly correct but probably good enough for beginners, is that these systems are like automated versions of symbolic AI. Their computing power allows for thousands, millions of rules to be created to extract even very faint patterns.\nThe ability to model complex relationships comes with a trade-off. Neural and deep learning models are data-intensive and computationally demanding. Training them often requires large datasets, which can be expensive to obtain, store, and manage. They also often need specialized and expensive computer hardware. These approaches are also less interpretable than simpler models. The very nature of the complex modeling process makes it more difficult to explain why a particular output was produced.\n\n\nHybrid Systems in Practice\n\n\n\nMost real-world AI systems blend machine learning, rules, and human oversight rather than relying on a single paradigm.\n\n\nAI is usually deployed as ahybrid system, combining features of symbolic reasoning systems, machine learning approaches, and neural networks. Most systems don’t rely on a single paradigm. Each approach contributes different strengths, and hybrid designs allow systems to balance performance, interpretability, and control.\nImagine a deep learning system, designed primarily for object recognition. This system might be used to search for specific objects within an image or a video. The output of that vision system might be fed into a rule-based layer, which might determine if the output meets a threshold for taking or triggering some specific action. That Action might be fed into a third system in that chain, and so on. Systems that focus on perception and prediction are frequently rated with symbolic rule-based systems to map operational constraints.\nHybrid systems also help address the practical limitations of individual approaches. Machine learning models can adapt to data and capture subtle patterns, but they may behave unpredictably outside familiar conditions. Symbolic logic imposes guardrails, prevents certain actions, or requires human review under specified circumstances. Statistical models can provide calibrated probabilities that support decision thresholds and prioritization. Together, these components form systems that are more robust than any single approach alone.\nThink of the AI system almost everyone has encountered: recommendation systems. These combine user data such as past purchases, preferences, and page views. When the model is run, a machine learning only model would show the items to the user with the highest probability of purchase, or the highest possible value. A more complex system might combine a symbolic approach and use a logic function to validate that the item is in stock and available, and that the recommendation does not violate any company rules or legal restrictions.\nIntelligence, or what we may take for intelligence, in computer systems is in practice distributed across different system components."
  },
  {
    "objectID": "textbook_src/ch7_json_apis_data_access.html",
    "href": "textbook_src/ch7_json_apis_data_access.html",
    "title": "JSON and APIs for Data Access",
    "section": "",
    "text": "This chapter introduces JSON as a structured data format and shows how Python can retrieve and parse JSON data from APIs. The goal is to understand how modern analytics and AI workflows obtain data from external services and convert it into forms suitable for analysis."
  },
  {
    "objectID": "textbook_src/ch7_json_apis_data_access.html#overview",
    "href": "textbook_src/ch7_json_apis_data_access.html#overview",
    "title": "JSON and APIs for Data Access",
    "section": "",
    "text": "This chapter introduces JSON as a structured data format and shows how Python can retrieve and parse JSON data from APIs. The goal is to understand how modern analytics and AI workflows obtain data from external services and convert it into forms suitable for analysis."
  },
  {
    "objectID": "textbook_src/ch7_json_apis_data_access.html#json-and-structured-data-exchange",
    "href": "textbook_src/ch7_json_apis_data_access.html#json-and-structured-data-exchange",
    "title": "JSON and APIs for Data Access",
    "section": "JSON and Structured Data Exchange",
    "text": "JSON and Structured Data Exchange\n\nWhat JSON is\nJSON (JavaScript Object Notation) is a text-based format used to represent structured data. Despite its name, JSON is language-agnostic and is widely used across programming languages and platforms.\nJSON represents data using key–value pairs, much like Python dictionaries. Values associated with keys can themselves be other objects, lists, or simple values such as numbers, strings, or booleans. This allows JSON to represent complex, nested structures.\nConceptually, JSON encodes data as a hierarchy rather than a table. Instead of rows and columns, JSON organizes information into objects that contain other objects or lists. This makes it well suited for representing entities with varying attributes or nested relationships.\nJSON is common in AI systems because it is:\n- human-readable,\n- easy for machines to parse,\n- flexible in structure,\n- and well suited for transmitting data over networks.\nAPIs frequently return data in JSON format because it allows systems to exchange structured information without requiring a fixed schema in advance. This flexibility is valuable in environments where data evolves over time or where different consumers may need different parts of the data.\n\n\nJSON vs tabular data\nAlthough JSON and tabular data both represent structured information, they differ in how that structure is expressed.\nTabular data is flat. Each row represents an observation, and each column represents a variable. This structure is ideal for many forms of analysis, especially when observations are uniform and relationships are simple.\nJSON data, by contrast, is hierarchical. Objects can contain nested objects or lists, and not every object must have the same keys. This allows JSON to represent more complex relationships, but it also makes direct analysis more challenging.\nMapping JSON to tables often requires flattening the structure. Nested objects may be expanded into columns, and lists may be transformed into multiple rows. This transformation step is common in data pipelines that ingest API data and prepare it for analysis with tools like Pandas."
  },
  {
    "objectID": "textbook_src/ch7_json_apis_data_access.html#apis-as-data-sources",
    "href": "textbook_src/ch7_json_apis_data_access.html#apis-as-data-sources",
    "title": "JSON and APIs for Data Access",
    "section": "APIs as Data Sources",
    "text": "APIs as Data Sources\n\nWhat an API is\nAn API (Application Programming Interface) is a structured way for one system to request information or services from another system. In data workflows, APIs are most often used as data access mechanisms. Sometimes it is helpful to think of APIs like a bridge, from one system to another, as a path to get information that is needed.\nRather than downloading a file manually, a program sends a request to an API endpoint and receives a response containing data. This interaction follows a predictable pattern:\n- the client sends a request,\n- the server processes it,\n- the server returns a response.\nThe response typically includes both the requested data and metadata about the request itself, such as whether it was successful.\nAPIs are widely used because they:\n- allow real-time or near-real-time access to data,\n- enable controlled and authenticated access,\n- support integration across systems and platforms.\nIn analytics and AI contexts, APIs are commonly used to retrieve data from web services, cloud platforms, and internal systems."
  },
  {
    "objectID": "textbook_src/ch7_json_apis_data_access.html#making-api-requests-with-python",
    "href": "textbook_src/ch7_json_apis_data_access.html#making-api-requests-with-python",
    "title": "JSON and APIs for Data Access",
    "section": "Making API Requests with Python",
    "text": "Making API Requests with Python\n\nSending a request\nPython provides several libraries for working with APIs. One of the most commonly used is the requests library, which simplifies sending HTTP requests and handling responses.\nThe basic workflow for making an API request involves:\n1. importing the requests library,\n2. sending a request to a URL,\n3. storing the response for further inspection.\nimport requests\nresponse = requests.get(\"https://api.example.com/data\")\nIn this example, a GET request is sent to the specified URL. The result is stored in a variable named response. At this stage, no assumptions are made about the content of the response; it is treated as an object that contains information returned by the server.\nHandling responses responsibly involves checking whether the request succeeded and understanding what kind of data was returned. Although error handling and authentication are not covered here, it is important to recognize that API requests can fail for many reasons, including network issues, invalid endpoints, or access restrictions.\nThe key idea is that APIs allow programs to retrieve data programmatically rather than manually.\n\n\nParsing JSON responses\nMany APIs return data in JSON format. Once a response has been received, the next step is to convert that JSON data into Python objects that can be inspected and manipulated.\nThe requests library provides a method for this purpose:\ndata = response.json()\nCalling json() on the response parses the JSON text and converts it into native Python data structures, typically dictionaries and lists. This transformation allows the data to be explored using familiar Python tools.\nAt this point, inspection becomes important. API responses often contain nested structures, metadata, or multiple layers of information. Examining the structure of the returned object helps determine which parts of the data are relevant and how they might be transformed into tabular form.\nParsing JSON responses reinforces a recurring theme: data often arrives in one structure and must be transformed into another before analysis can occur. APIs provide access to rich data sources, but turning that data into usable datasets requires effort."
  },
  {
    "objectID": "textbook_src/ch7_json_apis_data_access.html#chapter-summary",
    "href": "textbook_src/ch7_json_apis_data_access.html#chapter-summary",
    "title": "JSON and APIs for Data Access",
    "section": "Chapter Summary",
    "text": "Chapter Summary\nThis chapter introduced JSON and APIs as key components of modern data workflows. While earlier chapters focused on data stored in local files, this chapter expanded the view to include data that is retrieved dynamically from external systems.\nJSON was presented as a flexible, hierarchical data format that uses key–value pairs and nested structures to represent complex information. Its widespread use in AI and analytics systems reflects its strengths for data exchange and representation, even though it is not always convenient for direct analysis.\nAPIs were introduced as structured interfaces for requesting data and services from other systems. By sending requests and receiving responses, programs can obtain up-to-date information without manual file handling. The combination of APIs and JSON underpins many contemporary data pipelines.\nFinally, the chapter showed how Python’s requests library can be used to call APIs and parse JSON responses into native Python objects. This ability to retrieve and interpret external data is essential for building analytics and AI workflows that interact with real-world systems.\nTogether, JSON and APIs extend your data toolkit beyond static files. They enable your code to participate in larger ecosystems of services and data sources, setting the stage for more advanced topics such as automated pipelines, streaming data, and integration with AI services."
  },
  {
    "objectID": "textbook_src/ch1_foundations_analytics_ai_split.html",
    "href": "textbook_src/ch1_foundations_analytics_ai_split.html",
    "title": "Foundations of Analytics and AI",
    "section": "",
    "text": "Analytics is an probably best understood as a decision-support capability. Analytics methods help to summarize, explain, and forecast patterns in data, but they do not make decisions on their own.\nImportantly, analytics isn’t a single technique, or a single approach. Sometimes it helps to think of analytics approaches as a progression of questions organizations and analysts ask about their data. These questions range in purpose from understanding what has already happened to deciding what action should be taken next. One common, and useful way, to organize this progression is through four categories: descriptive, diagnostic, predictive, and prescriptive analytics.\nDescriptive analytics answers the question: What happened?\nThis is probably, for most beginners and businesses, the most familiar form of analytics. It focuses on summarizing historical data, and reporting on the past. All data is in the past of course, so this is not an unusual place to start! Examples include reports, dashboards, averages, totals, and trends of events or activity over time. Descriptive analytics does not attempt to explain why something occurred or what will happen next; rather it provides a picture of past outcomes.\nDiagnostic analytics asks: Why did it happen?\nOnce an outcome is observed, the next step is (sometimes) to understand its cause. Diagnostic analytics explores relationships, comparisons, pproperties in the data to identify factors that may have contributed to a givern outcome. This might involve segmenting or profiling customers, comparing performance across regions, or examining changes before and after a specific event. While still focused on historical data, the goal of diagnostic analytics is to move beyond description toward explanation.\nPredictive analytics moves the focus from the past, to the future by asking: What is likely to happen next?\nHere, statistical models and machine learning techniques are often used to estimate some future outcome based on the patterns in past data. Common examples include forecasting demand, predicting customer churn, or estimating the probability that an event will occur. Predictive analytics does not of course guarantee what will happen; it produces probabilistic estimates, perhaps best thought of as a data-educated guess, that support informed decision making.\nPrescriptive analytics asks What should we do about it?\nPrescriptive analytics builds on predictions by incorporating goals, constraints, policies and trade-offs to recommend actions. This may involve optimization models, business rules, or simulation. For example, a system might recommend how much inventory to reorder to avoid a forecasted product shortage, which customers are most likely to target with a promotion and how they shoulkd be approached, or how to allocate limited resources. At this stage, analytics becomes tightly connected to decision-making rather than analysis alone.\nThese categories are almost never mutually exclusive. Most modern systems will combine multiple aspects of these. For example, a workflow might summarize past performance (descriptive), given a finding here, it might move on identifying a problem area (diagnostic). More sophisticated systems might then estimate or try to quantify some future risk (predictive), and recommend a precise action (prescriptive).\nTo make this idea more concrete, let’s take the same inventory problem vexxing a business, and view it through these different levels of analytics.\n\nDescriptive: “Stockouts of product x increased last month.”\n\nDiagnostic: “The stockouts of product x spiked after supplier delays on key supply routes through the port of y.”\n\nPredictive: “Next month’s stockout risk for product x is approximately 0.35.”\n\nPrescriptive: “Increase the reorder point for product x, and route more goods through an alternate port (z) to to reduce stockout risk.”\n\nThe underlying domain is unchanged; what changes is the question being asked."
  },
  {
    "objectID": "textbook_src/ch1_foundations_analytics_ai_split.html#analytics-descriptive-diagnostic-predictive-prescriptive",
    "href": "textbook_src/ch1_foundations_analytics_ai_split.html#analytics-descriptive-diagnostic-predictive-prescriptive",
    "title": "Foundations of Analytics and AI",
    "section": "",
    "text": "Analytics is an probably best understood as a decision-support capability. Analytics methods help to summarize, explain, and forecast patterns in data, but they do not make decisions on their own.\nImportantly, analytics isn’t a single technique, or a single approach. Sometimes it helps to think of analytics approaches as a progression of questions organizations and analysts ask about their data. These questions range in purpose from understanding what has already happened to deciding what action should be taken next. One common, and useful way, to organize this progression is through four categories: descriptive, diagnostic, predictive, and prescriptive analytics.\nDescriptive analytics answers the question: What happened?\nThis is probably, for most beginners and businesses, the most familiar form of analytics. It focuses on summarizing historical data, and reporting on the past. All data is in the past of course, so this is not an unusual place to start! Examples include reports, dashboards, averages, totals, and trends of events or activity over time. Descriptive analytics does not attempt to explain why something occurred or what will happen next; rather it provides a picture of past outcomes.\nDiagnostic analytics asks: Why did it happen?\nOnce an outcome is observed, the next step is (sometimes) to understand its cause. Diagnostic analytics explores relationships, comparisons, pproperties in the data to identify factors that may have contributed to a givern outcome. This might involve segmenting or profiling customers, comparing performance across regions, or examining changes before and after a specific event. While still focused on historical data, the goal of diagnostic analytics is to move beyond description toward explanation.\nPredictive analytics moves the focus from the past, to the future by asking: What is likely to happen next?\nHere, statistical models and machine learning techniques are often used to estimate some future outcome based on the patterns in past data. Common examples include forecasting demand, predicting customer churn, or estimating the probability that an event will occur. Predictive analytics does not of course guarantee what will happen; it produces probabilistic estimates, perhaps best thought of as a data-educated guess, that support informed decision making.\nPrescriptive analytics asks What should we do about it?\nPrescriptive analytics builds on predictions by incorporating goals, constraints, policies and trade-offs to recommend actions. This may involve optimization models, business rules, or simulation. For example, a system might recommend how much inventory to reorder to avoid a forecasted product shortage, which customers are most likely to target with a promotion and how they shoulkd be approached, or how to allocate limited resources. At this stage, analytics becomes tightly connected to decision-making rather than analysis alone.\nThese categories are almost never mutually exclusive. Most modern systems will combine multiple aspects of these. For example, a workflow might summarize past performance (descriptive), given a finding here, it might move on identifying a problem area (diagnostic). More sophisticated systems might then estimate or try to quantify some future risk (predictive), and recommend a precise action (prescriptive).\nTo make this idea more concrete, let’s take the same inventory problem vexxing a business, and view it through these different levels of analytics.\n\nDescriptive: “Stockouts of product x increased last month.”\n\nDiagnostic: “The stockouts of product x spiked after supplier delays on key supply routes through the port of y.”\n\nPredictive: “Next month’s stockout risk for product x is approximately 0.35.”\n\nPrescriptive: “Increase the reorder point for product x, and route more goods through an alternate port (z) to to reduce stockout risk.”\n\nThe underlying domain is unchanged; what changes is the question being asked."
  },
  {
    "objectID": "textbook_src/ch1_foundations_analytics_ai_split.html#artificial-intelligence-systems-that-perform-tasks-requiring-human-like-judgment",
    "href": "textbook_src/ch1_foundations_analytics_ai_split.html#artificial-intelligence-systems-that-perform-tasks-requiring-human-like-judgment",
    "title": "Foundations of Analytics and AI",
    "section": "Artificial Intelligence: Systems That Perform Tasks Requiring Human-Like Judgment",
    "text": "Artificial Intelligence: Systems That Perform Tasks Requiring Human-Like Judgment\nIn practice, AI systems often:\n\nproduce scores, classes, or recommendations,\n\noperate at scale (many decisions, quickly), and\n\ntrigger or prioritize actions with limited human-in-the-loop.\n\nOne simple way to distinguish analytics from artificial intelligence is to focus on how the outputs of each are used. Analytics systems typically produce insights, these might be things like a business summary, explanations, or forecasts. These artifacts or outputs are then reviewed and acted upon by humans. AI systems, by contrast, are often designed to execute or automate decisions directly, using models and logic to act without human intervention in each instance.These characteristics distinguish AI systems from purely descriptive or exploratory analytics, even when they rely on similar mathematical tools.\nArtificial intelligence (AI) refers to a class of systems designed to perform tasks that would normally require human judgment, interpretation, or decision-making. Unlike traditional analytics, which primarily focuses on summarizing data or supporting decisions, AI systems are often embedded directly into processes where they make or influence decisions in real time.\nA defining feature of AI systems is that they operate in environments where rules are incomplete, uncertainty is often present, or inputs are too complex to handle with simple, hand-coded logic. Examples include recognizing objects in images, understanding natural language, detecting fraudulent transactions, or recommending products to users. In each case, the system must evaluate patterns, weigh evidence, and produce an output. Although the actual process a computer takes is different, it does mimic or resemble what a human might do, and how a human might approach the same problem(s).\nAI systems often (though not always!) rely on models trained from data rather than explicit instructions for every possible scenario. An AI system is not usuakky told exactly how to respond in each case it has to work with. Rather, the system learns statistical relationships or representations from historical examples. When presented with new inputs, new cases, it uses the learned patterns to generate predictions, classifications, or actions. This learning-based approach allows AI systems to scale to very complex tasks, while it also introduces uncertainty and the possibility of error.\nAnother important characteristic of AI is that its outputs are often probabilistic rather than definitive. This is a distinction that is key for decision makers using these systems to understand. An AI system may estimate the likelihood that an email is spam, that a customer will stop using a service, or that an image contains a particular object. To make these systems actionable requires combining these estimates with thresholds, business rules, or human oversight to determine what action is taken. It should be clear at this point that AI should be understood as a component within a broader decision system rather than as an autonomous replacement for human judgment. This is not to say these systems do not or cannot replace human decision making. It is not always clear in these systems when and how human judgment was involved in the making or deployment of a system, but it was involved.\nIt is useful at this point to distinguish between narrow AI and broader notions of artififical general intelligence. The systems discussed in this book are narrow by design: they are built to perform specific tasks under specific conditions, often very well, but they do not possess general understanding or awareness. Recognizing both the strengths and limitations of these systems is essential for using them responsibly and effectively in organizational settings.\n\nNested views of AI, machine learning, and deep learning\n\n\n\nAI is often described as a broad category that contains machine learning methods, with deep learning as a further subset.\n\n\nThis nested view is not the only way to define AI, but it is useful: - AI refers broadly to systems that participate in decisions.\n- Machine learning focuses on models learned from data.\n- Deep learning is a subset of machine learning that uses layered neural networks.\nLater chapters will revisit these distinctions in more detail."
  },
  {
    "objectID": "textbook_src/ch1_foundations_analytics_ai_split.html#analytics-vs.-ai-overlap-and-differences",
    "href": "textbook_src/ch1_foundations_analytics_ai_split.html#analytics-vs.-ai-overlap-and-differences",
    "title": "Foundations of Analytics and AI",
    "section": "Analytics vs. AI: Overlap and Differences",
    "text": "Analytics vs. AI: Overlap and Differences\nAnalytics and AI are related, but they shouldn’t be considered interchangeable. Many systems labeled as “AI” rely heavily on analytical techniques, and many analytical workflows now incorporate AI-based models. There is considerale overlap in the domains, and it is often misunderstood. It is also important for business decison makers to be able to differ from the reality of these systems, and the very real ways they can add value, and the hype surrounding them.\nWe’ve discussed earlier the specific characteristics of these systems. At a high level, analytics is primarily concerned with supporting human decision-making. The focus of these systems is in extracting insight from data, identifying patterns, and presenting information in ways that help humans better understand what is happening and decide what to do. Even in advanced forms such as predictive or prescriptive analytics, the output is often intended to inform a human decision-maker, who retains responsibility for interpreting the results and acting on them.\nAI, by contrast, is often designed to participate directly in the decision process. AI systems may initiate actions with little or no human involvement, this is especially the case in high-volume or time-sensitive contexts. While humans still define objectives, constraints, and oversight mechanisms, AI systems are frequently embedded into operational workflows where their outputs have immediate consequences.\nThere is, however, an area of overlap. Both of these systems, analytics and AI: - rely on data as the primary input, - use models to represent relationships or patterns, - and produce outputs that influence decisions.\nMany predictive analytics techniques, such as regression or classification models, are also foundational components of AI systems. One of the simplist ways to understand the difference: it lies not in the mathematics, but in how the results are used. A churn prediction model displayed on a dashboard for managers is typically considered analytics; the same model automatically triggering retention offers for customers may be considered AI.\nAnother key distinction is the role of interpretability and automation. Analytics tools are often designed and implemented in a way the emphasize transparency and explainability. They are built to allow users to drill down into results and ask follow-up questions. AI systems, one the other hand, often prioritize performance and scalability over this level of transparency. This sometimes triggers requirements such as additional governance and monitoring to ensure appropriate use. The lack of interpretability may also lead some users of these systems to lack trust in the outputs or the decisions the system makes.\nRather than viewing analytics and AI as competing approaches, it is more accurate to see them as points along a continuum. Most system deployed in the real world combine facets of analytical reporting, predictive modeling, and AI-driven automation into a single pipeline.\nThis distinction becomes increasingly important as we move into topics such as machine learning and generative AI, where the same underlying techniques can support very different organizational roles depending on how they are deployed.\n\nA continuum of decision autonomy\n\n\n\nDecision autonomy increases as model outputs move from being viewed by humans to driving actions automatically."
  },
  {
    "objectID": "textbook_src/ch1_foundations_analytics_ai_split.html#prediction-as-a-component",
    "href": "textbook_src/ch1_foundations_analytics_ai_split.html#prediction-as-a-component",
    "title": "Foundations of Analytics and AI",
    "section": "Prediction as a Component",
    "text": "Prediction as a Component\n\n\n\n\n\n\nImportant\n\n\n\nHow does decision logic, thresholds, constraints and appropriate oversite play in this system space?\nA prediction might help to answer: “What is likely to happen?”\nA system goal is often to answer: “What do we do now that we know that?”\n\n\nIt is not at all uncommon for many discussions of AI, especially those focused on machine learning, to treat prediction as the central task. Sometimes the only task! While prediction is a critical capability, it is important to recognize that prediction alone does not make a complete AI system. In practice, prediction is just one component within a larger structure that connects data, models, and decisions.\nA prediction answers a narrow question such as “What is the likelihood that this event will occur?” or “Which category does this input most likely belong to?” For example, a model might predict the probability that a customer will churn, that a transaction is fraudulent, or that a document belongs to a particular topic. These outputs are useful, but on their own they do not specify what action should be taken, and they do not take action independent of human action.\nWhat turns a prediction into something operational is the surrounding decision logic. Organizations must decide how to interpret a predicted probability, what threshold to apply, what constraints exist within the business environment, and what costs are associated with different actions. A churn prediction of 70%, for instance, does not automatically imply a given action should be taken. In a different business, or market that prediction may trigger different responses depending on business priorities or resource availability.\nThis distinction highlights why AI systems might be understood as socio-technical systems, not just a simple model. Data pipelines determine what information is available to the model. Models generate predictions based on learned patterns from data. Decision frameworks and processes translate those into actions, sometimes with with human oversight and governance layered in.\nRecognizing that prediction is a component rather than the whole system also helps clarify common misunderstandings about AI capability. High predictive accuracy does not guarantee good decisions, ethical outcomes, or organizational value! When an analyst or a business utilizes poorly designed thresholds, has misaligned incentives, or fails to appropriately examine assumptions, even the most accurate model can be undermined.\nThroughout this book, we will return to this idea repeatedly: models produce predictions, but systems produce decisions.\n\nQuick Check: Analytics, AI, or Both?\nThe goal in this short thought experiement is not to label each system perfectly, but to consider why something is considered analytics, AI, or a combination of both. The differences are fairly nuanced. In most real-world systems, the distinction depends less on the mathematical technique and more on how the output is used.\n\nA dashboard showing last quarter’s sales by region, with filters and charts\nAnalytics. The system summarizes historical data and presents it to a human decision-maker, who interprets the results and decides what action to take. It does not predict outcomes or take autonomous action.\nA model that predicts the probability a customer will cancel their subscription, displayed to a manager\nanalytics. Although perhaps this one is closer to edge. Even though a predictive model is used, the output supports human judgment rather than directly triggering an action.\nThe same churn prediction model automatically sending retention offers to high-risk customers\nDefinetly AI. The prediction is now embedded in an operational workflow, and the system is actively participating in decision-making and taking action rather than merely informing it.\nA fraud detection system that flags suspicious transactions for human review\nBoth analytics and AI?. This one is probably the most complex to cleanly seperate. The model performs a task that is very “AI-like” (pattern recognition and classification), but the final decision is still made by a human, creating a hybrid system.\nA recommendation engine that personalizes product suggestions in real time\nThis is clearly AI. The system continuously generates predictions and acts on them automatically at scale, often without direct human intervention for each decision.\n\n\n\n\n\n\n\nNote\n\n\n\nEven when the underlying model is identical, the system label may shift from “analytics” to “AI” as autonomy increases.\nA churn model whose scores appear only on a dashboard is typically analytics. The same model driving automatic retention offers is better described as AI.\n\n\nThe key takeaway is that the same underlying model can be analytics or AI depending on context. What matters is not just whether a model is used, but where it sits in the decision process, how much autonomy it has, and who—or what—ultimately acts on its output."
  },
  {
    "objectID": "appendix/appendix_data_infrastructure.html",
    "href": "appendix/appendix_data_infrastructure.html",
    "title": "Appendix: Data Infrastructure and Databases for AI",
    "section": "",
    "text": "Modern AI systems depend not only on algorithms and models, but on the data infrastructure that feeds them. Databases, storage systems, and data architectures determine:\n\nhow data is collected and stored,\n\nhow quickly it can be retrieved,\n\nwhat formats it can take,\n\nhow much data the system can scale to support,\n\nand how reliable AI outputs will be.\n\nThis appendix provides a conceptual overview of the major database paradigms, architectural choices, and design considerations relevant to AI work."
  },
  {
    "objectID": "appendix/appendix_data_infrastructure.html#sql-relational-databases",
    "href": "appendix/appendix_data_infrastructure.html#sql-relational-databases",
    "title": "Appendix: Data Infrastructure and Databases for AI",
    "section": "SQL (Relational Databases)",
    "text": "SQL (Relational Databases)\n\nOrganize data in tables with predefined schemas.\n\nUse the SQL query language for joins, aggregations, and complex analytical queries.\n\nStrong support for ACID transactions, ensuring reliability and correctness.\n\nExamples: PostgreSQL, MySQL, Oracle."
  },
  {
    "objectID": "appendix/appendix_data_infrastructure.html#nosql-non-relational-databases",
    "href": "appendix/appendix_data_infrastructure.html#nosql-non-relational-databases",
    "title": "Appendix: Data Infrastructure and Databases for AI",
    "section": "NoSQL (Non-Relational Databases)",
    "text": "NoSQL (Non-Relational Databases)\n\nUse flexible data models such as documents (JSON), key–value stores, wide-column tables, or graph structures.\n\nDesigned for horizontal scaling, large volume, and rapidly changing or semi-structured data.\n\nOften rely on BASE consistency models (eventual consistency).\n\nExamples: MongoDB, Cassandra, DynamoDB, Redis.\n\nComparison Table\n\n\n\n\n\n\n\n\nFeature\nSQL Databases\nNoSQL Databases\n\n\n\n\nData Model\nRelational (tables)\nFlexible (document, key-value, graph)\n\n\nSchema\nFixed\nSchema-less or dynamic\n\n\nConsistency\nStrong (ACID)\nOften eventual (BASE)\n\n\nScaling\nVertical; harder to distribute\nHorizontal; distributed by design\n\n\nQuery Power\nComplex SQL queries\nSimple lookups, model-specific querying\n\n\nUse Cases\nTransactions, structured analytics\nHigh-volume, unstructured or semi-structured data"
  },
  {
    "objectID": "appendix/appendix_data_infrastructure.html#acid-sql",
    "href": "appendix/appendix_data_infrastructure.html#acid-sql",
    "title": "Appendix: Data Infrastructure and Databases for AI",
    "section": "ACID (SQL)",
    "text": "ACID (SQL)\nACID ensures reliable and correct transactions.\n\nAtomicity: All or nothing\n\nConsistency: Maintains valid states\n\nIsolation: Transactions don’t interfere\n\nDurability: Once committed, data persists"
  },
  {
    "objectID": "appendix/appendix_data_infrastructure.html#base-nosql",
    "href": "appendix/appendix_data_infrastructure.html#base-nosql",
    "title": "Appendix: Data Infrastructure and Databases for AI",
    "section": "BASE (NoSQL)",
    "text": "BASE (NoSQL)\nBASE prioritizes scalability and availability:\n\nBasically Available\n\nSoft State (state may change over time)\n\nEventual Consistency (nodes converge eventually)\n\nThis tradeoff allows large-scale AI systems to ingest data at extremely high speed, but with weaker short-term consistency guarantees."
  },
  {
    "objectID": "appendix/appendix_data_infrastructure.html#vertical-scaling-scale-up",
    "href": "appendix/appendix_data_infrastructure.html#vertical-scaling-scale-up",
    "title": "Appendix: Data Infrastructure and Databases for AI",
    "section": "Vertical Scaling (“Scale Up”)",
    "text": "Vertical Scaling (“Scale Up”)\n\nAdd CPU, RAM, or SSD to a single server.\n\nSimpler to manage; strong for transactional workloads.\n\nLimited by hardware ceilings.\n\nCommon with: SQL databases."
  },
  {
    "objectID": "appendix/appendix_data_infrastructure.html#horizontal-scaling-scale-out",
    "href": "appendix/appendix_data_infrastructure.html#horizontal-scaling-scale-out",
    "title": "Appendix: Data Infrastructure and Databases for AI",
    "section": "Horizontal Scaling (“Scale Out”)",
    "text": "Horizontal Scaling (“Scale Out”)\n\nAdd more servers (nodes) in a distributed system.\n\nSupports massive throughput and storage growth.\n\nRequires sharding/partitioning and distributed consistency management.\n\nCommon with: NoSQL databases.\n\nComparison\n\n\n\nAspect\nVertical Scaling\nHorizontal Scaling\n\n\n\n\nApproach\nBigger machine\nMore machines\n\n\nEasy to Implement?\nYes\nHarder (distributed systems)\n\n\nLimits\nHardware\nNearly unlimited growth\n\n\nExamples\nPostgreSQL\nCassandra, DynamoDB"
  },
  {
    "objectID": "appendix/appendix_data_infrastructure.html#in-memory-databases",
    "href": "appendix/appendix_data_infrastructure.html#in-memory-databases",
    "title": "Appendix: Data Infrastructure and Databases for AI",
    "section": "In-Memory Databases",
    "text": "In-Memory Databases\n\nStore data primarily in RAM.\n\nExtremely fast; ideal for real-time AI use cases.\n\nVolatile unless snapshots/logging enable persistence.\n\nExamples: Redis, Memcached, SAP HANA."
  },
  {
    "objectID": "appendix/appendix_data_infrastructure.html#disk-based-databases",
    "href": "appendix/appendix_data_infrastructure.html#disk-based-databases",
    "title": "Appendix: Data Infrastructure and Databases for AI",
    "section": "Disk-Based Databases",
    "text": "Disk-Based Databases\n\nStore data on HDD/SSD, with caching layers for speed.\n\nSupport larger datasets with strong persistence.\n\nSlower than memory but more reliable for long-term storage.\n\nExamples: PostgreSQL, MongoDB.\n\nTable\n\n\n\n\n\n\n\n\nAspect\nIn-Memory\nDisk-Based\n\n\n\n\nSpeed\nVery fast\nSlower but persistent\n\n\nPersistence\nVolatile unless logged\nPersistent by default\n\n\nCapacity\nLimited by RAM\nScales via disk/sharding\n\n\nUse Cases\nCaching, sessions, real-time AI\nAnalytics, transactions, general data storage"
  },
  {
    "objectID": "appendix/appendix_data_infrastructure.html#open-source",
    "href": "appendix/appendix_data_infrastructure.html#open-source",
    "title": "Appendix: Data Infrastructure and Databases for AI",
    "section": "Open-Source",
    "text": "Open-Source\n\nFree to use and modify\n\nSupported by large communities\n\nHighly extensible\n\nExamples: PostgreSQL, MySQL, MongoDB"
  },
  {
    "objectID": "appendix/appendix_data_infrastructure.html#proprietary",
    "href": "appendix/appendix_data_infrastructure.html#proprietary",
    "title": "Appendix: Data Infrastructure and Databases for AI",
    "section": "Proprietary",
    "text": "Proprietary\n\nEnterprise-grade features\n\nVendor support\n\nIntegrated tooling for data governance and security\n\nExamples: Oracle, SQL Server, Amazon Aurora\n\nComparison Table\n\n\n\nCriteria\nOpen-Source\nProprietary\n\n\n\n\nCost\nFree\nPaid\n\n\nSupport\nCommunity-driven\nVendor-backed\n\n\nExtensibility\nVery customizable\nControlled by vendor\n\n\nLock-in\nLow\nHigh"
  },
  {
    "objectID": "appendix/appendix_data_infrastructure.html#example-entityrelationship-diagram-erd",
    "href": "appendix/appendix_data_infrastructure.html#example-entityrelationship-diagram-erd",
    "title": "Appendix: Data Infrastructure and Databases for AI",
    "section": "Example Entity–Relationship Diagram (ERD)",
    "text": "Example Entity–Relationship Diagram (ERD)\n\n\nA simplified relational schema for an e-commerce environment:\n\n\nerDiagram\n    CUSTOMER ||--o{ ORDER : places\n    ORDER ||--o{ ORDERITEM : includes\n    PRODUCT ||--o{ ORDERITEM : listed_in\n\n\n\n\nerDiagram\n    CUSTOMER ||--o{ ORDER : places\n    ORDER ||--o{ ORDERITEM : includes\n    PRODUCT ||--o{ ORDERITEM : listed_in"
  },
  {
    "objectID": "appendix/appendix_data_infrastructure.html#primary-and-foreign-keys",
    "href": "appendix/appendix_data_infrastructure.html#primary-and-foreign-keys",
    "title": "Appendix: Data Infrastructure and Databases for AI",
    "section": "Primary and Foreign Keys",
    "text": "Primary and Foreign Keys\nRelational databases use primary keys (PK) to uniquely identify each record in a table, and foreign keys (FK) to establish relationships between tables. These relationships enable joins that enrich feature engineering and support downstream analytics.\n\n\n\n\n\n\n\n\nTable\nPrimary Key (PK)\nForeign Keys (FK)\n\n\n\n\nCUSTOMER\ncustomer_id\n—\n\n\nORDER\norder_id\ncustomer_id\n\n\nPRODUCT\nproduct_id\n—\n\n\nORDERITEM\n(order_id, product_id)\norder_id, product_id\n\n\n\nThese relationships enable integrated datasets where customer attributes, product attributes, and transactional details can be combined for analytics, feature engineering, and modeling."
  },
  {
    "objectID": "appendix/appendix_data_infrastructure.html#how-database-choice-shapes-ai-systems",
    "href": "appendix/appendix_data_infrastructure.html#how-database-choice-shapes-ai-systems",
    "title": "Appendix: Data Infrastructure and Databases for AI",
    "section": "How Database Choice Shapes AI Systems",
    "text": "How Database Choice Shapes AI Systems\nDatabase choices directly influence the reliability, speed, and scalability of AI systems. Key dimensions include:\n\nPerformance\n\nSlow reads or writes → delays in model training\n\nBottlenecks in feature retrieval → slower real-time predictions\n\n\n\nScalability\n\nAI systems accumulate massive datasets\nNoSQL architectures often become necessary for high-volume logs, telemetry, or clickstream data\n\n\n\nConsistency Requirements\n\nHigh-stakes domains (e.g., fraud detection, finance) require ACID guarantees\n\nRecommendation engines and other high-throughput AI applications can tolerate eventual consistency\n\n\n\nData Structure Variety\n\nSQL: ideal for structured, tabular data\n\nNoSQL: more natural for JSON documents, event logs, sensor streams, and heterogeneous data\n\n\n\nLatency and Throughput\n\nReal-time inference: best supported by in-memory stores or fast NoSQL systems\n\nBatch training: disk-based analytical stores suffice and can handle larger volumes"
  },
  {
    "objectID": "appendix/appendix_data_infrastructure.html#real-time-vs.-batch-data-workflows-in-ai",
    "href": "appendix/appendix_data_infrastructure.html#real-time-vs.-batch-data-workflows-in-ai",
    "title": "Appendix: Data Infrastructure and Databases for AI",
    "section": "Real-Time vs. Batch Data Workflows in AI",
    "text": "Real-Time vs. Batch Data Workflows in AI\n\nReal-Time AI\n\nContinuous ingestion of events\n\nLow-latency feature retrieval and decision-making\n\nPowers applications like:\n\nlive recommendation engines\n\nanomaly detection\n\nstreaming fraud detection\n\nchatbots and conversational agents\n\n\n\n\nBatch AI\n\nPeriodic, scheduled data ingestion\n\nUsed for:\n\nnightly or weekly model retraining\n\noffline scoring\n\nlong-term historical analysis\n\n\nMost production AI systems use both approaches:\nreal-time for immediate decisions, and batch for maintaining or improving the models."
  },
  {
    "objectID": "appendix/appendix_data_infrastructure.html#summary",
    "href": "appendix/appendix_data_infrastructure.html#summary",
    "title": "Appendix: Data Infrastructure and Databases for AI",
    "section": "Summary",
    "text": "Summary\nThis appendix outlines core concepts in data infrastructure that support AI systems. Key takeaways include:\n\nNo single database is “best” for all AI applications — choices should reflect workload, data structures, and consistency needs.\n\nSQL databases excel for structured, transactional, strongly consistent data.\n\nNoSQL databases excel for scale, flexibility, and high-volume unstructured or semi-structured data.\n\nScaling strategy matters: vertical scaling increases hardware capacity on one machine, while horizontal scaling distributes data across many nodes.\n\nIn-memory vs. disk-based architectures determine latency, throughput, and persistence strategies.\n\nData architecture decisions directly impact model quality, model-training speed, feature delivery, and operational reliability.\n\nAI models depend critically on the systems beneath them.\nUnderstanding database infrastructure is therefore an essential component of understanding modern AI systems."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundations of Analytics & AI",
    "section": "",
    "text": "The initial draft of this textbook was written by the author. Subsequent revisions and refinements were supported by digital writing tools, including large language models and grammar-support software, such as Grammarly, Gemini, ChatGPT, and various markdown, text and Python linters. These tools were used to assist with organization, revision, and clarity of exposition. All conceptual content, interpretations, examples, and pedagogical decisions remain the responsibility of the author, who reviewed, edited, and verified all material for accuracy and coherence."
  },
  {
    "objectID": "index.html#use-of-ai-assisted-tools",
    "href": "index.html#use-of-ai-assisted-tools",
    "title": "Foundations of Analytics & AI",
    "section": "",
    "text": "The initial draft of this textbook was written by the author. Subsequent revisions and refinements were supported by digital writing tools, including large language models and grammar-support software, such as Grammarly, Gemini, ChatGPT, and various markdown, text and Python linters. These tools were used to assist with organization, revision, and clarity of exposition. All conceptual content, interpretations, examples, and pedagogical decisions remain the responsibility of the author, who reviewed, edited, and verified all material for accuracy and coherence."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Foundations of Analytics & AI",
    "section": "Welcome",
    "text": "Welcome\nFoundations of Analytics & AI is a modular, evolving textbook designed to introduce the core ideas, computational tools, and analytical workflows that underpin modern data-driven decision-making. The text integrates conceptual understanding with hands-on examples. Including:\n\nExplanations of key concepts\n\nDemonstrations and worked Python examples\n\nShort conceptual notes to reinforce essential ideas\n\nMini-labs that integrate skills into complete workflows\n\nThe book is structured so readers can progress linearly or for those with some experience, navigate directly to topics of interest."
  },
  {
    "objectID": "index.html#how-this-textbook-is-organized",
    "href": "index.html#how-this-textbook-is-organized",
    "title": "Foundations of Analytics & AI",
    "section": "How This Textbook Is Organized",
    "text": "How This Textbook Is Organized\nThe material is grouped into three major parts: foundational concepts → computational skills → applied machine learning and AI methods.\n\n\nPart I — Conceptual Foundations of Analytics & AI\nThese chapters develop the core ideas that unify analytics, machine learning, and AI systems. They are concept-only and require no coding background.\n\nChapter 1 — What Is Analytics and AI?\n\nChapter 2 — AI Systems: Data, Models, and Logic\n\nChapter 3 — Data Pipelines & Decision Frameworks\n\n\n\n\nPart II — Python & Data Foundations\nThese chapters introduce programming concepts and data skills used throughout modern analytics and AI practice.\nThey combine explanation with practical Python examples.\n\nChapter 4 — Python Execution Foundations\n\nChapter 5 — Python Basics: Data, Control, Functions\n\nChapter 6 — Working with Tabular Data in Pandas\n\nChapter 7 — JSON and APIs for Data Access\n\nChapter 8 — Simulation and Synthetic Data\n\nChapter 9 — Visualization: From Data to Insight\n\nReaders already familiar with Python may skim or skip based on experience.\n\n\n\nPart III — Machine Learning & Applied AI\n(Chapters appear as they are completed.)\nEach chapter in this part will integrate:\n\nConceptual Foundations\n\nPython Implementation\n\nInterpretation & Diagnostics\n\nMini-Labs\n\nPlanned topics include:\n\nSupervised learning\n\nRegression and classification\n\nModel evaluation and diagnostics\n\nOverfitting and the bias–variance tradeoff\n\nML pipelines\n\nUnsupervised learning and clustering\n\nPCA and dimensionality reduction\n\nNeural networks and deep learning\n\nConvolutional networks\n\nLarge Language Models (LLMs)\n\nRetrieval-Augmented Generation (RAG)\n\nAs chapters are released, links will appear here."
  },
  {
    "objectID": "index.html#learning-philosophy",
    "href": "index.html#learning-philosophy",
    "title": "Foundations of Analytics & AI",
    "section": "Learning Philosophy",
    "text": "Learning Philosophy\nThis textbook is built around two reinforcing goals:\n\nAI Literacy\nUnderstanding how analytical and AI systems are structured, why they behave as they do, and how they influence decision-making.\n\n\nPractical Capabilities\nDeveloping the ability to implement real analytical workflows using Python, diagnostic tools, and modern data processing libraries."
  },
  {
    "objectID": "index.html#accessing-code-and-data",
    "href": "index.html#accessing-code-and-data",
    "title": "Foundations of Analytics & AI",
    "section": "Accessing Code and Data",
    "text": "Accessing Code and Data\nAll example code and supporting data for the text are provided in organized folders:\n\ncode/ — downloadable Python scripts and examples\n\ndata/ — datasets used in demonstrations and mini-labs\n\nDownload the full repository or browse files individually."
  },
  {
    "objectID": "index.html#copyright-and-use",
    "href": "index.html#copyright-and-use",
    "title": "Foundations of Analytics & AI",
    "section": "Copyright and Use",
    "text": "Copyright and Use\n© 2026 Joel Davis. This work,including all text, figures, and diagrams, may be used, shared, adapted, remixed, and redistributed for any purpose, including commercial use. No permission is required. Attribution is appreciated but not required. This content is provided as-is, without warranty.\nThis work is dedicated to the public domain under the Creative Commons CC0 1.0 Universal license.\nWelcome to Foundations of Analytics & AI.\nHave fun!"
  },
  {
    "objectID": "textbook_src/ch6_working_with_tabular_data_pandas.html#data-concepts-what-is-a-dataset",
    "href": "textbook_src/ch6_working_with_tabular_data_pandas.html#data-concepts-what-is-a-dataset",
    "title": "Working with Tabular Data in Pandas",
    "section": "Data Concepts: What Is a Dataset?",
    "text": "Data Concepts: What Is a Dataset?\nBefore working with tools like Pandas or loading files into Python, it is important to clarify what we mean by a dataset. In analytics and AI, datasets are not just collections of numbers or text—they are structured representations of observations about the world. How data is organized determines what kinds of questions can be asked and what kinds of analysis are possible.\n\nWhat a dataset represents\nA dataset is a structured collection of observations. Each observation represents a single instance, case, or entity, and each observation is described using a consistent set of attributes.\nIn most analytical contexts, datasets are organized in a tabular form:\n- Rows represent individual observations or records.\n- Columns represent variables or attributes measured for each observation.\nThis is probably a familiar structure. Most excel worksheets are organized in the same way. For example, a dataset of students might contain one row per student and columns for attributes such as major, exam score, or graduation year. A dataset of transactions might contain one row per transaction and columns describing amount, date, or location.\nThis structure allows datasets to be treated as inputs to analytics and AI systems. Models, summaries, and visualizations all assume that data is organized in a consistent way, where each row means the same thing and each column has a defined interpretation.\nConceptually, a dataset answers the question:\nWhat observations do we have, and what do we know about each one?\n\n\nSchema and structure\nA dataset is more than just values arranged in rows and columns. It also has a schema, which defines the structure and meaning of the data.\nA schema specifies:\n- what columns exist,\n- what each column represents,\n- and what type of data each column contains.\nFor example, a column might represent numeric values, categorical labels, dates, or text. These distinctions matter because different operations are valid for different types of data. Numeric columns can be averaged or summed, while text columns cannot. Boolean columns encode yes/no logic, while categorical columns group observations into meaningful categories.\nStructure is what allows computers to process data reliably. When the structure is clear and consistent, programs can apply the same operations across all rows without ambiguity. When structure is unclear or inconsistent, errors become more likely and results become harder to interpret.\nIn analytics and AI workflows, much of the effort is spent not on modeling itself, but on ensuring that data conforms to an expected schema.\n\n\nStructured vs unstructured data\nNot all data is organized in neat tables. It is useful to distinguish between structured and unstructured data.\nStructured data follows a consistent format, with clearly defined rows, columns, and data types. Examples include spreadsheets, CSV files, and database tables. This kind of data is well suited for tools like Pandas, which are designed to operate on tabular structures.\nUnstructured data, by contrast, does not naturally fit into a fixed table. Examples include free-form text, images, audio recordings, and video. While these data types are extremely important in modern AI systems, they require different representations and tools before they can be analyzed in the same way as structured data.\nMany, but not all, AI workflows begin by transforming unstructured data into structured form. For example, text may be converted into counts, embeddings, or labels; images may be converted into feature vectors. Once data is structured, it can be stored in datasets and processed using familiar analytical tools."
  },
  {
    "objectID": "textbook_src/ch6_working_with_tabular_data_pandas.html#getting-started-with-pandas",
    "href": "textbook_src/ch6_working_with_tabular_data_pandas.html#getting-started-with-pandas",
    "title": "Working with Tabular Data in Pandas",
    "section": "Getting Started with Pandas",
    "text": "Getting Started with Pandas\nAs datasets grow in size and complexity, basic Python data structures such as lists and dictionaries begin to show their limitations. While these structures are essential building blocks, they are not designed to efficiently represent or manipulate large, tabular datasets. Pandas was created to fill this gap.\n\nWhat Pandas is\nLists and dictionaries are flexible and powerful, but they are not well suited for representing tables of data. Lists organize values by position, and dictionaries organize values by keys, but neither naturally represents a dataset with many rows and many columns where operations need to be applied consistently across variables.\nFor example, storing each column of a dataset as a separate list quickly becomes difficult to manage. Ensuring that all lists stay aligned, handling missing values, and performing column-wise operations requires substantial manual effort and careful bookkeeping.\nPandas is a data analysis library designed specifically to address these challenges. It provides data structures and functions that make it easier to load, inspect, clean, transform, and summarize structured data. Rather than working with individual values or small collections, Pandas allows programs to operate directly on entire datasets.\nAt the center of Pandas is the DataFrame, which represents a dataset as a table with labeled columns and indexed rows. This abstraction closely mirrors how analysts and decision-makers think about data, making code more readable and reducing the cognitive gap between analysis intent and implementation.\n\n\nDataFrames and Series\nA DataFrame is Pandas’ primary data structure. It represents data in a two-dimensional, tabular form, with rows and columns. Each column has a name (the column label), and each row has an index that identifies it.\nWithin a DataFrame, each column is represented as a Series. A Series is a one-dimensional array of values with an associated index. While DataFrames represent entire datasets, Series represent individual variables within those datasets.\nThe distinction is important: - A DataFrame represents the whole table. - A Series represents a single column from that table.\nThis structure allows Pandas to apply operations across columns, across rows, or to individual variables in a consistent way. For example, summary statistics can be computed column by column, and filters can be applied row by row.\nThe following example illustrates the creation of a simple DataFrame with two columns:\nimport pandas as pd\ndf = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\nIn this example:\n- The DataFrame has two columns, labeled a and b.\n- Each column is a Series containing numeric values.\n- Each row represents a single observation.\nAlthough this example is small, the same structure scales to datasets with thousands or millions of rows. The consistent organization of rows and columns is what enables Pandas to support efficient data manipulation and analysis."
  },
  {
    "objectID": "textbook_src/ch6_working_with_tabular_data_pandas.html#files-and-paths-in-data-workflows",
    "href": "textbook_src/ch6_working_with_tabular_data_pandas.html#files-and-paths-in-data-workflows",
    "title": "Working with Tabular Data in Pandas",
    "section": "Files and Paths in Data Workflows",
    "text": "Files and Paths in Data Workflows\nAs you begin working with real datasets, managing how and where data is stored becomes a critical part of the workflow. Data used in analytics and AI is almost always stored in external files rather than embedded directly in code. Understanding how Python locates and accesses these files is essential for building reliable and reproducible data workflows.\n\nWhy file paths matter for data\nIn most data workflows, Python scripts and data files are separate. A script contains instructions for what to do, while data files contain the information to be processed. File paths are what connect the two.\nA file path is a description of where a file lives on a computer. When a script loads a dataset, it uses a file path to tell Python where to find that file. If the path is incorrect, the script cannot access the data, regardless of whether the file exists somewhere else on the system.\nThis separation between code and data is intentional. It allows:\n- the same code to be reused with different datasets,\n- data to be updated without changing code,\n- projects to be organized into clear directory structures.\nCorrectly managing file paths is therefore not just a technical detail; it is part of designing a clean and maintainable data workflow.\n\n\nRelative vs absolute paths (revisited)\nRecall that file paths can be either absolute or relative.\nAn absolute path specifies the full location of a file starting from the root of the file system. Absolute paths are precise, but they are tied to a specific machine and directory layout. This makes them fragile when projects are moved or shared.\nA relative path specifies a file’s location relative to the current working directory of the script. Relative paths are preferred in most projects because they make code portable. As long as the internal project structure remains the same, relative paths continue to work across machines and environments.\nIn data workflows, relative paths are especially important because datasets are often stored in project subfolders such as data/. A typical pattern is to keep data files separate from scripts, but within the same project directory.\ndata_path = \"data/sample.csv\"\nThis example assumes that the script is being run from the project’s root directory and that the dataset is located in a folder named data. Using relative paths in this way allows the entire project to be moved or shared without modifying file references.\n\n\nCommon file path errors (also revisted)\nFile path issues are among the most common sources of errors in data workflows. These errors are predictable and usually easy to diagnose once you know what to look for.\nOne frequent error is file not found. This occurs when Python cannot locate the file at the specified path. Common causes include misspelled file names, incorrect extensions, or incorrect assumptions about where the file is stored.\nAnother common issue is running a script from the wrong working directory. Relative paths are interpreted based on the directory from which the script is executed, not the location of the script file itself. If the working directory is not what you expect, relative paths may fail even if the file exists.\nWhen debugging file access issues, it is often helpful to inspect the current working directory and list the files Python can see:\nimport os\nprint(os.getcwd())\nprint(os.listdir())\nThese checks help confirm whether the script is looking in the correct place and whether the expected files are present.\nA useful debugging strategy is to temporarily use an absolute path to confirm that the file can be loaded at all. Once the issue is resolved, the path can be converted back to a relative one to restore portability.\nUnderstanding and resolving file path errors reinforces an important lesson: many data-loading problems are not caused by Pandas or Python itself, but by mismatches between assumptions about directory structure and the actual execution context."
  },
  {
    "objectID": "textbook_src/ch6_working_with_tabular_data_pandas.html#loading-data-from-csv-files",
    "href": "textbook_src/ch6_working_with_tabular_data_pandas.html#loading-data-from-csv-files",
    "title": "Working with Tabular Data in Pandas",
    "section": "Loading Data from CSV Files",
    "text": "Loading Data from CSV Files\nOne of the most common ways datasets are stored and shared is through CSV files. CSV files are simple, flexible, and widely supported, which makes them a standard format in analytics workflows.\n\nCSV files as datasets\nA CSV file (Comma-Separated Values) represents a dataset in a plain-text format. Each line in the file corresponds to a row in the dataset, and values within a row are separated by commas. Typically, the first row contains column names that describe the variables in the dataset.\nConceptually, a CSV file maps directly to the idea of a tabular dataset:\n\neach row represents an observation,\n\neach column represents a variable,\n\nand each cell contains a single value.\n\nBecause CSV files are plain text, they are easy to create, inspect, and share. They can be opened in spreadsheet software, text editors, and programming environments without requiring specialized tools.\nHowever, CSV files also have limitations. They do not explicitly store data types, constraints, or relationships between columns. Everything in a CSV file is initially read as text, and structure must be inferred by the software that loads it. This is why inspection and cleaning steps are so important after loading data.\nDespite these limitations, CSV files remain a foundational format for analytics because they strike a balance between simplicity and usefulness.\n\n\nReading CSV files with Pandas\nPandas provides a dedicated function for loading CSV files into a DataFrame: read_csv. This function reads the contents of a CSV file and constructs a DataFrame where:\n- rows correspond to records,\n- columns correspond to variables,\n- and column labels are inferred from the header row.\ndf = pd.read_csv(\"data/sample.csv\")\nIn this example, the CSV file located at data/sample.csv is read into a DataFrame named df. From this point forward, the dataset can be manipulated using Pandas operations rather than low-level file handling.\nAssigning the result of read_csv to a variable is essential. The DataFrame becomes the central object through which all subsequent inspection, cleaning, transformation, and analysis steps are performed.\nAlthough read_csv has many optional parameters, the default behavior is sufficient for many well-formed datasets. Additional options can be introduced later as data complexity increases.\n\n\nVerifying successful data loading\nAfter loading a dataset, it is important to verify that the data was read correctly. This step helps catch issues early, before errors propagate through later analysis.\nTwo simple checks are especially useful. First, examining the shape of the DataFrame confirms the number of rows and columns:\ndf.shape\nThe shape provides a quick sanity check. If the number of rows or columns is unexpected, it may indicate a problem with the file path, the delimiter, or the structure of the CSV file.\nSecond, inspecting the column names helps verify that variables were read correctly:\ndf.columns\nThis allows you to confirm that column labels match expectations and that no unexpected formatting issues occurred.\nAt this stage, the goal is not to deeply analyze the data, but to establish confidence that the dataset is present, structured, and ready for further inspection. Verifying successful data loading is a small step that prevents much larger problems later in the workflow."
  },
  {
    "objectID": "textbook_src/ch6_working_with_tabular_data_pandas.html#data-cleaning-and-exploration",
    "href": "textbook_src/ch6_working_with_tabular_data_pandas.html#data-cleaning-and-exploration",
    "title": "Working with Tabular Data in Pandas",
    "section": "Data Cleaning and Exploration",
    "text": "Data Cleaning and Exploration\nOnce a dataset has been loaded into a DataFrame, the next step is to understand what you are working with. Data cleaning and exploration begin not by changing anything, but by inspecting structure, contents, and basic properties of the data.\nThis section introduces a small set of inspection tools that provide high-value information early in the workflow. These tools help establish expectations, reveal potential problems, and guide subsequent cleaning and transformation steps.\n\nInspecting data structure\nThe first task after loading a dataset is to examine its overall structure. This involves looking at both the data itself and the metadata Pandas has inferred about it. Data inspection is iterative; unexpected results in summaries or plots should prompt analysts to revisit structure, missing values, and transformations rather than proceeding directly to modeling.\nOne common starting point is to view the first few rows of the dataset. This provides a quick sense of what each column represents and how values are formatted.\ndf.head()\nThe output of head() shows the first rows of the DataFrame, including column names and sample values. This makes it easier to spot obvious issues such as unexpected column names, misaligned values, or formatting problems.\nAnother essential inspection step is examining the data types and completeness of each column.\ndf.info()\nThe info() method provides a summary of the DataFrame, including:\n- the number of rows,\n- the names of columns,\n- the data type inferred for each column,\n- and the count of non-missing values.\nThis information is critical for understanding how Pandas interprets the dataset. For example, a column intended to represent numbers may be interpreted as text, or a column may contain fewer non-null values than expected.\nAt this stage, the goal is not to fix problems, but to identify them. Inspection establishes a baseline understanding of the dataset before any modifications are made.\n\n\nDescriptive summaries\nAfter inspecting structure and types, it is useful to examine summary statistics. Descriptive summaries condense large amounts of data into a small number of informative metrics.\nPandas provides the describe() method for this purpose.\ndf.describe()\nFor numeric columns, describe() typically reports: - count, - mean, - standard deviation, - minimum and maximum values, - and key percentiles.\nThese summaries help reveal the distribution and scale of the data. Extremely large or small values, unexpected ranges, or missing observations can often be identified at this stage.\nBy default, describe() focuses on numeric data. Non-numeric columns, such as strings or categorical labels, require different inspection strategies. This distinction reinforces an important idea: different types of data require different forms of analysis.\nDescriptive summaries do not provide answers on their own, but they guide reasoning. They help determine whether values look reasonable, whether further cleaning is required, and which variables may be relevant for analysis.\nTogether, inspection and descriptive summaries form the foundation of data cleaning. Before transforming or modeling data, it is essential to know what the data contains, how it is structured, and where potential issues may lie."
  },
  {
    "objectID": "textbook_src/ch6_working_with_tabular_data_pandas.html#handling-missing-values",
    "href": "textbook_src/ch6_working_with_tabular_data_pandas.html#handling-missing-values",
    "title": "Working with Tabular Data in Pandas",
    "section": "Handling Missing Values",
    "text": "Handling Missing Values\nMissing data is a common and unavoidable feature of real-world datasets. Values may be absent for many reasons, and how missing data is handled can significantly influence analytical results and model behavior. This section introduces missing values as a concept, shows how to detect them, and outlines simple, practical strategies for dealing with them.\nThe emphasis here is not on finding a single “correct” solution, but on understanding the tradeoffs involved in different approaches.\n\nWhat missing data represents\nA missing value indicates that a data point is absent where a value is expected. In Pandas, missing values are typically represented using special markers that indicate the absence of data rather than a meaningful value.\nMissing data can occur for many reasons:\n- information was not collected,\n- a measurement failed or was skipped,\n- data was lost during transfer or processing,\n- a value was not applicable in a particular context.\nImportantly, missing values are not the same as zero, empty strings, or false values. They represent unknown or unavailable information, and treating them as ordinary values can lead to incorrect conclusions.\nMissing data has important implications for analytics and AI systems. Many statistical operations and models assume complete data, and missing values can cause calculations to fail or produce misleading results. For example, averages may be skewed, relationships may appear weaker or stronger than they truly are, and models may learn patterns based on incomplete information.\nUnderstanding what missing data represents is the first step toward deciding how to handle it responsibly.\n\n\nDetecting missing values\nBefore missing data can be addressed, it must be identified. Pandas provides tools to detect and summarize missing values across a dataset.\nA common approach is to check which values are missing and count how many missing values appear in each column.\ndf.isna().sum()\nThe isna() method returns a DataFrame of boolean values indicating whether each entry is missing. When combined with sum(), it produces a count of missing values for each column.\nThis summary helps answer key questions:\n- Which columns contain missing values?\n- How many values are missing in each column?\n- Are missing values concentrated in specific variables or spread throughout the dataset?\nDetecting missing values early allows informed decisions about whether data cleaning is required and which variables may need special attention.\n\n\nSimple strategies for handling missing data\nOnce missing values have been identified, several basic strategies can be used to handle them. Each approach has advantages and disadvantages, and the appropriate choice depends on the context and goals of the analysis.\nOne simple strategy is dropping rows or columns that contain missing values. This approach is straightforward, but it can result in the loss of potentially valuable data, especially if missing values are common.\nAnother approach is filling missing values with a substitute value. For numeric data, this might involve using a constant, an average, or another summary statistic. For categorical data, a placeholder value may be used. Filling allows the dataset to remain complete, but it introduces assumptions about what the missing values should represent.\nBoth strategies involve tradeoffs:\n- Dropping data reduces sample size but avoids introducing assumptions.\n- Filling data preserves sample size but may distort distributions or relationships.\nAt this stage, the goal is not to apply advanced imputation techniques, but to develop an awareness of how missing data affects analysis and why handling it requires deliberate choice. Simple strategies provide a starting point and help illustrate the consequences of different decisions.\nHandling missing values is an essential step in preparing data for analysis and modeling. Thoughtful treatment of missing data improves the reliability and interpretability of results and lays the groundwork for more advanced techniques later."
  },
  {
    "objectID": "textbook_src/ch6_working_with_tabular_data_pandas.html#renaming-and-dropping-columns",
    "href": "textbook_src/ch6_working_with_tabular_data_pandas.html#renaming-and-dropping-columns",
    "title": "Working with Tabular Data in Pandas",
    "section": "Renaming and Dropping Columns",
    "text": "Renaming and Dropping Columns\nAfter inspecting a dataset and addressing missing values, a common next step is to clean up the columns themselves. Column names and column selection play a central role in how readable, interpretable, and usable a dataset is. This section focuses on improving dataset clarity by renaming columns and removing those that are unnecessary.\n\nWhy column names matter\nColumn names are not just labels; they are part of the dataset’s schema. They communicate what each variable represents and how it should be interpreted. Clear, consistent column names make data easier to understand, easier to analyze, and less error-prone to work with.\nPoorly chosen column names can introduce confusion. Names may be too vague, too long, inconsistently formatted, or reflect internal system conventions rather than analytical meaning. For example, column names inherited from raw data sources may include abbreviations, spaces, or special characters that make code harder to read and write.\nImproving column names serves several purposes:\n- It increases readability for humans.\n- It reduces the likelihood of mistakes when referencing columns in code.\n- It clarifies the intended meaning of each variable.\nBecause column names are used repeatedly throughout an analysis, treating them as part of the schema—and cleaning them early—pays dividends later in the workflow.\n\n\nRenaming columns\nRenaming columns is a common data-cleaning task. Pandas allows columns to be renamed by providing a mapping from old names to new names. This approach supports incremental cleanup, where only problematic columns are renamed rather than rewriting the entire schema at once.\ndf.rename(columns={\"old\": \"new\"}, inplace=True)\nIn this example, the column originally named \"old\" is renamed to \"new\". Other columns remain unchanged. This targeted approach makes it easier to track changes and reduces the risk of unintended consequences.\nRenaming columns is often used to:\n- replace cryptic or abbreviated names with descriptive ones,\n- standardize capitalization or naming conventions,\n- remove spaces or special characters,\n- align column names with analytical concepts rather than source-system terminology.\nPerforming renaming early in the analysis ensures that subsequent code is easier to read and that variable references are consistent throughout the project.\n\n\nDropping columns\nNot all columns in a dataset are useful for every analysis. Some columns may be redundant, irrelevant, or simply not needed for the current task. Dropping columns reduces dataset complexity and helps focus attention on the variables that matter.\ndf.drop(columns=[\"unused\"], inplace=True)\nIn this example, the column named \"unused\" is removed from the DataFrame. Dropping unnecessary columns can: - reduce memory usage,\n- simplify inspection and analysis,\n- make code easier to understand,\n- and reduce the risk of accidentally using irrelevant variables.\nDeciding which columns to drop is a substantive analytical decision. Removing data too aggressively can eliminate useful information, while keeping too many columns can obscure important patterns. Don’t make these decision too quickly.\nRenaming and dropping columns are small operations individually, but together they play a crucial role in shaping a dataset that is well-structured, interpretable, and ready for further transformation and analysis."
  },
  {
    "objectID": "textbook_src/ch6_working_with_tabular_data_pandas.html#data-transformation-filtering-and-selecting-data",
    "href": "textbook_src/ch6_working_with_tabular_data_pandas.html#data-transformation-filtering-and-selecting-data",
    "title": "Working with Tabular Data in Pandas",
    "section": "Data Transformation: Filtering and Selecting Data",
    "text": "Data Transformation: Filtering and Selecting Data\nOnce a dataset has been loaded, inspected, and cleaned, the next step is often to focus on the parts of the data that matter for a specific question. Data transformation involves selecting relevant variables, filtering observations, and reshaping datasets to support analysis.\nThis section introduces basic selection and filtering techniques in Pandas, emphasizing how these operations help turn raw datasets into analytically useful subsets.\n\nSelecting columns\nSelecting columns allows you to focus on a subset of variables within a dataset. Rather than working with every column at once, column selection narrows attention to the variables that are relevant for a particular analysis.\nColumn selection matters for several reasons:\n- It improves readability by reducing clutter.\n- It makes code more explicit about which variables are being used.\n- It reduces the chance of accidentally incorporating irrelevant data.\nIn Pandas, selecting columns produces a new DataFrame or Series that contains only the specified variables. This operation does not change the original dataset unless explicitly assigned back to it.\nConceptually, selecting columns answers the question:\nWhich variables from this dataset are relevant right now?\nFocusing on relevant columns is an important analytical habit. It encourages intentional use of data rather than treating all available variables as equally important.\n\n\nFiltering rows with conditions\nFiltering rows allows you to select observations that meet specific criteria. Instead of analyzing all rows in a dataset, filtering narrows the dataset to those records that satisfy a condition.\nIn Pandas, row filtering is typically done using boolean masks. A boolean mask is a sequence of True and False values that indicates whether each row meets a condition.\nfiltered = df[df[\"score\"] &gt; 80]\nIn this example, the condition df[\"score\"] &gt; 80 produces a boolean mask. Pandas uses this mask to keep only the rows where the condition is true.\nFiltering rows is a powerful way to explore subsets of data, such as:\n- high-performing observations,\n- records from a specific category,\n- or cases that meet defined thresholds.\nConceptually, filtering answers the question:\nWhich observations should be included in this analysis?\nBecause filtering is based on conditions, it directly connects to the conditional logic introduced earlier in the book.\n\n\nUsing .loc and .iloc\nPandas provides two explicit indexing tools for selecting data: .loc and .iloc. These tools clarify whether selection is based on labels or positions.\n\n.loc is used for label-based selection. It selects rows and columns using index labels and column names.\n.iloc is used for position-based selection. It selects rows and columns using integer positions, similar to list indexing.\n\nChoosing between .loc and .iloc depends on context:\n- Use .loc when working with meaningful labels.\n- Use .iloc when selection depends on row or column position.\nAlthough basic filtering and selection can be done without these methods, .loc and .iloc become increasingly important as datasets grow more complex and analyses become more detailed."
  },
  {
    "objectID": "textbook_src/ch6_working_with_tabular_data_pandas.html#descriptive-statistics-with-pandas",
    "href": "textbook_src/ch6_working_with_tabular_data_pandas.html#descriptive-statistics-with-pandas",
    "title": "Working with Tabular Data in Pandas",
    "section": "Descriptive Statistics with Pandas",
    "text": "Descriptive Statistics with Pandas\nAfter data has been cleaned, filtered, and transformed, the next step is often to summarize what the data contains. Descriptive statistics provide a way to move from raw rows and columns to interpretable information that supports reasoning and decision-making.\n\nWhy summarization matters\nRaw datasets can be large and difficult to interpret directly. Even after filtering and cleaning, looking at individual rows rarely provides a clear picture of overall patterns or tendencies.\nSummarization condenses many observations into a small number of meaningful quantities. These summaries help answer questions such as:\n- What is typical in this dataset?\n- How much variation exists?\n- Are values generally large or small?\n- Are there obvious extremes or anomalies?\nDescriptive statistics are often the first step in turning data into insight. They provide critical context before more advanced analysis is attempted. In analytics and AI workflows, descriptive summaries help analysts understand what the model will see and what assumptions may be reasonable.\n\n\n\n\n\n\nNote\n\n\n\nDon’t skip this step! This is often overlooked as being too simple, and not providing the same rich detail a more complicated model provides. But often, this kind of simple is exactly what is needed.\n\n\n\n\nCommon summary statistics\nPandas provides convenient methods for computing common descriptive statistics. These statistics describe central tendency, spread, and range.\nSome of the most frequently used summary statistics include:\n- count, the number of non-missing values,\n- mean, the average value,\n- median, the middle value,\n- minimum and maximum, which describe the range of values.\nFor example, computing the average of a numeric column can be done directly:\ndf[\"score\"].mean()\nThese operations aggregate information across all rows in a column, producing a single value that summarizes the data. Similar methods exist for other statistics, and many can be applied column by column across an entire DataFrame.\nSummary statistics are especially useful for identifying potential issues: - unexpected ranges,\n- unusually large or small values,\n- or discrepancies between measures such as mean and median.\nWhile these statistics are simple, they play an essential role in exploratory analysis. They help determine whether further cleaning is needed and guide decisions about what analyses are appropriate.\n\n\nGroup-level summaries (conceptual preview)\nIn many datasets, observations belong to meaningful groups or categories. For example, data may be grouped by region, category, or time period. Summarizing data across the entire dataset can obscure important differences between groups.\nGroup-level summaries address this by computing descriptive statistics within categories rather than across all observations at once. This allows comparisons such as:\n- average scores by group,\n- counts by category,\n- or ranges within subpopulations.\nGroup-level summaries allow analysts to move from “overall” descriptions to more nuanced views that reveal structure within the data.\nGroup-level summaries prepare the ground for deeper analysis, including comparisons, modeling, and evaluation. They represent a natural progression from understanding individual variables to understanding relationships between variables and categories."
  },
  {
    "objectID": "textbook_src/ch6_working_with_tabular_data_pandas.html#chapter-summary",
    "href": "textbook_src/ch6_working_with_tabular_data_pandas.html#chapter-summary",
    "title": "Working with Tabular Data in Pandas",
    "section": "Chapter Summary",
    "text": "Chapter Summary\nThis chapter focused on working with data as an object of analysis, rather than as isolated values or small collections. The emphasis shifted from writing Python logic to understanding how real-world data is structured, accessed, cleaned, transformed, and summarized in preparation for analytics and AI tasks.\nConceptually, the chapter introduced datasets as structured collections of observations governed by a schema. Understanding rows as records and columns as variables provided a foundation for reasoning about data quality, consistency, and meaning. The distinction between structured and unstructured data clarified why tools like Pandas are central to analytics workflows and how unstructured data is often transformed before analysis.\nPandas was introduced as a way to treat datasets as first-class objects through DataFrames and Series. This abstraction made it possible to load data from external files, inspect structure and types, handle missing values, and clean schemas in a systematic way. Rather than viewing data cleaning as a peripheral task, the chapter emphasized it as a core part of responsible analysis.\nThe chapter also highlighted how data location and access shape workflows. File paths and relative directories were treated as integral components of data pipelines rather than technical afterthoughts. Understanding how data is retrieved from CSV files expanded the range of data sources that Python programs can work with.\nThrough filtering, selection, and descriptive statistics, the chapter demonstrated how raw datasets are transformed into interpretable summaries. These summaries do not provide final answers, but they support sense-making, guide further analysis, and surface potential data quality issues. The mini-lab reinforced this end-to-end workflow by integrating import, inspection, cleaning, transformation, and interpretation into a single analytical process.\nData workflows form a dependency chain: choices made during loading and cleaning directly shape summaries, visualizations, and interpretations later in the analysis. This is often overlooked, even though it seems obvious on the surface.\nBy the end of this chapter, you should be able to move confidently from raw data to cleaned, summarized information, while understanding how each step affects the results. These skills form the foundation for the next stage of the book, where datasets are no longer just described, but used to support modeling, prediction, and decision-making."
  },
  {
    "objectID": "textbook_src/ch4_terminal_paths_envs.html",
    "href": "textbook_src/ch4_terminal_paths_envs.html",
    "title": "Terminal, Paths, and Virtual Environments",
    "section": "",
    "text": "This chapter explains how Python scripts are located and executed on a computer by introducing the basic ideas behind the terminal, file systems, and paths. Then we introduce virtual environments as a way to control which version of Python and which packages are used for a given project."
  },
  {
    "objectID": "textbook_src/ch4_terminal_paths_envs.html#overview",
    "href": "textbook_src/ch4_terminal_paths_envs.html#overview",
    "title": "Terminal, Paths, and Virtual Environments",
    "section": "",
    "text": "This chapter explains how Python scripts are located and executed on a computer by introducing the basic ideas behind the terminal, file systems, and paths. Then we introduce virtual environments as a way to control which version of Python and which packages are used for a given project."
  },
  {
    "objectID": "textbook_src/ch4_terminal_paths_envs.html#opening-the-terminal",
    "href": "textbook_src/ch4_terminal_paths_envs.html#opening-the-terminal",
    "title": "Terminal, Paths, and Virtual Environments",
    "section": "Opening the Terminal",
    "text": "Opening the Terminal\nThe exact steps to opening a terminal depend on the operating system and tools being used, but the underlying concept is the same: opening a window where text-based commands can be entered and executed.\nOn macOS, the terminal application is called Terminal. It can be opened in several ways:\n\nBy using Spotlight search and typing “Terminal”\n\nBy navigating to Applications → Utilities → Terminal\n\nOnce opened, the Terminal window provides direct access to the macOS command line.\nOn Windows, the terminal experience may appear under different names depending on configuration. Common options include:\n\nCommand Prompt\n\nWindows PowerShell\n\nWindows Terminal (a newer application that supports multiple shells)\n\nAny of these can be opened by searching from the Start menu. While they may look slightly different, they all allow commands to be typed and executed in the same basic way.\nMany developers use Visual Studio Code, which includes an integrated terminal. This terminal runs inside the editor itself and behaves just like a regular system terminal, but with important advantages for programming.\nTo open the integrated terminal in Visual Studio Code:\n\nUse the menu option View → Terminal\n\nOr use the keyboard shortcut that opens the terminal panel\n\nThe integrated terminal often starts in the context of the current project folder, which reduces the need to navigate manually through the file system. This makes it especially convenient for running Python scripts and managing project files.\nRegardless of how the terminal is opened, the same ideas apply. Commands are typed, executed, and produce output."
  },
  {
    "objectID": "textbook_src/ch4_terminal_paths_envs.html#what-the-terminal-is-and-why-it-matters",
    "href": "textbook_src/ch4_terminal_paths_envs.html#what-the-terminal-is-and-why-it-matters",
    "title": "Terminal, Paths, and Virtual Environments",
    "section": "What the Terminal Is and Why It Matters",
    "text": "What the Terminal Is and Why It Matters\nThe terminal is a text-based interface for interacting directly with a computer’s operating system. Instead of clicking on icons or navigating menus, instructions are typed as commands. The computer executes those commands and returns output in the same window.\nWhile graphical interfaces are designed for ease of use, the terminal is designed for precision and control. It allows you to specify exactly what you want the computer to do, where to do it, and how to do it.\nIn a graphical interface, running a program often involves clicking on a file. In contrast, running a Python script from the terminal requires two pieces of information:\n\nWhich Python interpreter to use\n\nWhere the script is located\n\nThe terminal makes both of these explicit.\nWhen you type a command into the terminal, you are issuing an instruction and then waiting for a response. For example, a simple command that asks the computer where you currently are in the file system looks like this:\npwd\nAfter pressing Enter, the terminal responds by printing the current working directory. This directory is the location the terminal is “pointing to,” and it determines which files the computer can see when you issue commands.\nThis idea of location is critical. When you run a Python script using a command such as:\npython first_program.py\nPython looks for the file named first_program.py in the current working directory. If the file is not located there, Python cannot run it, even if the file exists elsewhere on your computer. This is one of the most common sources of confusion for beginners, and it highlights why understanding the terminal matters.\nThe terminal also differs from clicking files in an important way: commands are repeatable and explicit. When you type a command, you can see exactly what was executed. This makes it easier to reproduce results, diagnose errors, and understand what the computer is doing step by step.\nEvery terminal interaction follows the same basic pattern:\n\nYou type a command.\n\nThe computer executes it.\n\nThe terminal displays output or an error message.\n\nControl returns to you.\n\nFor example, listing the contents of the current directory in the terminal (MacOS or VSCode terminal) looks like this:\nls\nThe terminal responds by showing the files and folders in that location. This immediate feedback loop—command followed by output—is central to working effectively with the terminal and will be used frequently in analytics and AI workflows.\nThe best way to understand and learn the terminal is not to memorize the commands. Those can be looked up, or cheat sheets printed until they are known. Instead, try to develop a mental model of how the computer interprets instructions, how files are located, and how programs are executed."
  },
  {
    "objectID": "textbook_src/ch4_terminal_paths_envs.html#files-folders-and-working-directories",
    "href": "textbook_src/ch4_terminal_paths_envs.html#files-folders-and-working-directories",
    "title": "Terminal, Paths, and Virtual Environments",
    "section": "Files, Folders, and Working Directories",
    "text": "Files, Folders, and Working Directories\nComputers organize information using files and folders (also called directories). A file contains data or instructions, such as a Python script, while a folder is a container that holds files and other folders. Every file on a computer exists inside exactly one folder, and folders can be nested inside other folders.\nWhen working in the terminal, the computer always keeps track of a single location called the current working directory. This directory represents “where you are” in the file system at any given moment. All commands you type into the terminal are interpreted relative to this location unless you explicitly say otherwise. This is one of biggest sources of confusion and issue with new programmers, so take note how to find and change your working directory. It will avoid a lot of confusion and frustration later.\nYou can ask the terminal to show the current working directory using the following command:\npwd\nThe output shows the full path to the folder the terminal is currently using. This location matters because most commands—including those that run Python scripts—operate on files in this directory by default.\nTo see what files and folders exist in the current working directory, you can list its contents:\nls\nThe terminal responds by displaying the names of files and folders in that location. If a Python script does not appear in this list, it means the terminal cannot “see” it from the current directory.\nThis explains why a command such as:\npython script.py\nonly works when the file named script.py is located in the current working directory. When this command is run, Python looks for script.py in the folder the terminal is currently pointing to. If the file is elsewhere, Python cannot run it and will report that the file cannot be found.\nPrograms locate files using the same logic. When a program is executed, it starts in the current working directory and interprets file references relative to that location. If a script refers to another file without specifying a full path, Python assumes that file is located in—or relative to—the directory where the program was run.\nChanging the current working directory changes what files are visible to the terminal and to any programs launched from it. Moving between folders is a fundamental skill when working with scripts and project-based code."
  },
  {
    "objectID": "textbook_src/ch4_terminal_paths_envs.html#relative-vs-absolute-paths",
    "href": "textbook_src/ch4_terminal_paths_envs.html#relative-vs-absolute-paths",
    "title": "Terminal, Paths, and Virtual Environments",
    "section": "Relative vs Absolute Paths",
    "text": "Relative vs Absolute Paths\nA path is a description of where a file or folder is located on a computer. Paths allow both humans and programs to refer to specific locations in the file system. When working in the terminal, paths are how you tell the computer which file or folder you mean.\nThere are two main types of paths: absolute paths and relative paths. The difference between them depends on where the path begins.\nAn absolute path describes a location starting from the root of the file system. It specifies the full sequence of folders that must be followed to reach a file, regardless of the current working directory. Because absolute paths always start from the same place, they uniquely identify a file’s location.\nFor example, an absolute path might look like this:\n/Users/username/projects/week1/hello_world.py\nThis path tells the computer exactly where the file lives, no matter where the terminal is currently pointed. Absolute paths are precise, but they can be long, system-specific, and inconvenient to type repeatedly.\nA relative path, by contrast, describes a location starting from the current working directory. Instead of beginning at the root of the file system, a relative path is interpreted based on where you are at the moment the command is run.\nFor example, if the terminal is already inside the week1 folder, the same file could be referred to simply as:\nhello_world.py\nRelative paths are shorter and easier to read, but they only make sense in relation to the current working directory. This is why understanding where you are in the file system is so important when using the terminal.\nIn most projects, relative paths are used far more often than absolute paths. Relative paths make code easier to move between computers and directories without modification. If a project folder is copied or shared, relative paths continue to work as long as the internal structure of the project remains the same.\nTwo special symbols are commonly used in relative paths. The symbol . refers to the current directory, while .. refers to the parent directory, which is the folder that contains the current one. These symbols provide a concise way to navigate up and down the folder hierarchy.\nFor example, moving up one level in the directory structure looks like this:\ncd ..\nUsing these symbols allows paths to express relationships between folders rather than fixed locations. This relational view of file locations is central to working effectively with scripts, projects, and command-line tools."
  },
  {
    "objectID": "textbook_src/ch4_terminal_paths_envs.html#essential-navigation-commands-conceptual-overview",
    "href": "textbook_src/ch4_terminal_paths_envs.html#essential-navigation-commands-conceptual-overview",
    "title": "Terminal, Paths, and Virtual Environments",
    "section": "Essential Navigation Commands (Conceptual Overview)",
    "text": "Essential Navigation Commands (Conceptual Overview)\nWorking in the terminal involves a small set of core commands that allow you to navigate the file system and understand what files are available at any given moment. These commands are used constantly when working with Python scripts, not because they are complex, but because they express intent very directly.\nOne of the most common tasks is moving between folders. This is done by changing the current working directory. Conceptually, this is no different from opening a different folder in a graphical interface, except that it is done by issuing a command rather than clicking.\nFor example, moving into a folder looks like this:\ncd projects\nAfter this command runs, the terminal’s current working directory changes to the projects folder. All subsequent commands are interpreted relative to this new location.\nAnother common task is listing the contents of a folder. This allows you to see which files and subfolders exist in the current directory:\nls\nListing files is often the first step when something does not work as expected. If a file does not appear in the listing, it means the terminal cannot see it from the current location.\nFolders are also created directly from the terminal. Creating folders is especially useful for organizing projects and keeping related files together:\nmkdir week1\nThis command creates a new folder named week1 inside the current working directory. Organizing code into folders helps keep scripts, data, and outputs clearly separated, which becomes increasingly important as projects grow.\nThese navigation commands matter because the terminal always operates in a specific location. Python scripts are run from that location, files are created there by default, and relative paths are resolved based on it. When a command behaves unexpectedly, the cause is often not the command itself, but the directory from which it was run.\nIf you are new in the terminal, don’t memorize commands, but try to understand their intent:\n\nWhere am I?\n\nWhat files are here?\n\nWhere do I want to go next?\n\nOnce this mental model is in place, the specific command names become easier to remember, and working in the terminal becomes a predictable and logical process rather than a trial-and-error activity."
  },
  {
    "objectID": "textbook_src/ch4_terminal_paths_envs.html#how-the-terminal-connects-to-python-execution",
    "href": "textbook_src/ch4_terminal_paths_envs.html#how-the-terminal-connects-to-python-execution",
    "title": "Terminal, Paths, and Virtual Environments",
    "section": "How the Terminal Connects to Python Execution",
    "text": "How the Terminal Connects to Python Execution\nThe terminal plays a central role in running Python scripts because it provides the context in which commands are interpreted. When a Python script is executed, the terminal is responsible for telling Python which file to run and where to find it.\nWhen you type a command such as:\npython first_program.py\n(note in some MacOS systems you might write python3 instead of python)\nyou are giving Python two pieces of information at once. First, you are specifying that the Python interpreter should be used. Second, you are specifying the name of the file to execute. What is not explicitly stated—but is critically important—is where Python should look for that file.\nBy default, Python looks for the script in the current working directory. That directory is determined entirely by the terminal’s location at the moment the command is run. If the file named first_program.py is located in that directory, Python can execute it. If it is not, Python reports an error indicating that the file cannot be found.\nThis explains why errors such as “file not found” or “no such file or directory” occur so frequently. In many cases, the issue is not that the file does not exist, but that the terminal is pointed at the wrong folder when the command is issued.\nChanging directories changes what files are visible to Python. Listing files allows you to verify whether the script you want to run is actually present in the current location. A simple but powerful workflow:\n\nNavigate to the folder containing the script.\n\nConfirm the file is present.\n\nRun the script using the Python command.\n\nProject folder structure reinforces this workflow. When scripts are organized into predictable folders, it becomes easier to navigate to the correct location and run code reliably. Relative paths and consistent organization reduce the likelihood of execution errors and make projects easier to understand and maintain.\nThis leads to a useful mental model for running Python scripts:\nlocation → command → execution\nFirst, the terminal’s location determines what files are accessible. Next, the command specifies what action to take. Finally, Python executes the script based on that context. When something goes wrong, tracing the problem through these three steps is often the fastest way to identify and resolve the issue.\nUnderstanding this connection between the terminal and Python execution transforms error messages from obstacles into signals. Rather than guessing, it becomes possible to reason systematically about what the computer is doing and why a particular command succeeds or fails."
  },
  {
    "objectID": "textbook_src/ch4_terminal_paths_envs.html#virtual-environments-overview",
    "href": "textbook_src/ch4_terminal_paths_envs.html#virtual-environments-overview",
    "title": "Terminal, Paths, and Virtual Environments",
    "section": "Virtual Environments Overview",
    "text": "Virtual Environments Overview\nA virtual environment defines the execution context of a Python project, including its library versions and dependencies. This isolation ensures that code behaves consistently across machines and over time. Without virtual environments, the same script may run successfully in one setting and fail in another, even when the code itself is unchanged. Virtual environments add an important system level question, and lock in options: which version of Python, and which set of installed tools, should be used when a script runs?\nThis question becomes especially important as projects grow and as additional libraries are introduced. Virtual environments provide a structured way to manage this complexity by controlling the software context in which Python programs execute.\n\nWhy Virtual Environments Exist\nVirtual environments exist to solve a practical problem that arises when working with Python across multiple projects: different projects often require different packages, or worse different versions of the same package.\nBy default, Python allows packages to be installed globally, meaning they are available to all Python programs on a computer. While this may seem convenient at first, it quickly leads to conflicts. One project may require a newer version of a library, while another depends on an older version. Installing or upgrading a package for one project can unintentionally break another.\nThese conflicts are especially common in analytics and AI work, where projects often rely on rapidly evolving libraries. Changes in package behavior, version compatibility, or dependencies can cause code that once worked correctly to fail without any changes to the code itself.\nVirtual environments address this problem by providing isolation. Each environment contains its own copy of the Python interpreter along with its own set of installed packages. This allows each project to define and control exactly which packages and versions it uses, without affecting other projects or the system-wide Python installation.\nImportantly, virtual environments are not about adding complexity for its own sake. They are a way to reduce uncertainty and prevent accidental breakage. By isolating dependencies, environments make projects more predictable, easier to reproduce, and easier to share with others.\nFrom a workflow perspective, virtual environments support a simple principle: a project should carry its own assumptions about its software dependencies. When those assumptions are explicit and isolated, problems become easier to diagnose and fixes become easier to apply.\n\n\nWhat a Virtual Environment Is\nA virtual environment is a self-contained Python setup that exists alongside, but separate from, the system-wide Python installation. It provides a controlled space in which a project can run using a specific Python interpreter and a specific set of installed packages.\nEach virtual environment includes its own Python interpreter and its own package directory. When a program is run inside an environment, Python uses the interpreter and packages associated with that environment rather than those installed globally on the system. This separation is what allows multiple projects with different requirements to coexist on the same computer without interfering with one another.\nVirtual environments are closely tied to two components: the Python interpreter and installed packages. The interpreter determines which version of Python is used, while the installed packages determine which libraries and tools are available to the program. Together, these define the software context in which a script executes.\nWhen a virtual environment is activated, the terminal is instructed to use the environment’s Python interpreter by default. As a result, running Python commands or executing scripts uses the environment’s configuration rather than the system-wide one. Activation does not change Python itself; it changes which Python is selected when commands are run.\nIt is important to understand what activation does not do. Activating a virtual environment does not delete or replace the system Python installation. It does not modify other environments, and it does not affect projects outside the current working context. The system Python remains available and unchanged; the environment simply provides an alternative that is used temporarily.\nThis distinction is essential for developing confidence with environments. Virtual environments do not take control of the computer or permanently alter Python. They provide a scoped, reversible context in which projects can be developed and executed reliably.\n\n\nA Standard Environment Workflow (Using uv)\nTo manage virtual environments consistently, it is helpful to use a single tool. One modern option is uv, which can create and manage environments efficiently. Standardizing on one tool reduces confusion, minimizes setup errors, and helps ensure that examples behave the same way across machines.\nAt a high level, an environment workflow consists of three steps:\n\nCreate an environment\nA new virtual environment is created for the project. This environment serves as an isolated space where the project’s Python interpreter and packages will live.\nActivate the environment\nActivating the environment tells the terminal to use the environment’s Python interpreter by default. From this point forward, Python commands and scripts run within the context of the environment rather than the system-wide installation.\nInstall dependencies\nRequired packages are installed into the environment. These packages become available only within that environment, ensuring that the project’s dependencies are isolated and controlled.\n\nThese steps form a repeatable process that can be used for every project. While specific commands depend on the tool and operating system, the underlying structure remains the same. Understanding this structure makes it easier to reason about environment errors, recover from mistakes, and apply the same approach in future projects.\n\n\nActivating and Deactivating Environments (Conceptual)\nActivating a virtual environment changes how the terminal interprets Python-related commands. Rather than altering Python itself, activation tells the terminal which Python interpreter and which set of packages should be used when commands are executed.\nWhen an environment is activated, the terminal is temporarily configured so that Python commands refer to the environment’s Python interpreter instead of the system-wide one. As a result, any script that is run uses the packages installed in that environment. This is why activation is such an important step in the workflow: it determines the software context in which code executes.\nActivation affects two key things. First, it determines which Python runs. Even if multiple versions of Python exist on a computer, activation ensures that the correct interpreter is used for the current project. Second, it determines which packages are available. Only the packages installed in the active environment can be imported and used by scripts.\nForgetting to activate an environment is a common source of confusion. When this happens, Python may still run, but it may use the system-wide interpreter and packages instead of the project-specific ones. This can lead to errors where code appears correct but fails because required packages are missing or the wrong versions are being used.\nDeactivating an environment simply returns the terminal to its default state. After deactivation, Python commands once again refer to the system-wide Python installation. No files are deleted, and no environments are removed; the change is temporary and reversible.\nUnderstanding activation and deactivation reinforces an important idea: virtual environments are contexts, not permanent changes. They define how Python behaves in a given terminal session, and they can be entered and exited as needed.\n\n\nHow Environments Connect to the Terminal and Visual Studio Code\nVirtual environments are closely tied to the terminal session in which they are activated. When an environment is activated, the change applies only to that specific terminal window. Other terminal windows remain unaffected unless the environment is activated there as well. This session-specific behavior is intentional and allows different projects to be worked on simultaneously using different environments.\nBecause activation is terminal-specific, it is possible for two terminals on the same computer to behave differently at the same time. One terminal may be using a project’s virtual environment, while another is using the system-wide Python installation. Understanding this distinction helps explain why code may run successfully in one terminal but fail in another.\nVisual Studio Code builds on this behavior by integrating the terminal and the editor. When a project folder is opened in VS Code, the editor can detect virtual environments. If an environment is found, VS Code can associate that environment with the project and use it when running Python scripts.\nVS Code’s Python tooling uses this association to determine which Python interpreter should be used for execution, linting, and debugging. When the correct environment is selected, running a script from the editor or from the integrated terminal uses the same Python configuration. This alignment reduces confusion and helps ensure consistent behavior across tools.\nAt this point, several concepts come together:\n\nThe terminal location determines which files are visible.\n\nThe active environment determines which Python interpreter and packages are used.\n\nThe execution command tells Python which script to run.\n\nAll three must align for code to run as expected. If any one of them is incorrect—wrong directory, wrong environment, or wrong command—errors can occur even if the code itself is correct.\nViewing these elements as a coordinated system rather than independent pieces makes troubleshooting much easier. Instead of guessing, it becomes possible to check each part in turn: where the terminal is, which environment is active, and which Python is being used.\n\n\nCommon Environment Mistakes and Recovery Strategies\nMistakes involving virtual environments are extremely common and occur at all experience levels. These issues rarely indicate a problem with the code itself. Instead, they usually arise from a mismatch between the environment that was intended and the one that is actually being used.\nOne frequent mistake is installing packages in the wrong environment. This happens when packages are installed while the system-wide Python environment is active instead of the project’s virtual environment. As a result, a script may fail to import a package even though it appears to be installed. The package exists—but not in the environment the script is using.\nAnother common issue is running scripts with the wrong Python interpreter. Because multiple Python interpreters can exist on the same computer, it is possible to run a script using a different interpreter than expected. When this occurs, the script may behave inconsistently across terminals or tools, even though no changes were made to the code.\nIt is also easy to forget which environment is active, especially when switching between projects or terminal windows. Since activation is specific to each terminal session, opening a new terminal often means starting without any environment activated. This can lead to confusion when previously working code suddenly fails.\nRecovering from environment-related issues usually involves a small number of simple checks:\n\nConfirm which environment is active in the current terminal.\n\nVerify which Python interpreter is being used.\n\nEnsure required packages are installed in that environment.\n\nRe-activate the intended environment if necessary.\n\nThese steps are effective because they reestablish clarity about the execution context before any changes are made. Guessing or reinstalling packages repeatedly is rarely helpful without first confirming which environment is actually in use.\nMost importantly, these mistakes are routine and fixable. Even experienced developers encounter them regularly, particularly when working across multiple projects or tools. With practice, identifying environment issues becomes a quick diagnostic step rather than a source of frustration.\nVirtual environments are designed to make work more reliable, not more fragile. Once their role is understood, environment-related problems become signals to check context rather than obstacles to progress."
  },
  {
    "objectID": "textbook_src/ch8_simulation_synthetic_data.html",
    "href": "textbook_src/ch8_simulation_synthetic_data.html",
    "title": "Simulation and Synthetic Data",
    "section": "",
    "text": "(chapter is incomplete)"
  },
  {
    "objectID": "textbook_src/ch8_simulation_synthetic_data.html#overview",
    "href": "textbook_src/ch8_simulation_synthetic_data.html#overview",
    "title": "Simulation and Synthetic Data",
    "section": "",
    "text": "(chapter is incomplete)"
  },
  {
    "objectID": "textbook_src/ch8_simulation_synthetic_data.html#why-simulate-data",
    "href": "textbook_src/ch8_simulation_synthetic_data.html#why-simulate-data",
    "title": "Simulation and Synthetic Data",
    "section": "Why Simulate Data?",
    "text": "Why Simulate Data?\nReal datasets often suffer from limitations:\n\nMissing or incomplete data\nNon-representative observations\nPrivacy restrictions or regulatory barriers\nNew systems lacking historical data\n\nSimulation addresses these gaps by enabling “what if” exploration:\n\nWhat would outcomes look like under different assumptions?\nHow sensitive are results to randomness?\nWhat patterns should we expect in idealized conditions?\n\nSimulation complements, rather than replaces, real data. Real-world data tells us what has happened; simulated data helps us understand what could happen."
  },
  {
    "objectID": "textbook_src/ch8_simulation_synthetic_data.html#randomness-and-stochastic-processes",
    "href": "textbook_src/ch8_simulation_synthetic_data.html#randomness-and-stochastic-processes",
    "title": "Simulation and Synthetic Data",
    "section": "Randomness and Stochastic Processes",
    "text": "Randomness and Stochastic Processes\nComputers do not produce true randomness, they generate pseudo-random values: sequences that appear random but are produced by deterministic algorithms.\nPython’s random module provides a simple interface:\nimport random\nrandom.random()   # Returns a value between 0 and 1\nRepeated calls produce a distribution of values suitable for modeling noise and uncertainty.\n\nStochastic Processes\nA process involving randomness is called stochastic. Many real-world systems behave stochastically:\n\nCustomer arrivals\nWebsite traffic patterns\nWeather fluctuations\nSensor noise\n\nStochastic processes generate distributions of outcomes, not a single deterministic result.\n\n\nSeeds and Reproducibility\nSince pseudo-random sequences are deterministic, setting a seed ensures repeatability:\nrandom.seed(1955)\nThis is essential for debugging, experimentation, and scientific reproducibility.\nRandomness introduces controlled variability, which is what makes simulation valuable for exploring uncertainty."
  },
  {
    "objectID": "textbook_src/ch8_simulation_synthetic_data.html#generating-synthetic-datasets",
    "href": "textbook_src/ch8_simulation_synthetic_data.html#generating-synthetic-datasets",
    "title": "Simulation and Synthetic Data",
    "section": "Generating Synthetic Datasets",
    "text": "Generating Synthetic Datasets\nSynthetic datasets allow analysts to explore patterns and test workflows before relying on real-world data.\n\nSimulating Numeric Variables\nA simple simulation of 100 random values:\nimport random\nvalues = [random.random() for _ in range(100)]\nThese values can be summarized, visualized, or transformed. Questions that arise include:\n\nIs the distribution uniform?\nHow does the mean behave with larger samples?\nWhat happens if noise is added?\n\nNumeric simulation builds intuition about randomness and distributions.\n\n\n\nSimulating Categorical Variables\nMany datasets include categorical attributes such as:\n\nGender\nProduct categories\nRegions\nUser types\n\nSimulating categories:\ncategories = [\"A\", \"B\", \"C\"]\nassignments = [random.choice(categories) for _ in range(100)]\nCategorical simulation is useful for:\n\nTesting grouping/aggregation logic\nExploring category imbalance\nComparing visual patterns across groups\n\n\n\n\nCombining Variables into Structured Datasets\nMost real datasets contain multiple attributes. We can simulate data row-by-row:\nimport random\n\ndata = []\nfor _ in range(100):\n    record = {\n        \"value\": random.random(),\n        \"group\": random.choice([\"A\", \"B\"])\n    }\n    data.append(record)\nThis mimics real records and supports:\n\nFiltering by category\nGroupwise summarization\nRelationship exploration\n\n\n\n\nSimulation as a Tool for Testing Analytical Logic\nOne of simulation’s most important uses is verifying your workflow:\n\nDoes filtering logic behave as expected?\nDo summary statistics match known patterns?\nDoes a visualization reflect the data-generating process?\n\nBecause assumptions are controlled, simulated datasets act as unit tests for analytics.\nIn machine learning, simulation helps:\n\nTest preprocessing and modeling pipelines\nExamine behavior under noise or drift\nValidate system constraints before real deployment"
  },
  {
    "objectID": "textbook_src/ch8_simulation_synthetic_data.html#mini-exercise-build-a-simple-synthetic-dataset",
    "href": "textbook_src/ch8_simulation_synthetic_data.html#mini-exercise-build-a-simple-synthetic-dataset",
    "title": "Simulation and Synthetic Data",
    "section": "Mini-Exercise: Build a Simple Synthetic Dataset",
    "text": "Mini-Exercise: Build a Simple Synthetic Dataset\nimport random\n\nrandom.seed(123)\n\ndata = []\nfor _ in range(200):\n    record = {\n        \"measurement\": random.gauss(mu=50, sigma=10),\n        \"category\": random.choice([\"control\", \"treated\"])\n    }\n    data.append(record)\n\ndata[:5]\nThis produces a Gaussian-distributed numeric variable and a binary category—useful for early testing of:\n\nFiltering\n\nGroup comparisons\n\nVisualization\n\nRegression or classification pipelines"
  },
  {
    "objectID": "textbook_src/ch9_visualization_from_data_to_insight.html",
    "href": "textbook_src/ch9_visualization_from_data_to_insight.html",
    "title": "Visualization: From Data to Insight",
    "section": "",
    "text": "Visualization translates data into visual form, enabling humans to detect patterns that may be invisible in raw numbers. This chapter introduces visualization as a reasoning tool, teaches fundamental plot types using matplotlib and seaborn, and explains how to interpret visuals with clarity—not just read them, but understand them."
  },
  {
    "objectID": "textbook_src/ch9_visualization_from_data_to_insight.html#overview",
    "href": "textbook_src/ch9_visualization_from_data_to_insight.html#overview",
    "title": "Visualization: From Data to Insight",
    "section": "",
    "text": "Visualization translates data into visual form, enabling humans to detect patterns that may be invisible in raw numbers. This chapter introduces visualization as a reasoning tool, teaches fundamental plot types using matplotlib and seaborn, and explains how to interpret visuals with clarity—not just read them, but understand them."
  },
  {
    "objectID": "textbook_src/ch9_visualization_from_data_to_insight.html#why-visualization-matters",
    "href": "textbook_src/ch9_visualization_from_data_to_insight.html#why-visualization-matters",
    "title": "Visualization: From Data to Insight",
    "section": "Why Visualization Matters",
    "text": "Why Visualization Matters\n\nVisualization as a Reasoning Tool\nTables of numbers impose heavy cognitive load. Humans are far better at perceiving:\n\nTrends\nClusters\nRelationships\nAnomalies\nDistributions\n\nExploratory visualizations help analysts:\n\nDiagnose data quality issues\n\nIdentify unexpected patterns\n\nForm hypotheses\n\nGuide modeling choices\n\nUnderstand uncertainty"
  },
  {
    "objectID": "textbook_src/ch9_visualization_from_data_to_insight.html#fundamentals-of-visual-representation",
    "href": "textbook_src/ch9_visualization_from_data_to_insight.html#fundamentals-of-visual-representation",
    "title": "Visualization: From Data to Insight",
    "section": "Fundamentals of Visual Representation",
    "text": "Fundamentals of Visual Representation\nBefore diving into specific plot types, it helps to recognize that every visualization—no matter how simple or complex—relies on encodings, the rules that map data values to visual properties. These encodings are the “language” of visualization: they determine what information is communicated, how accurately it is perceived, and how easily viewers can extract meaning from what they see.\nCommon encodings include:\n\nPosition\n\nLength\n\nAngle\n\nColor\n\nShape\n\nDensity\n\nSome encodings are more precise than others. Position on a common scale, for example, is one of the most accurate ways humans perceive quantitative differences, which is why scatter plots and line plots are so effective. Length is also interpreted reliably, making bar charts useful for comparing magnitudes. Color, angle, and shape communicate useful information but with less precision; they are powerful when used deliberately, but can become ambiguous or misleading when overused.\nA visualization succeeds only when its encodings clearly represent the underlying data. If the chosen encoding does not match the structure of the variable, for example, using color to represent a subtle numeric difference, or using position for unordered categories—the resulting plot may confuse more than it clarifies. Effective analysts therefore develop an awareness not just of what a plot shows, but how it shows it, and why that encoding is appropriate for the question at hand."
  },
  {
    "objectID": "textbook_src/ch9_visualization_from_data_to_insight.html#core-plot-types",
    "href": "textbook_src/ch9_visualization_from_data_to_insight.html#core-plot-types",
    "title": "Visualization: From Data to Insight",
    "section": "Core Plot Types",
    "text": "Core Plot Types\n(examples in Python’s matplotlib)\n\nLine Plots\n\n\nLine Plots\nA line plot shows how a quantity changes across an ordered sequence, most commonly time. Each point represents a single observation, and the line connecting allows the reader to see the display as a trajectory rather than a collection of isolated values. This makes line plots especially effective for questions like “Are we improving?” or “When did things change?” rather than “How big is this number in isolation?”\nLine plots are appropriate whenever the horizontal axis has a meaningful order: dates, time steps, ranks, or indices. In these cases, the continuity of the line reflects the continuity of the underlying process. When that order is real, a line plot can reveal long-term trends, local spikes or dips, seasonal patterns, and periods of stability or volatility. For example, plotting daily website traffic as a line allows you to quickly see weekdays versus weekends, holiday surges, or the impact of a new marketing campaign.\nWhen reading a line plot, look for:\n\nOverall trend: increasing, decreasing, or flat.\nLocal patterns: spikes, dips, or plateaus.\nSeasonality or cycles: repeating patterns across intervals.\nVolatility: how jagged or smooth the line appears.\nStructural breaks: points where the pattern changes abruptly.\n\nSeveral pitfalls are common with line plots. The first is using them when there is no natural order on the x-axis. This is generally not an appropriate chart in this case. For example, connecting product categories or regions. In those cases, the line suggests a trend that does not actually exist. A second issue arises when too many series are plotted on the same axes: the resulting “spaghetti plot” becomes difficult to read, and subtle patterns disappear into visual clutter. Finally, axis scaling matters greatly. Truncating the y-axis or using different scales on similar plots can unintentionally exaggerate or hide meaningful changes.\nplt.plot(values)\nplt.title(\"Simulated Time Series\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"Value\")\n\n\n\nBar Plots\nBar plots represent values for discrete categories using the height or length of bars. Because the visual encoding relies on comparing bar lengths, bar plots are well suited for questions about magnitude and contrast: “Which category is largest?”, “How do these groups differ?”, or “Where is performance strongest or weakest?”\nBar plots are most appropriate when the categories on the horizontal axis are distinct, unordered groups. Think product types, customer segments, or regions. In these cases, bars provide a clean visual summary of how totals, averages, or counts differ across groups. A well-designed bar plot can immediately reveal whether one category dominates, whether values are relatively uniform, or whether the distribution is highly imbalanced. Small choices, such as sorting bars from largest to smallest, can make important patterns easier to see.\nWhen reading a bar plot, look for:\n\nDifferences in bar height, especially between adjacent categories.\n\nThe ordering of categories—sorted bars reveal structure more clearly.\n\nCategories that stand out as unusually high or low.\n\nWhether differences appear substantively meaningful or trivially small.\n\nOne of the most common pitfalls in bar charts is starting the vertical axis above zero, which exaggerates differences in bar height and can mislead interpretation. Another is attempting to show too many categories at once; long category labels or dense clusters of bars quickly overwhelm the visual space. Stacked bars are often misused as well—while they save space, they make it difficult to compare components that are not anchored to a common baseline. Finally, bar plots are not appropriate for continuous or time-ordered data; a line plot or scatter plot communicates those relationships more effectively.\nplt.bar([\"A\", \"B\", \"C\"], [10, 15, 7])\nplt.title(\"Category Counts\")\nplt.ylabel(\"Count\")\n\n\n\nScatter Plots\nScatter plots display pairs of numeric values as points on a two-dimensional coordinate system. Each point represents a single observation, and its location encodes the relationship between two variables. This makes scatter plots invaluable when the goal is to understand how one quantity changes with another, whether a relationship appears linear or nonlinear, or whether distinct clusters or unusual points emerge from the data. Unlike bar or line plots, scatter plots emphasize relationships rather than comparisons or sequences.\nScatter plots are most appropriate when both variables are continuous and measured on a meaningful numeric scale. In these situations, the arrangement of points can reveal patterns that are otherwise invisible in summary statistics alone. A tight, upward-sloping cloud of points suggests a strong positive association, whereas a widely dispersed cloud implies a weaker relationship. Curved or funnel-shaped patterns may indicate nonlinearities, heteroskedasticity, or the presence of subgroups. Scatter plots are also often the first tool analysts use to identify outliers, which may reflect extreme cases, data-entry errors, or important special conditions worth investigating.\nWhen reading a scatter plot, look for:\n\nDirection: whether the cloud slopes upward, downward, or shows no consistent pattern.\n\nStrength: how tightly the points cluster around an imagined line or curve.\n\nLinearity vs. curvature: whether a straight line adequately summarizes the pattern.\n\nClusters or subgroups: evidence of multiple populations within the same display.\n\nOutliers: points far from the main cloud that may influence summary statistics or models.\n\nOne frequent issue is overplotting, when thousands of points overlap, the resulting chart just looks like big “ink blob” and hides the true structure of the data. Transparency, hexbin plots, or sampling strategies can help mitigate this. Another pitfall is treating correlation as causation: even though we know we shouldn’t, scatterplots make this almost subconcious. Even a strong visual pattern does not reveal which variable influences the other, nor does it rule out the effect of unobserved confounders. Like with other plots, analysts must also be careful to watch axis scaling; log or standardized scales may be necessary when values span several orders of magnitude. Finally, outliers deserve careful attention: a single extreme point can dramatically change a model’s slope or correlation estimate, making it important to decide whether it reflects real variation or a data-quality issue.\nplt.scatter(x, y)\nplt.title(\"Scatter Plot Example\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\n\n\n\nHistograms\nRather than showing individual data points, a histogram groups values into bins and displays how many observations fall into each bin. This allows the reader to see the overall shape of the data, whether values cluster in certain regions, whether the distribution is symmetric or skewed, and whether the data exhibit multiple peaks. Histograms are often the first visualization analysts turn to when exploring an unfamiliar dataset because they reveal structure that cannot be inferred from simple summary statistics alone.\nHistograms are most effective when the underlying variable is continuous or takes on many distinct values. In these cases, the binning process provides a useful abstraction: the analyst sees patterns in density and concentration rather than a long list of raw values. For example, a histogram of transaction amounts may reveal heavy right-skew due to a small number of very large purchases, or a histogram of sensor readings may show several peaks corresponding to different operating states of a machine. Because histograms emphasize distributional shape, they are particularly helpful in diagnosing data quality issues, such as impossible values, unexpected gaps, or measurement artifacts.\nWhen reading a histogram, look for:\n\nShape: whether the distribution is symmetric, skewed, flat, or multi-peaked.\n\nSpread: how wide the distribution is and whether values are tightly clustered or dispersed.\n\nCentral tendency: the approximate region where most values fall.\n\nGaps: intervals with unexpectedly few observations, which may signal missing data or structural boundaries.\n\nOutliers: isolated bars far from the main concentration of values.\n\nHistograms also have several common pitfalls. The most important is sensitivity to bin width: too few bins oversimplify the distribution and hide meaningful structure, while too many bins create visual noise and obscure broader patterns. For this reason, comparing histograms requires consistent bin choices, otherwise differences may reflect bin settings rather than real properties of the data. Another pitfall arises when comparing groups with different sample sizes; one distribution may appear “taller” simply because it contains more observations. In such cases, density plots or standardized histograms provide a clearer comparison. Finally, histograms assume continuous or near-continuous data; when variables are naturally discrete, bar plots or empirical frequency tables may offer better clarity.\nplt.hist(values, bins=20)\nplt.title(\"Histogram of Values\")\n\n\nBoxplots\nBoxplots provide a compact summary of a distribution by highlighting its median, quartiles, and potential outliers. Unlike histograms, which emphasize the full shape of a distribution, boxplots focus on key summary statistics. This makes them particularly well suited for comparing multiple groups side by side. A single box encodes where the “middle” of the data lies, how spread out the values are, and whether there are observations that fall far from the typical range. Because of their condensed form, boxplots are widely used in exploratory data analysis, quality control, and any setting where many distributions must be compared quickly.\nBoxplots are most effective for continuous variables and for situations where differences between groups are of interest. For example, comparing the distribution of customer purchase amounts across demographic segments or examining test scores across classrooms can be done efficiently with boxplots. Their ability to summarize multiple distributions in a consistent visual format allows analysts to see whether groups differ in their central tendency, their variability, or their presence of outliers—three characteristics that often have substantive implications.\nWhen reading a boxplot, look for:\n\nMedian location: a line inside the box showing the middle of the distribution.\n\nInterquartile range (IQR): the height of the box, representing the spread of the central 50% of values.\n\nWhisker length: showing how far the bulk of the data extends beyond the IQR.\n\nOutliers: individual points beyond the whiskers, which may represent unusual cases or data issues.\n\nGroup comparisons: differences in median or spread across categories displayed side by side.\n\nOne limitation to boxplots is that they hide the shape of the distribution; two groups may have identical boxplots but very different underlying patterns, such as bimodality. Another issue arises when sample sizes differ greatly between groups—small samples can produce unstable quartile estimates, which make comparisons misleading. Analysts must also decide how to treat outliers: boxplots flag them mechanically, but not all flagged points are errors, and not all errors will be flagged. Finally, boxplots are inappropriate when groups have extremely skewed or categorical data; in those cases, histograms, violin plots, or frequency displays may communicate the distribution more effectively.\nsns.boxplot(x=category, y=value)\n\n\nHeatmaps\nHeatmaps visualize a matrix of values using color intensity, allowing the reader to quickly grasp patterns, clusters, or anomalies across two dimensions at once. Because the visual encoding relies on color rather than position alone, heatmaps can reveal structure that might be difficult to detect in numerical tables. They are especially common in analytics and AI workflows for displaying correlation matrices, confusion matrices, feature–target relationships, and any grid-like data where the magnitude of values matters more than their exact numeric labels.\nHeatmaps are most effective when the underlying data represent a meaningful two-dimensional relationship. For example, a correlation heatmap allows you to see which variables are strongly related and which are largely independent. A confusion matrix, presented as a heatmap, shows where a classification model succeeds or fails by highlighting concentration along the diagonal or leakage into off-diagonal cells. In operational systems, heatmaps can highlight seasonal or hourly patterns in activity, or detect anomalies when a particular row or column deviates sharply from the expected color pattern.\nWhen reading a heatmap, look for:\n\nColor intensity: darker or lighter regions that indicate high or low values.\n\nClusters or blocks: visually contiguous regions suggesting variables or categories with similar behavior.\n\nSymmetry: especially in correlation heatmaps, where symmetry is expected and deviations may indicate data-processing issues.\n\nOutliers or sharp transitions: individual cells that abruptly differ from their neighbors.\n\nDominant rows or columns: which may signal influential variables or structural patterns in the data.\n\nHeatmaps also come with several common pitfalls. One of the most important is the choice of colormap: perceptually uneven palettes (such as rainbow or jet) can distort interpretation by making small differences appear large. Similarly, inappropriate scaling—such as compressing all values between −0.2 and 0.2 into the same color range, hiding meaningful variation. Labeling is another constraint: heatmaps with many rows or columns can become unreadable if axis labels are crowded or rotated excessively. Finally, heatmaps are not suitable when exact values matter; the purpose is to highlight patterns, not precise magnitudes. Analysts must always verify numerical values when decisions depend on them.\nsns.heatmap(df.corr(), annot=True)"
  },
  {
    "objectID": "textbook_src/ch9_visualization_from_data_to_insight.html#customizing-visualizations",
    "href": "textbook_src/ch9_visualization_from_data_to_insight.html#customizing-visualizations",
    "title": "Visualization: From Data to Insight",
    "section": "Customizing Visualizations",
    "text": "Customizing Visualizations\nA visualization is more than a direct translation of data into marks on a screen. Small design choices on facets suchs as titles, labels, scales, colors, and layout—shape how readers interpret the information and whether they interpret it correctly. Good customization does not embellish a plot; rather, it clarifies its purpose and reduces the cognitive work required to understand it. Poor customization, by contrast, can distort patterns, obscure important comparisons, or lead to misinterpretation even when the underlying data are correct.\nClear, descriptive titles and axis labels are essential. A title should communicate the purpose of the plot, not merely restate variable names. Axis labels should specify units, transformations, or categories where relevant. Ambiguous or missing labels force readers to infer meaning, increasing the risk of misunderstanding. In professional settings, unclear labeling is one of the most common reasons stakeholders misinterpret visual summaries.\nColor is a powerful but easily misused visual encoding. Accessible color palettes help ensure that plots remain interpretable for readers with color-vision deficiencies and that meaning is tied to structure rather than decoration. Colors should signal differences that matter: categorical differences, intensity gradients, or grouping structures. Using too many colors, or colors with no conceptual mapping, creates visual noise. Red–green contrasts, in particular, should be avoided unless paired with alternate encodings such as texture or shape.\nOne of the most critical customization decisions involves scales and axis limits. Truncating the y-axis—starting it above zero—can dramatically exaggerate differences in bar or line heights, making minor variations appear significant. Conversely, setting overly wide limits can flatten meaningful differences. Logarithmic or standardized scales may be appropriate when data span several orders of magnitude, but these should always be labeled clearly. In general, a reader should never be surprised by how an axis has been configured.\nAdditional customization tools, such as gridlines, legends, and annotations, should be used with intention. Gridlines can support accurate comparisons but become distracting when too dense. Legends should appear in locations that do not obscure the data. Annotations can highlight noteworthy points or ranges, but excessive annotation can clutter a plot. Good visualization practice balances clarity with simplicity, emphasizing the data’s story rather than the plot’s mechanics.\nUltimately, customization is not about aesthetics, although that plays a role, it is about honest communication. A well-customized visualization respects the reader’s effort, supports accurate inference, and reduces opportunities for confusion. These qualities become especially important as we transition into model evaluation, where misleading visualizations can produce overconfidence in models or misdiagnose sources of error."
  },
  {
    "objectID": "textbook_src/ch9_visualization_from_data_to_insight.html#interpreting-visualizations",
    "href": "textbook_src/ch9_visualization_from_data_to_insight.html#interpreting-visualizations",
    "title": "Visualization: From Data to Insight",
    "section": "Interpreting Visualizations",
    "text": "Interpreting Visualizations\nInterpreting a visualization is not the same as describing what it looks like. Description is surface-level: “the line goes up,” “this bar is taller,” or “these points are scattered.” Interpretation goes deeper. It connects the visual structure to the underlying data-generating process, the analytical question at hand, and the limitations or uncertainties that shape our conclusions. Effective interpretation requires curiosity, skepticism, and a disciplined habit of checking what a plot really shows—and what it might hide.\nA good starting point is to ask what is being encoded. Every visualization maps data to visual properties such as position, length, color, or shape. Before drawing any conclusions, readers should identify which attributes correspond to which variables and whether those mappings are appropriate. A scatter plot with log-scaled axes tells a different story than the same data plotted on a linear scale. A heatmap’s color range can emphasize or downplay variation depending on how its limits are set. Understanding the encoding is a prerequisite for understanding the plot.\nNext, look for patterns in the distribution: where values cluster, where they spread out, and whether there are unexpected gaps or concentrations. These features often reveal how the underlying system behaves. A long right tail may indicate rare but consequential events. A bimodal distribution may suggest two subpopulations with different behaviors. Unexpected spikes or gaps can signal measurement issues or structural breaks in the data. Visualization makes these clues visible, but interpretation requires linking them to plausible explanations.\nInterpreting visualizations also involves scrutinizing outliers. Outliers can be the most informative points in a dataset or the most misleading. A single extreme value may represent a critical event (such as a fraudulent transaction), a rare but real phenomenon (a customer making a very large purchase), or a simple data-entry error. Visualization helps analysts identify outliers quickly, but it cannot determine their meaning. That judgment must come from domain knowledge and further investigation.\nAnother important habit is to consider the role of noise and sample size. Patterns that look compelling in small samples may vanish when more data are collected, while real relationships may appear weak or jagged when data are limited. Analysts must be cautious not to over-interpret small fluctuations or assume that smooth patterns indicate certainty. Many seemingly strong visual trends are driven by random variation, and many messy plots reflect systems with genuine complexity.\nWhen relationships between variables are visualized—especially in scatter plots or line plots—analysts should resist the temptation to infer causation from correlation. Visual relationships are powerful and persuasive, and they often guide model building. But they do not explain why the relationship exists or rule out confounding factors. A downward-sloping trend might reflect a true causal effect, a seasonal pattern, or an omitted variable that influences both axes.\nTo interpret responsibly, ask yourself:\n\nWhat does the visualization encode, and are those encodings appropriate?\n\nWhat patterns are visible, and which might be artifacts of binning, smoothing, or scale?\n\nAre there outliers, and what could they represent?\n\nHow might sample size or noise influence what I see?\n\nWhat alternative explanations could produce this pattern?\n\nDoes this visualization support the analytic question I am trying to answer?\n\nVisualization provides evidence, not conclusions! It shapes hypotheses, guides modeling decisions, and surfaces questions that require deeper inquiry. Rich interpretation comes from integrating what the visualization shows with what the analyst knows about the system, the data, and the methodological tools available for further analysis.\nUltimately, interpreting visualizations is a form of critical thinking. It combines visual literacy with domain insight and statistical reasoning."
  },
  {
    "objectID": "textbook_src/ch9_visualization_from_data_to_insight.html#integrated-workflow",
    "href": "textbook_src/ch9_visualization_from_data_to_insight.html#integrated-workflow",
    "title": "Visualization: From Data to Insight",
    "section": "Integrated Workflow",
    "text": "Integrated Workflow\nThe process of turning raw data into visual insight is almost never linear. It looks more like an iterative cycle in which each step informs and sets up the next. Visualization plays a central role in this cycle: it clarifies what the data contain, reveals whether summaries are accurate, and suggests how models or transformations should be adjusted. A useful mental model for this process consists of five recurring steps:\n\nSimulate or load data\nEvery workflow begins with a dataset—either drawn from the real world or generated synthetically to test ideas. The goal at this stage is not interpretation but orientation: understanding what data are available and how they are structured.\nSummarize\nBefore creating any visualization, compute simple summaries: counts, means, ranges, and distributional statistics. These summaries provide a baseline understanding of scale, variability, and potential anomalies. They also create expectations that can later be checked against visual evidence. If a summary statistic and a visualization tell different stories, take note! That discrepancy is often a clue worth investigating.\nVisualize\nVisualization makes the structure of the data visible. Patterns that are unclear in numeric form, think clusters, gaps, skew, relationships, often emerge immediately when plotted.\nInterpret\nInterpretation involves asking what the visual patterns suggest about the underlying system, whether anomalies are meaningful or artifacts, and whether observed relationships align with theory or domain knowledge.\nRefine\nThe first view is rarely the final one. Analysts adjust bin widths, try alternate scales, filter subgroups, or generate additional plots to clarify ambiguous patterns. They may revise summaries or return to earlier steps entirely, especially when visualizations reveal inconsistencies or unexpected behavior. In complex systems, refinement is not a sign of error but a hallmark of responsible analytical reasoning."
  },
  {
    "objectID": "guides/vscode_update_stability.html",
    "href": "guides/vscode_update_stability.html",
    "title": "Locking VS Code Updates for a Stable Semester",
    "section": "",
    "text": "OPTIONAL! This guide and these steps are optional. I would suggest attempting this if you are relatively computer savy. If not, you should skip it. The consequences for not locking down surpise updates are not that severe. Some of the code I run might look different to you, minor changes in behavior.\n\nThis guide walks you through a one-time configuration that prevents surprise changes in Visual Studio Code during the semester:\n\nDisabling automatic VS Code updates\nDisabling automatic extension updates\nInstalling the Python extension (Microsoft)\nVerifying your Python interpreter selection\nConfirming everything still runs normally\n\n\n\n\n\n\n\nNote\n\n\n\nFollow this top-to-bottom, in order.\nMost problems happen when settings are changed in the wrong place or only partially applied. Again, don’t attempt this if you are not very comfortable problem solving on a PC or MAC.\n\n\n\n\n\nThis guide assumes you can:\n\nopen VS Code\n\nopen a terminal (Mac Terminal / Windows PowerShell)\n\ncopy/paste text\n\nsave a file\n\n\n\n\n\nBy the end of this guide/walkthrough, you should have:\n\nVS Code configured to only update when you choose\nExtensions configured to only update when you choose\nThe Python extension installed\nA quick health check confirming Python runs\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are working in a class project folder, you can apply these settings at the workspace level so the course environment behaves consistently in that folder."
  },
  {
    "objectID": "guides/vscode_update_stability.html#overview",
    "href": "guides/vscode_update_stability.html#overview",
    "title": "Locking VS Code Updates for a Stable Semester",
    "section": "",
    "text": "OPTIONAL! This guide and these steps are optional. I would suggest attempting this if you are relatively computer savy. If not, you should skip it. The consequences for not locking down surpise updates are not that severe. Some of the code I run might look different to you, minor changes in behavior.\n\nThis guide walks you through a one-time configuration that prevents surprise changes in Visual Studio Code during the semester:\n\nDisabling automatic VS Code updates\nDisabling automatic extension updates\nInstalling the Python extension (Microsoft)\nVerifying your Python interpreter selection\nConfirming everything still runs normally\n\n\n\n\n\n\n\nNote\n\n\n\nFollow this top-to-bottom, in order.\nMost problems happen when settings are changed in the wrong place or only partially applied. Again, don’t attempt this if you are not very comfortable problem solving on a PC or MAC.\n\n\n\n\n\nThis guide assumes you can:\n\nopen VS Code\n\nopen a terminal (Mac Terminal / Windows PowerShell)\n\ncopy/paste text\n\nsave a file\n\n\n\n\n\nBy the end of this guide/walkthrough, you should have:\n\nVS Code configured to only update when you choose\nExtensions configured to only update when you choose\nThe Python extension installed\nA quick health check confirming Python runs\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are working in a class project folder, you can apply these settings at the workspace level so the course environment behaves consistently in that folder."
  },
  {
    "objectID": "guides/vscode_update_stability.html#phase-1-make-vs-code-updates-manual",
    "href": "guides/vscode_update_stability.html#phase-1-make-vs-code-updates-manual",
    "title": "Locking VS Code Updates for a Stable Semester",
    "section": "Phase 1 — Make VS Code Updates Manual",
    "text": "Phase 1 — Make VS Code Updates Manual\n\n1. Open the Settings (JSON) file\n\nOpen VS Code\n\nOpen Settings\n\nIn the upper right, click the icon: Open Settings (JSON)\n\nYou are now editing a file that looks like:\n{\n  // settings live here\n}\n\n\n\n\n\n\nImportant\n\n\n\nYou must edit Settings (JSON), not the normal Settings UI, so you can copy the exact configuration reliably. Stop here if this looks scary.\n\n\n\n\n\n2. Add the “stability settings”\nCopy and paste the following exactly into your Settings (JSON).\nIf there are already settings in the file, paste these inside the top-level { ... }.\n{\n  \"update.mode\": \"manual\",\n  \"extensions.autoUpdate\": false,\n  \"extensions.autoCheckUpdates\": false\n}\nSave the file.\n\n\n\n3. Confirm the settings were applied\nDo a quick confirmation:\n\nOpen Settings UI and search for Update Mode\n\nIt should show Manual\n\nOpen Settings UI and search for Extensions: Auto Update\n\nIt should be off/unchecked\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you accidentally pasted an extra comma or broke the JSON formatting, VS Code will show a warning in the Settings (JSON) editor. Fix formatting first."
  },
  {
    "objectID": "guides/vscode_update_stability.html#phase-2-install-only-the-required-extension",
    "href": "guides/vscode_update_stability.html#phase-2-install-only-the-required-extension",
    "title": "Locking VS Code Updates for a Stable Semester",
    "section": "Phase 2 — Install Only the Required Extension",
    "text": "Phase 2 — Install Only the Required Extension\n\n4. Install the Python extension (Microsoft)\nNote: You may have this installed already… check!\n\nOpen the Extensions panel\nSearch for: Python\nInstall Python by Microsoft\n\n\n\n\n\n\n\nImportant\n\n\n\nDo not install preview/insider Python extensions."
  },
  {
    "objectID": "guides/vscode_update_stability.html#phase-3-interpreter-consistency-common-hidden-source-of-errors",
    "href": "guides/vscode_update_stability.html#phase-3-interpreter-consistency-common-hidden-source-of-errors",
    "title": "Locking VS Code Updates for a Stable Semester",
    "section": "Phase 3 — Interpreter Consistency (Common “Hidden” Source of Errors)",
    "text": "Phase 3 — Interpreter Consistency (Common “Hidden” Source of Errors)\n\n5. Select the correct Python interpreter\nEven if VS Code is stable, your code can still fail if VS Code uses the wrong interpreter.\n\nOpen the Command Palette\n\nmacOS: Cmd + Shift + P\n\nWindows: Ctrl + Shift + P\n\n\nChoose: Python: Select Interpreter\nSelect the interpreter you want (The course suggests a UV solution):\n\nSystem Python (if required), or\nA project environment interpreter (commonly a .venv folder)\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe wrong interpreter is the #1 cause of: - ModuleNotFoundError - “package not found” - code working in terminal but not in VS Code (or vice versa)\n\n\n\n\n\n6. Quick visual check (bottom-right)\nLook at the bottom-right corner of VS Code.\n\nYou should see a Python version + environment name\nIf you click it, it should show the interpreter you selected"
  },
  {
    "objectID": "guides/vscode_update_stability.html#phase-4-manual-updates",
    "href": "guides/vscode_update_stability.html#phase-4-manual-updates",
    "title": "Locking VS Code Updates for a Stable Semester",
    "section": "Phase 4 — Manual Updates",
    "text": "Phase 4 — Manual Updates\n\n7. Updating VS Code (manual)\n\nWindows / macOS:\nHelp → Check for Updates\n\n\n\n\n\n\n\nNote\n\n\n\nDo not update VS Code right before an exam, That may lead to unexpectected and frustrating changes to your working environment.\n\n\n\n\n\n8. Updating extensions (manual)\n\nOpen Extensions\nClick the … menu in the Extensions panel\nChoose Check for Updates\nUpdate only what you need"
  },
  {
    "objectID": "guides/vscode_update_stability.html#optional-apply-these-settings-to-a-course-workspace-folder",
    "href": "guides/vscode_update_stability.html#optional-apply-these-settings-to-a-course-workspace-folder",
    "title": "Locking VS Code Updates for a Stable Semester",
    "section": "Optional: Apply These Settings to a Course Workspace Folder",
    "text": "Optional: Apply These Settings to a Course Workspace Folder\nIf you have a course project folder (recommended), you can apply settings to that folder only.\n\nWorkspace settings file\nInside your project folder, create:\n.vscode/settings.json\nThen paste the same settings into that file:\n{\n  \"update.mode\": \"manual\",\n  \"extensions.autoUpdate\": false,\n  \"extensions.autoCheckUpdates\": false\n}\n\n\n\n\n\n\nTip\n\n\n\nWorkspace settings are helpful because they travel with the project folder.\nUser settings apply to all VS Code projects on your machine."
  },
  {
    "objectID": "guides/vscode_update_stability.html#troubleshooting",
    "href": "guides/vscode_update_stability.html#troubleshooting",
    "title": "Locking VS Code Updates for a Stable Semester",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\n“I can’t find Open Settings (JSON)”\nTry either of these:\n\nCommand Palette → Preferences: Open User Settings (JSON)\nSettings UI → use the top-right icon for JSON\n\n\n\n\n“My JSON is broken”\nSymptoms include red underlines and warnings in the settings editor.\nFixes: - Make sure there is only one outer { ... } - Every setting line except the last must end with a comma - Strings must be wrapped in quotes\nCorrect example:\n{\n  \"update.mode\": \"manual\",\n  \"extensions.autoUpdate\": false\n}\n\n\n\n“Python works in terminal but not in VS Code”\nThis is almost always interpreter mismatch.\nChecklist: - Run Python: Select Interpreter - Confirm the bottom-right interpreter matches your project environment"
  },
  {
    "objectID": "guides/vscode_update_stability.html#final-checkpoint",
    "href": "guides/vscode_update_stability.html#final-checkpoint",
    "title": "Locking VS Code Updates for a Stable Semester",
    "section": "Final checkpoint",
    "text": "Final checkpoint\n\nVS Code updates are set to manual\nExtensions do not auto-update\nPython extension (Microsoft) is installed\nVS Code shows the correct Python interpreter\nYou can run a simple Python file successfully"
  },
  {
    "objectID": "guides/terminal_powershell_basics.html",
    "href": "guides/terminal_powershell_basics.html",
    "title": "Terminal & PowerShell Basics for This Course",
    "section": "",
    "text": "Many students in this course have never used the terminal (Mac) or PowerShell (Windows) before.\nThat is completely normal.\nThis short guide teaches only what you need to successfully complete the course setup and run Python code later.\nYou do not need to memorize commands or understand how your computer works internally.\nYou just need to be comfortable running a small number of commands and recognizing when something worked."
  },
  {
    "objectID": "guides/terminal_powershell_basics.html#why-this-guide-exists",
    "href": "guides/terminal_powershell_basics.html#why-this-guide-exists",
    "title": "Terminal & PowerShell Basics for This Course",
    "section": "",
    "text": "Many students in this course have never used the terminal (Mac) or PowerShell (Windows) before.\nThat is completely normal.\nThis short guide teaches only what you need to successfully complete the course setup and run Python code later.\nYou do not need to memorize commands or understand how your computer works internally.\nYou just need to be comfortable running a small number of commands and recognizing when something worked."
  },
  {
    "objectID": "guides/terminal_powershell_basics.html#what-the-terminal-powershell-is-plain-language",
    "href": "guides/terminal_powershell_basics.html#what-the-terminal-powershell-is-plain-language",
    "title": "Terminal & PowerShell Basics for This Course",
    "section": "What the terminal / PowerShell is (plain language)",
    "text": "What the terminal / PowerShell is (plain language)\nThink of the terminal as:\n\nA text-based way to tell your computer exactly what to do, step by step.\n\nInstead of clicking buttons, you:\n- type a command\n- press Enter\n- read the result\nThe terminal:\n- works inside a folder\n- runs commands in order\n- shows errors immediately when something goes wrong"
  },
  {
    "objectID": "guides/terminal_powershell_basics.html#what-this-guide-assumes-and-teaches",
    "href": "guides/terminal_powershell_basics.html#what-this-guide-assumes-and-teaches",
    "title": "Terminal & PowerShell Basics for This Course",
    "section": "What this guide assumes (and teaches)",
    "text": "What this guide assumes (and teaches)\nBy the end of this guide, you should be able to:\n\nopen Terminal (Mac) or PowerShell (Windows)\n\ntype or paste a command\n\nrun a command\n\nknow whether it worked\n\nknow which folder you are in\n\nmove into a different folder"
  },
  {
    "objectID": "guides/terminal_powershell_basics.html#part-a-opening-the-terminal",
    "href": "guides/terminal_powershell_basics.html#part-a-opening-the-terminal",
    "title": "Terminal & PowerShell Basics for This Course",
    "section": "Part A — Opening the terminal",
    "text": "Part A — Opening the terminal\n\nmacOS: Open Terminal\nYou have two common options:\n\nOption 1: Spotlight (recommended)\n\nPress Command + Space\n\nType Terminal\n\nPress Enter\n\n\n\nOption 2: Applications folder\n\nApplications → Utilities → Terminal\n\n\n\n\n\nWindows: Open PowerShell\n\nOption 1: Start Menu (recommended)\n\nClick Start\n\nType PowerShell\n\nOpen Windows PowerShell or Windows Terminal\n\n\n\nOption 2: Right-click menu (sometimes available)\n\nRight-click in a folder\n\nChoose Open in Terminal or Open PowerShell here\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor this course, Terminal (Mac) and PowerShell (Windows) are equivalent.\nAll commands will be clearly labeled for your system."
  },
  {
    "objectID": "guides/terminal_powershell_basics.html#part-b-what-you-see-when-it-opens",
    "href": "guides/terminal_powershell_basics.html#part-b-what-you-see-when-it-opens",
    "title": "Terminal & PowerShell Basics for This Course",
    "section": "Part B — What you see when it opens",
    "text": "Part B — What you see when it opens\nWhen the terminal opens, you will see:\n- a block of text\n- a blinking cursor\nExample (Mac):\nusername@computer-name ~ %\nExample (Windows):\nPS C:\\Users\\username&gt;\nThis line is called the prompt.\nIt tells you:\n- who you are\n- which computer you are on\n- which folder you are currently in\n\n\nThe most important rule\n\n\n\n\n\n\nImportant\n\n\n\nCommands run in the current folder.\nIf you are in the wrong folder, commands will fail or create files in the wrong place."
  },
  {
    "objectID": "guides/terminal_powershell_basics.html#part-c-running-commands",
    "href": "guides/terminal_powershell_basics.html#part-c-running-commands",
    "title": "Terminal & PowerShell Basics for This Course",
    "section": "Part C — Running commands",
    "text": "Part C — Running commands\n\nHow to run a command\n\nClick in the terminal window\nType (or paste) a command\nPress Enter\n\nExample:\npython --version\nThe computer will respond immediately.\n\n\n\nCopying and pasting commands\n\nMac: Cmd + C (copy), Cmd + V (paste)\nWindows: Ctrl + C (copy), Ctrl + V (paste)\n\n\n\n\n\n\n\nTip\n\n\n\nIt is perfectly fine to copy and paste commands from course materials.\nThis is not cheating; it is standard practice.\n\n\n\n\n\nHow to tell if a command worked\nA command usually worked if:\n- no error message appears\n- you see output (text) that looks reasonable\n- the prompt returns and waits for the next command\nExample of success:\nPython 3.13.1\n\n\n\nHow to tell if a command failed\nA command likely failed if: - you see words like error, not found, or command not recognized\n- the output looks red or alarming\n- nothing happened and the prompt did not return\nExample of failure:\npython : The term 'python' is not recognized\nWhen this happens:\n- stop\n- read the message\n- do not keep running commands blindly"
  },
  {
    "objectID": "guides/terminal_powershell_basics.html#part-d-where-am-i-folders",
    "href": "guides/terminal_powershell_basics.html#part-d-where-am-i-folders",
    "title": "Terminal & PowerShell Basics for This Course",
    "section": "Part D — Where am I? (folders)",
    "text": "Part D — Where am I? (folders)\n\nCheck your current folder\n\nMac\npwd\n\n\nWindows\ncd\nThis prints the folder you are currently in.\n\n\n\n\nList files in the current folder\n\nMac\nls\n\n\nWindows\ndir\nYou should recognize file names you expect to see."
  },
  {
    "objectID": "guides/terminal_powershell_basics.html#part-e-moving-between-folders",
    "href": "guides/terminal_powershell_basics.html#part-e-moving-between-folders",
    "title": "Terminal & PowerShell Basics for This Course",
    "section": "Part E — Moving between folders",
    "text": "Part E — Moving between folders\n\nChange folders (cd)\ncd means change directory.\n\nExample (Mac)\ncd Documents\n\n\nExample (Windows)\ncd Documents\n\n\n\n\nMove into a specific folder\ncd CLASS_FOLDER\n(Replace CLASS_FOLDER with the actual folder name.)\n\n\n\nMove up one level\ncd ..\nThis is useful if you accidentally went into the wrong folder.\n\n\n\n\n\n\n\nWarning\n\n\n\nFolder names must match exactly, including capitalization.\nIf a folder name has spaces, it is harder to use in the terminal."
  },
  {
    "objectID": "guides/terminal_powershell_basics.html#part-f-stopping-a-command",
    "href": "guides/terminal_powershell_basics.html#part-f-stopping-a-command",
    "title": "Terminal & PowerShell Basics for This Course",
    "section": "Part F — Stopping a command",
    "text": "Part F — Stopping a command\nIf a command appears to be stuck or running too long:\n\nPress Ctrl + C\n\nThis safely stops the command."
  },
  {
    "objectID": "guides/terminal_powershell_basics.html#part-g-common-beginner-mistakes-and-fixes",
    "href": "guides/terminal_powershell_basics.html#part-g-common-beginner-mistakes-and-fixes",
    "title": "Terminal & PowerShell Basics for This Course",
    "section": "Part G — Common beginner mistakes (and fixes)",
    "text": "Part G — Common beginner mistakes (and fixes)\n\n“Nothing happened”\n\nDid you press Enter?\n\nIs the cursor still blinking?\n\n\n\n\n“Command not found”\n\nYou may have typed the command incorrectly\n\nYou may not have the software installed yet\n\nCheck spelling carefully\n\n\n\n\n“Permission denied” (Mac/Linux)\n\nThis usually means you are trying to modify a protected folder\n\nMove your project into Documents or your home folder\n\n\n\n\nI’m nervous I’ll break something\n\n\n\n\n\n\nImportant\n\n\n\nYou are very unlikely to damage your computer by running commands in this course.\n\n\nWe do not:\n- delete system files\n- modify protected locations\n- require administrator access\n\n\n\nFinal confidence check\nBefore moving on to the course setup guide, make sure you can:\n\nopen Terminal or PowerShell\nrun a command\nsee output\nmove into a folder\nstop a command with Ctrl + C\n\nIf you can do those things, you are ready."
  }
]